{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df272772",
   "metadata": {},
   "source": [
    "# A Notebook to Finetune A SCL Model, reading datafiles and calling train and eval file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7158d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Necessary Imports\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "745415dc-6f87-4521-ae3a-f1492eb39356",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_max.csv']#,'/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5cd4046-6108-476d-b80b-479a51a7e2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10264\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_paths[0])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3fe0480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels  64\n",
      "2024-12-01 21:18:45.130384: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 21:18:45.875892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "finished imports\n",
      "beginning execution\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlqmeyers\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/wandb/run-20241201_211849-g2jlw4oo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbreezy-fire-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/bioclip_layerwise_finetuning64_ids_batch1_sample_num_max\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/bioclip_layerwise_finetuning64_ids_batch1_sample_num_max/runs/g2jlw4oo\u001b[0m\n",
      "Date and time when this experiment was started: 24-12-01 21:18\n",
      "Data Settings:\n",
      "{'aug_p': 0.3, 'batch_size': 64, 'crop_height': None, 'crop_left': None, 'crop_top': None, 'crop_width': None, 'cropped': False, 'datafiles': {'gallery': None, 'query': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv', 'reference': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_02.csv', 'test': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv', 'train': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_max.csv', 'valid': ''}, 'dataset': 'summer_2023', 'fname_col': 'new_filepath', 'gallery_id': 'gallery_id', 'image_id_col': 'image_id', 'input_size': [250, 250], 'iteration_id': 'iteration_id', 'label_col': 'ID', 'n_distractors': 9, 'percent_reference': 0.1, 'percent_valid': 0.2, 'sample_reference': True, 'sample_valid': True, 'split_type': 'closed'}\n",
      "Train Settings:\n",
      "{'checkpoint_to_load': None, 'early_stop_consecutive_epochs': 10, 'early_stopping': True, 'early_stopping_metric': 'valid_loss', 'finetune_epochs': 150, 'gpu': 1, 'learning_rate': 0.001, 'margin': 0.2, 'num_epochs': 250, 'print_k': 10, 'resume_from_saved': False, 'save_checkpoint_freq': 50, 'unfreeze_patience_epochs': 5, 'wandb_dir_path': '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/', 'wandb_entity_name': 'meyers_luke_lab', 'wandb_project_name': 'bioclip_layerwise_finetuning64_ids_batch1_sample_num_max', 'wandb_run_id': None}\n",
      "Model Settings:\n",
      "{'latent_dim': 128, 'model_class': 'clip_reid', 'model_path': '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/64_ids_batch1_sample_num_max.pth', 'num_labels': '64'}\n",
      "Using GPU 1\n",
      "Creating train and valid dataloaders...\n",
      "Using 2053 samples for validation set\n",
      "8211 total training samples\n",
      "initializing evaluation dataloaders...\n",
      "Using 1087 samples for reference set\n",
      "9779 total test samples\n",
      "Batch image shape: torch.Size([64, 3, 250, 250])\n",
      "Batch label shape: torch.Size([64])\n",
      "Building model....\n",
      "{'image_size': 224, 'layers': 12, 'width': 768, 'patch_size': 16}\n",
      "Loss: TripletMarginLoss(\n",
      "  (distance): CosineSimilarity()\n",
      "  (reducer): AvgNonZeroReducer()\n",
      ")\n",
      "Getting clip feature extractor...\n",
      "/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Found device: cuda\n",
      "Training model head...\n",
      "[1,   129] train_loss: 1.1566 | val_loss: 0.0868\n",
      "[2,   129] train_loss: 1.0963 | val_loss: 0.0852\n",
      "[3,   129] train_loss: 1.0803 | val_loss: 0.0842\n",
      "[4,   129] train_loss: 1.0690 | val_loss: 0.0829\n",
      "[5,   129] train_loss: 1.0388 | val_loss: 0.0819\n",
      "[6,   129] train_loss: 1.0241 | val_loss: 0.0829\n",
      "[7,   129] train_loss: 1.0329 | val_loss: 0.0824\n",
      "[8,   129] train_loss: 1.0346 | val_loss: 0.0825\n",
      "[9,   129] train_loss: 1.0244 | val_loss: 0.0802\n",
      "[10,   129] train_loss: 1.0268 | val_loss: 0.0815\n",
      "[11,   129] train_loss: 1.0272 | val_loss: 0.0814\n",
      "[12,   129] train_loss: 1.0167 | val_loss: 0.0790\n",
      "[13,   129] train_loss: 1.0000 | val_loss: 0.0796\n",
      "[14,   129] train_loss: 1.0178 | val_loss: 0.0807\n",
      "[15,   129] train_loss: 1.0243 | val_loss: 0.0802\n",
      "[16,   129] train_loss: 1.0155 | val_loss: 0.0785\n",
      "[17,   129] train_loss: 0.9968 | val_loss: 0.0790\n",
      "[18,   129] train_loss: 0.9982 | val_loss: 0.0810\n",
      "[19,   129] train_loss: 0.9988 | val_loss: 0.0794\n",
      "[20,   129] train_loss: 1.0090 | val_loss: 0.0775\n",
      "[21,   129] train_loss: 0.9884 | val_loss: 0.0785\n",
      "[22,   129] train_loss: 0.9848 | val_loss: 0.0793\n",
      "[23,   129] train_loss: 0.9804 | val_loss: 0.0794\n",
      "[24,   129] train_loss: 0.9808 | val_loss: 0.0780\n",
      "[25,   129] train_loss: 0.9904 | val_loss: 0.0782\n",
      "[26,   129] train_loss: 0.9818 | val_loss: 0.0795\n",
      "[27,   129] train_loss: 0.9886 | val_loss: 0.0793\n",
      "[28,   129] train_loss: 0.9839 | val_loss: 0.0790\n",
      "[29,   129] train_loss: 0.9867 | val_loss: 0.0784\n",
      "[30,   129] train_loss: 0.9702 | val_loss: 0.0775\n",
      "[31,   129] train_loss: 0.9735 | val_loss: 0.0787\n",
      "[32,   129] train_loss: 0.9634 | val_loss: 0.0806\n",
      "[33,   129] train_loss: 0.9667 | val_loss: 0.0786\n",
      "[34,   129] train_loss: 0.9782 | val_loss: 0.0779\n",
      "[35,   129] train_loss: 0.9748 | val_loss: 0.0784\n",
      "[36,   129] train_loss: 0.9792 | val_loss: 0.0773\n",
      "[37,   129] train_loss: 0.9604 | val_loss: 0.0782\n",
      "[38,   129] train_loss: 0.9748 | val_loss: 0.0774\n",
      "[39,   129] train_loss: 0.9742 | val_loss: 0.0768\n",
      "[40,   129] train_loss: 0.9747 | val_loss: 0.0777\n",
      "[41,   129] train_loss: 0.9722 | val_loss: 0.0771\n",
      "[42,   129] train_loss: 0.9658 | val_loss: 0.0779\n",
      "[43,   129] train_loss: 0.9757 | val_loss: 0.0768\n",
      "[44,   129] train_loss: 0.9767 | val_loss: 0.0785\n",
      "[45,   129] train_loss: 0.9669 | val_loss: 0.0781\n",
      "[46,   129] train_loss: 0.9761 | val_loss: 0.0762\n",
      "[47,   129] train_loss: 0.9648 | val_loss: 0.0782\n",
      "[48,   129] train_loss: 0.9593 | val_loss: 0.0782\n",
      "[49,   129] train_loss: 0.9638 | val_loss: 0.0769\n",
      "[50,   129] train_loss: 0.9746 | val_loss: 0.0795\n",
      "Saving checkpoint 50\n",
      "[51,   129] train_loss: 0.9884 | val_loss: 0.0776\n",
      "[52,   129] train_loss: 0.9531 | val_loss: 0.0781\n",
      "[53,   129] train_loss: 0.9627 | val_loss: 0.0765\n",
      "[54,   129] train_loss: 0.9655 | val_loss: 0.0797\n",
      "[55,   129] train_loss: 0.9535 | val_loss: 0.0775\n",
      "[56,   129] train_loss: 0.9608 | val_loss: 0.0759\n",
      "[58,   129] train_loss: 0.9496 | val_loss: 0.0759\n",
      "[59,   129] train_loss: 0.9574 | val_loss: 0.0777\n",
      "[60,   129] train_loss: 0.9749 | val_loss: 0.0777\n",
      "[61,   129] train_loss: 0.9511 | val_loss: 0.0773\n",
      "[62,   129] train_loss: 0.9534 | val_loss: 0.0769\n",
      "[63,   129] train_loss: 0.9673 | val_loss: 0.0778\n",
      "[64,   129] train_loss: 0.9731 | val_loss: 0.0783\n",
      "[65,   129] train_loss: 0.9747 | val_loss: 0.0757\n",
      "[66,   129] train_loss: 0.9447 | val_loss: 0.0781\n",
      "[67,   129] train_loss: 0.9456 | val_loss: 0.0777\n",
      "[68,   129] train_loss: 0.9640 | val_loss: 0.0760\n",
      "[69,   129] train_loss: 0.9543 | val_loss: 0.0783\n",
      "[70,   129] train_loss: 0.9560 | val_loss: 0.0770\n",
      "[71,   129] train_loss: 0.9518 | val_loss: 0.0769\n",
      "[72,   129] train_loss: 0.9484 | val_loss: 0.0768\n",
      "[73,   129] train_loss: 0.9515 | val_loss: 0.0784\n",
      "[74,   129] train_loss: 0.9465 | val_loss: 0.0779\n",
      "[75,   129] train_loss: 0.9405 | val_loss: 0.0764\n",
      "Unfreezing pooler and last layer of backbone at 75 due to no improvement in valid_loss for 10 consecutive epochs\n",
      "Will continue to train for 150 with learning rate of 0.0001 .\n",
      "[76,   129] train_loss: 0.9802 | val_loss: 0.0747\n",
      "[77,   129] train_loss: 0.9422 | val_loss: 0.0696\n",
      "[78,   129] train_loss: 0.8870 | val_loss: 0.0657\n",
      "[79,   129] train_loss: 0.7909 | val_loss: 0.0633\n",
      "[80,   129] train_loss: 0.7852 | val_loss: 0.0698\n",
      "[81,   129] train_loss: 0.7699 | val_loss: 0.0627\n",
      "[82,   129] train_loss: 0.7464 | val_loss: 0.0606\n",
      "[83,   129] train_loss: 0.6571 | val_loss: 0.0588\n",
      "[84,   129] train_loss: 0.6746 | val_loss: 0.0587\n",
      "[85,   129] train_loss: 0.7018 | val_loss: 0.0642\n",
      "[86,   129] train_loss: 0.5498 | val_loss: 0.0624\n",
      "[87,   129] train_loss: 0.5614 | val_loss: 0.0529\n",
      "[88,   129] train_loss: 0.5827 | val_loss: 0.0580\n",
      "[89,   129] train_loss: 0.4796 | val_loss: 0.0502\n",
      "[90,   129] train_loss: 0.4418 | val_loss: 0.0515\n",
      "[91,   129] train_loss: 0.4904 | val_loss: 0.0427\n",
      "[92,   129] train_loss: 0.4684 | val_loss: 0.0517\n",
      "[93,   129] train_loss: 0.4384 | val_loss: 0.0456\n",
      "[94,   129] train_loss: 0.3463 | val_loss: 0.0507\n",
      "[95,   129] train_loss: 0.4095 | val_loss: 0.0384\n",
      "[96,   129] train_loss: 0.4563 | val_loss: 0.0533\n",
      "[97,   129] train_loss: 0.3370 | val_loss: 0.0384\n",
      "[98,   129] train_loss: 0.3556 | val_loss: 0.0424\n",
      "[99,   129] train_loss: 0.2588 | val_loss: 0.0421\n",
      "[100,   129] train_loss: 0.2963 | val_loss: 0.0482\n",
      "Saving checkpoint 100\n",
      "[101,   129] train_loss: 0.2396 | val_loss: 0.0471\n",
      "[102,   129] train_loss: 0.3657 | val_loss: 0.0452\n",
      "Unfreezing layer -2 of backbone at 102 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[103,   129] train_loss: 0.7138 | val_loss: 0.0639\n",
      "[104,   129] train_loss: 0.6383 | val_loss: 0.0594\n",
      "[105,   129] train_loss: 0.6450 | val_loss: 0.0537\n",
      "[106,   129] train_loss: 0.5984 | val_loss: 0.0494\n",
      "[107,   129] train_loss: 0.5275 | val_loss: 0.0428\n",
      "Unfreezing layer -3 of backbone at 107 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[108,   129] train_loss: 0.6644 | val_loss: 0.0640\n",
      "[109,   129] train_loss: 0.6905 | val_loss: 0.0703\n",
      "[110,   129] train_loss: 0.6871 | val_loss: 0.0642\n",
      "[111,   129] train_loss: 0.6248 | val_loss: 0.0547\n",
      "[112,   129] train_loss: 0.5893 | val_loss: 0.0522\n",
      "Unfreezing layer -4 of backbone at 112 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[113,   129] train_loss: 0.1955 | val_loss: 0.0267\n",
      "[114,   129] train_loss: 0.1537 | val_loss: 0.0260\n",
      "[115,   129] train_loss: 0.1678 | val_loss: 0.0350\n",
      "[116,   129] train_loss: 0.0786 | val_loss: 0.0325\n",
      "[117,   129] train_loss: 0.0989 | val_loss: 0.0186\n",
      "[118,   129] train_loss: 0.1009 | val_loss: 0.0237\n",
      "[119,   129] train_loss: 0.0894 | val_loss: 0.0307\n",
      "[120,   129] train_loss: 0.0588 | val_loss: 0.0200\n",
      "[121,   129] train_loss: 0.0758 | val_loss: 0.0136\n",
      "[122,   129] train_loss: 0.0635 | val_loss: 0.0222\n",
      "[123,   129] train_loss: 0.0598 | val_loss: 0.0230\n",
      "[124,   129] train_loss: 0.0736 | val_loss: 0.0331\n",
      "[125,   129] train_loss: 0.0960 | val_loss: 0.0226\n",
      "[126,   129] train_loss: 0.0515 | val_loss: 0.0207\n",
      "Unfreezing layer -5 of backbone at 126 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[127,   129] train_loss: 0.0839 | val_loss: 0.0168\n",
      "[128,   129] train_loss: 0.1233 | val_loss: 0.0270\n",
      "[129,   129] train_loss: 0.1581 | val_loss: 0.0338\n",
      "[130,   129] train_loss: 0.1496 | val_loss: 0.0230\n",
      "[131,   129] train_loss: 0.1480 | val_loss: 0.0225\n",
      "Unfreezing layer -6 of backbone at 131 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[132,   129] train_loss: 0.1937 | val_loss: 0.0160\n",
      "[133,   129] train_loss: 0.0981 | val_loss: 0.0425\n",
      "[134,   129] train_loss: 0.2209 | val_loss: 0.0235\n",
      "[135,   129] train_loss: 0.1544 | val_loss: 0.0261\n",
      "[136,   129] train_loss: 0.0636 | val_loss: 0.0241\n",
      "Unfreezing layer -7 of backbone at 136 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[137,   129] train_loss: 0.1612 | val_loss: 0.0207\n",
      "[138,   129] train_loss: 0.2418 | val_loss: 0.0111\n",
      "[139,   129] train_loss: 0.1950 | val_loss: 0.0304\n",
      "[140,   129] train_loss: 0.1730 | val_loss: 0.0187\n",
      "[141,   129] train_loss: 0.0773 | val_loss: 0.0357\n",
      "[142,   129] train_loss: 0.1308 | val_loss: 0.0375\n",
      "[143,   129] train_loss: 0.1708 | val_loss: 0.0234\n",
      "Unfreezing layer -8 of backbone at 143 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[144,   129] train_loss: 0.1561 | val_loss: 0.0416\n",
      "[145,   129] train_loss: 0.1675 | val_loss: 0.0123\n",
      "[146,   129] train_loss: 0.0871 | val_loss: 0.0152\n",
      "[147,   129] train_loss: 0.1048 | val_loss: 0.0156\n",
      "[148,   129] train_loss: 0.1006 | val_loss: 0.0222\n",
      "Unfreezing layer -9 of backbone at 148 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[149,   129] train_loss: 0.2098 | val_loss: 0.0277\n",
      "[150,   129] train_loss: 0.1845 | val_loss: 0.0334\n",
      "Saving checkpoint 150\n",
      "[151,   129] train_loss: 0.1354 | val_loss: 0.0176\n",
      "[152,   129] train_loss: 0.2082 | val_loss: 0.0218\n",
      "[153,   129] train_loss: 0.1514 | val_loss: 0.0215\n",
      "Unfreezing layer -10 of backbone at 153 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[154,   129] train_loss: 0.0990 | val_loss: 0.0131\n",
      "[155,   129] train_loss: 0.0950 | val_loss: 0.0236\n",
      "[156,   129] train_loss: 0.0882 | val_loss: 0.0299\n",
      "[157,   129] train_loss: 0.1174 | val_loss: 0.0137\n",
      "[158,   129] train_loss: 0.0939 | val_loss: 0.0187\n",
      "Unfreezing layer -11 of backbone at 158 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "[159,   129] train_loss: 0.0666 | val_loss: 0.0230\n",
      "[160,   129] train_loss: 0.1236 | val_loss: 0.0273\n",
      "[161,   129] train_loss: 0.1491 | val_loss: 0.0435\n",
      "[162,   129] train_loss: 0.1360 | val_loss: 0.0251\n",
      "[163,   129] train_loss: 0.1349 | val_loss: 0.0181\n",
      "Early stopping at epoch 163 due to no improvement in valid_loss for 5 consecutive epochs\n",
      "using epoch 137 with loss 0.011094346613390371 for eval\n",
      "Total train time: 497.232867026329min\n",
      "Evaluating model...\n",
      "generating embeddings\n",
      "Reference (or Train) Loss: 0.0877\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0794\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.9644\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.9568\n",
      "\n",
      "Per label 3NN test accuracy:\n",
      "65\t0.99\n",
      "66\t0.97\n",
      "67\t0.99\n",
      "68\t0.98\n",
      "69\t0.95\n",
      "70\t0.87\n",
      "71\t0.81\n",
      "72\t0.98\n",
      "73\t0.99\n",
      "74\t0.96\n",
      "75\t0.99\n",
      "76\t0.99\n",
      "77\t0.99\n",
      "78\t0.93\n",
      "79\t0.97\n",
      "80\t0.94\n",
      "81\t0.95\n",
      "82\t0.99\n",
      "83\t0.99\n",
      "84\t0.97\n",
      "85\t0.97\n",
      "86\t0.97\n",
      "87\t0.98\n",
      "88\t0.99\n",
      "89\t0.80\n",
      "90\t1.00\n",
      "91\t0.99\n",
      "92\t1.00\n",
      "93\t0.96\n",
      "94\t0.94\n",
      "95\t0.98\n",
      "96\t0.96\n",
      "97\t0.94\n",
      "98\t1.00\n",
      "99\t0.98\n",
      "100\t1.00\n",
      "101\t0.88\n",
      "102\t0.96\n",
      "103\t1.00\n",
      "104\t0.96\n",
      "105\t0.54\n",
      "106\t0.90\n",
      "107\t1.00\n",
      "108\t0.98\n",
      "109\t0.96\n",
      "110\t0.96\n",
      "111\t0.97\n",
      "112\t0.97\n",
      "113\t0.93\n",
      "114\t0.00\n",
      "115\t0.96\n",
      "116\t0.99\n",
      "117\t1.00\n",
      "118\t0.99\n",
      "119\t0.99\n",
      "120\t0.94\n",
      "121\t0.88\n",
      "122\t0.99\n",
      "123\t1.00\n",
      "124\t0.87\n",
      "125\t0.90\n",
      "126\t0.67\n",
      "127\t0.95\n",
      "128\t0.95\n",
      "\n",
      "Printing Confusion Matrix:\n",
      "[[185   0   0 ...   0   0   0]\n",
      " [  0  77   0 ...   0   0   0]\n",
      " [  0   0 221 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  45   0   0]\n",
      " [  0   0   0 ...   0  60   0]\n",
      " [  0   0   0 ...   0   0  94]]\n",
      "{'1NN_acc': 0.9644, '3NN_acc': 0.9568, 'label_list': array([ 65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]), 'knn_class': array([0.9893, 0.9747, 0.991 , 0.9821, 0.9463, 0.8659, 0.8128, 0.9834,\n",
      "       0.991 , 0.9615, 0.9889, 0.9911, 0.9945, 0.9329, 0.975 , 0.9377,\n",
      "       0.9466, 0.9945, 0.9903, 0.9655, 0.9737, 0.9706, 0.9784, 0.9927,\n",
      "       0.7955, 0.9965, 0.9888, 1.    , 0.9598, 0.9434, 0.9795, 0.9646,\n",
      "       0.9362, 1.    , 0.9781, 1.    , 0.8779, 0.9554, 1.    , 0.9643,\n",
      "       0.5424, 0.9012, 1.    , 0.9815, 0.9612, 0.9609, 0.9704, 0.9697,\n",
      "       0.931 , 0.    , 0.9627, 0.9888, 1.    , 0.9946, 0.9888, 0.9398,\n",
      "       0.876 , 0.9925, 1.    , 0.8723, 0.8989, 0.6716, 0.9524, 0.9495]), 'knn_conf': array([[185,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,  77,   0, ...,   0,   0,   0],\n",
      "       [  0,   0, 221, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,  45,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,  60,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,  94]]), 'train_loss': 0.0}\n",
      "g2jlw4oo\n",
      "Results saved to pickle file\n",
      "Saving model...\n",
      "Finished\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train loss ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñÉ‚ñá‚ñÅ‚ñÅ‚ñá‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   triplet_num ‚ñà‚ñà‚ñÖ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    valid loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   triplet_num 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    valid loss 0.01815\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbreezy-fire-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/bioclip_layerwise_finetuning64_ids_batch1_sample_num_max/runs/g2jlw4oo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/wandb/run-20241201_211849-g2jlw4oo/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If testing multiple lengths of training, add multiple #'s to epochs_to_test\n",
    "epochs_to_test = [250]\n",
    "\n",
    "#Loops through number of epochs\n",
    "for n in epochs_to_test: \n",
    "    \n",
    "    #Loops through files\n",
    "    for i in range(len(file_paths)):\n",
    "        train_file = file_paths[i] #train file path\n",
    "       \n",
    "        if i == 0: #for training on different batches at the same time\\\n",
    "            reference_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_02.csv'\n",
    "            test_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv'\n",
    "        else:\n",
    "            reference_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_02.csv'\n",
    "            test_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_max.csv'\n",
    "\n",
    "\n",
    "        ##----------------Filter particular runs (if needed)----------\n",
    "        #if num_images == 'max':\n",
    "\n",
    "        ##---------- Initilize new config .yml for new training file---------------\n",
    "        \n",
    "        #open config yaml to update experiment params\n",
    "        with open('../yml_and_csv_files/reid_template.yml', 'r') as fo:\n",
    "            config = yaml.safe_load(fo)\n",
    "        \n",
    "        #------------ Initilize and make new dir for each training set-----\n",
    "\n",
    "        # Saves wandb folder, checkpoints, and outputs to folder named based on run string, which should contain attrs that differntiate run\n",
    "        run_str = os.path.basename(train_file)[34:-4] #MAY NEED TO MODIFY BASED ON CSV NAME\n",
    "        run_dir_name = os.path.join(config['train_settings']['wandb_dir_path'],run_str+'/')\n",
    "        if not os.path.exists(run_dir_name):\n",
    "            os.mkdir(run_dir_name)\n",
    "        split_parts = run_str.rsplit('_', 1) #String parse csv name\n",
    "        if len(split_parts) > 1: # Check if there is at least one underscore in the string\n",
    "            # Get the substring after the last underscore\n",
    "            num_images = split_parts[1]\n",
    "            num_ids = split_parts[0][:2]\n",
    "        else:\n",
    "            # Handle the case where there are no underscores in the string\n",
    "            num_images = run_str \n",
    "        \n",
    "        # Saves wandb folder, checkpoints, and outputs to folder named based on run string, which should contain attrs that differntiate run\n",
    "        config['train_settings']['wandb_project_name'] = \"bioclip_layerwise_finetuning\"\n",
    "        wandb_name =  config['train_settings']['wandb_project_name']+run_str #set wandb_dir_name\n",
    "        \n",
    "        #Update params\n",
    "        config['model_settings']['num_labels']= num_ids\n",
    "        print('Num labels ',num_ids)\n",
    "\n",
    "        #Check if batch size needs to be updated\n",
    "        df = pd.read_csv(train_file)\n",
    "        if config['data_settings']['batch_size'] > len(df):\n",
    "            config['data_settings']['batch_size'] = len(df)\n",
    "            print('Updated batch to contain all Data. Size = ',len(df))\n",
    "        \n",
    "        #Check if print_k needs to be updated for small dataset\n",
    "        print_k = config['train_settings']['print_k']\n",
    "        if print_k > len(df)/config['data_settings']['batch_size']:\n",
    "            print_k = len(df)/config['data_settings']['batch_size']\n",
    "            config['train_settings']['print_k'] = print_k\n",
    "            print('Updating print_k to contain whole epoch. Num_batches =',print_k)\n",
    "        \n",
    "        #Testing a differnt num of epochs based on loop\n",
    "        config['train_settings']['num_epochs'] = n\n",
    "        config['train_settings']['gpu'] = 1\n",
    "\n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_file\n",
    "        config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "        #config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = test_file\n",
    "        config['data_settings']['datafiles']['valid']= ''\n",
    "        config['data_settings']['datafiles']['query']= test_file\n",
    "\n",
    "        #update Model path\n",
    "        config['model_settings']['model_path'] = os.path.join(run_dir_name,run_str+'.pth')\n",
    "\n",
    "        #update pickle_file to prevent being overwritten\n",
    "        pickle_file = os.path.join(run_dir_name,'results.pkl')\n",
    "        config['eval_settings']['pickle_file'] = pickle_file\n",
    "        csv_file = os.path.join(run_dir_name,\"results.csv\")\n",
    "        config['eval_settings']['results_file'] = csv_file\n",
    "\n",
    "        #update wandb_project_name\n",
    "        config['train_settings']['wandb_project_name'] = wandb_name\n",
    "        config['train_settings']['wandb_dir_path'] = run_dir_name #this should make a seperate wandb folder for runs\n",
    "\n",
    "        #save yml\n",
    "        new_yml_file = run_dir_name+run_str+'.yml'\n",
    "        with open(new_yml_file, 'w') as fo:\n",
    "                yaml.dump(config,fo)   \n",
    "\n",
    "        #---------- actually run training too--------------\n",
    "        !python pytorch_finetune_unfreeze_layerwise_and_eval_reid.py --config_file {new_yml_file}\n",
    "\n",
    "        # Save model to wandb file location to prevent overwriting\n",
    "        # new dir in wandb/ will be generated each training run\n",
    "        !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "        # read python results from pickle file, \n",
    "        with open(config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "            results = pickle.load(fi)  \n",
    "            \n",
    "        # Write out run summary to results tracking document\n",
    "       \n",
    "        results_dict =  {'run_str': run_str,\n",
    "                                            'wandb_id':results['wandb_id'],\n",
    "                                            'num_ids':num_ids,\n",
    "                                            'num_images_per_id':num_images,\n",
    "                                            'total_training_images':len(pd.read_csv(train_file))-(len(pd.read_csv(train_file))*config['data_settings']['percent_valid']),\n",
    "                                            'batch_size':config['data_settings']['batch_size'],\n",
    "                                            'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                            'train_loss':results['train_loss'],\n",
    "                                            'valid_loss':results['valid_loss'],\n",
    "                                            '1NN':results['1NN_acc'],\n",
    "                                            '3NN':results['3NN_acc'],\n",
    "                                            'training_file':train_file,\n",
    "                                            'reference_file':reference_file,\n",
    "                                            'query_file':test_file,\n",
    "                                            'start_time':results['start_time'],\n",
    "                                            'train_time':results['train_time'],\n",
    "                                            'unfreeze_epochs': results['unfreeze_epoch'],\n",
    "                                            'stop_epoch':results['stop_epoch']}\n",
    "        \n",
    "        if not os.path.exists(config['eval_settings']['results_file']):\n",
    "            !touch {config['eval_settings']['results_file']}\n",
    "            results_df = pd.DataFrame(results_dict,index=[0])\n",
    "            results_df.to_csv(config['eval_settings']['results_file'])\n",
    "        else:\n",
    "            #read df and append row\n",
    "            results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "            results_df.loc[len(results_df)] = results_dict\n",
    "            results_df.to_csv(config['eval_settings']['results_file'],index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc0b48",
   "metadata": {},
   "source": [
    "## If folder contains csvs from both batches: run below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06a4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_max.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_16.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_04.csv\n",
      "32 4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './reid_template.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(num_images, num_ids)\n\u001b[1;32m    102\u001b[0m  \u001b[38;5;66;03m#open config yaml to update experiment params\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./reid_template.yml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fo:\n\u001b[1;32m    104\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(fo)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#Update params\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#num labels should be taken from pandas because batch1!=batch2\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './reid_template.yml'"
     ]
    }
   ],
   "source": [
    "# if you want only train on batch1, add the string after the asterisk\n",
    "#here im training on 32 ids batch1\n",
    "directory_of_csvs ='/home/gsantiago/summer_bee_data/open_sets/new*04*batch1'\n",
    "\n",
    "sample_num_of_interest = \"sample_num_\"\n",
    "\n",
    "#this variable is for creating a folder that will contain the files we want\n",
    "type_of_train_wanted = '/home/gsantiago/ReID_model_training/new_auto_train_eval/models_trained/'\n",
    "#type_of_train_wanted = '/home/lmeyers/ReID_complete/few_shot_experiments/new_training_models/'\n",
    "\n",
    "#wandb_dir_name is the directory where eveything wandb saves or whatevah\n",
    "wandb_dir_name = \"/home/gsantiago/ReID_model_training/new_auto_train_eval/models_trained/\"\n",
    "\n",
    "results_pickle=\"/home/gsantiago/ReID_model_training/new_auto_train_eval/\"\n",
    "\n",
    "csv_for_results = \"/home/gsantiago/ReID_model_training/new_auto_train_eval/Few_shot_expirament_results_tracking.csv\"\n",
    "epochs_to_test = [1500]\n",
    "label_col = 'reID'\n",
    "gpu_id = '1'\n",
    "\n",
    "\n",
    "\n",
    "## optional!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "## REMOVE LATER\n",
    "\n",
    "\n",
    "non_trained_models=[\n",
    "\n",
    "# '/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_16.csv',\n",
    "\n",
    "'/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_32.csv',\n",
    "\n",
    "'/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_64.csv'\n",
    "\n",
    "]\n",
    "if not os.path.exists(type_of_train_wanted):\n",
    "    os.mkdir(type_of_train_wanted)\n",
    "\n",
    "\n",
    "\n",
    "for n in epochs_to_test:\n",
    "    #loops through every file that starts with that string\n",
    "    for split in glob(directory_of_csvs):\n",
    "        #print(split)\n",
    "        \n",
    "        #get number of batch to reference and test on the other\n",
    "        \n",
    "        batch = split.split('_')[-1]\n",
    "        #print(batch)\n",
    "        #print(os.path.dirname(split))\n",
    "        parent_dir = os.path.dirname(split)\n",
    "        \n",
    "        if batch == \"batch1\":\n",
    "            query_file = parent_dir +'/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_query_64_ids_batch2.csv'\n",
    "            reference_file = parent_dir +'/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_reference_64_ids_batch2.csv'\n",
    "            valid_file = parent_dir +'/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_valid_64_ids_batch2.csv'\n",
    "        else:\n",
    "            query_file = parent_dir +'/open_reference_query_testing_batch1/summer_bee_dataset_closed_test_bee_query_64_ids_batch1.csv'\n",
    "            reference_file = parent_dir +'/open_reference_query_testing_batch1/summer_bee_dataset_closed_test_bee_reference_64_ids_batch1.csv'\n",
    "            valid_file = parent_dir +'/open_reference_query_testing_batch1/summer_bee_dataset_closed_test_bee_valid_64_ids_batch1.csv'\n",
    "        split +='/*'+sample_num_of_interest+'*'\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #this loops through each directory and takes the csv with the sample num of interest\n",
    "        # if you want to loop through every sample num, sample_num should be \"sample_num\"\n",
    "        for csv in glob(split):\n",
    "            if csv not in non_trained_models:\n",
    "                print(csv)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            train_file = csv\n",
    "            wandb_name = os.path.basename(os.path.dirname(train_file))\n",
    "            run_str = os.path.basename(train_file)[:-4]\n",
    "            run_dir_name = type_of_train_wanted+run_str+'/'\n",
    "            #print(wandb_name)\n",
    "            if not os.path.exists(run_dir_name):\n",
    "                os.mkdir(run_dir_name)\n",
    "            #print(run_dir_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            split_parts = train_file.split('/')[-1].split('_')\n",
    "            #print(split_parts)\n",
    "            # Check if there is at least one underscore in the string\n",
    "            if len(split_parts) > 1:\n",
    "                # Get the substring after the last underscore\n",
    "                num_images = split_parts[-1].replace('.csv', '')\n",
    "                num_ids = split_parts[6]\n",
    "            else:\n",
    "                # Handle the case where there are no underscores in the string\n",
    "                num_images = run_str\n",
    "\n",
    "            print(num_images, num_ids)\n",
    "\n",
    "             #open config yaml to update experiment params\n",
    "            with open('./reid_template.yml', 'r') as fo:\n",
    "                config = yaml.safe_load(fo)\n",
    "\n",
    "            #Update params\n",
    "\n",
    "            #num labels should be taken from pandas because batch1!=batch2\n",
    "\n",
    "            df = pd.read_csv(train_file)\n",
    "            num_labels = df['ID'].nunique()\n",
    "\n",
    "            config['model_settings']['num_labels']= num_labels\n",
    "            \n",
    "            \n",
    "            #ubdating label_col\n",
    "            config['data_settings']['label_col']=  label_col\n",
    "            \n",
    "            #gpu\n",
    "            config['train_settings']['gpu'] = gpu_id\n",
    "            print('Num labels ',num_labels)\n",
    "\n",
    "            #Check if batch size needs to be updated\n",
    "\n",
    "            config['data_settings']['batch_size'] = 64\n",
    "            if config['data_settings']['batch_size'] > len(df):\n",
    "                config['data_settings']['batch_size'] = len(df)\n",
    "                print('Updated batch to contain all Data. Size = ',len(df))\n",
    "\n",
    "            #Check if print_k needs to be updated for small dataset\n",
    "            print_k = config['train_settings']['print_k']\n",
    "            if print_k > len(df)/config['data_settings']['batch_size']:\n",
    "                print_k = len(df)/config['data_settings']['batch_size']\n",
    "                config['train_settings']['print_k'] = print_k\n",
    "                print('Updating print_k to contain whole epoch. Num_batches =',print_k)\n",
    "\n",
    "            #Testing a differnt num epochs (EXPIRAMENT HERE)\n",
    "            config['train_settings']['num_epochs'] = n\n",
    "\n",
    "            #updating datafiles\n",
    "            config['data_settings']['datafiles']['train']=train_file\n",
    "            config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "\n",
    "            #config['data_settings']['datafiles']['train']=train_csv\n",
    "            config['data_settings']['datafiles']['test'] = query_file\n",
    "            config['data_settings']['datafiles']['valid']= valid_file \n",
    "            config['data_settings']['datafiles']['query']= query_file\n",
    "\n",
    "            #update Model path\n",
    "            config['model_settings']['model_path'] = run_dir_name+run_str+'.pth'\n",
    "\n",
    "            #update wandb_project_name\n",
    "            config['train_settings']['wandb_project_name'] = wandb_name\n",
    "            config['train_settings']['wandb_dir_path'] = wandb_dir_name + run_str #this should make a seperate wandb folder for runs\n",
    "            \n",
    "            #pickle_config_file\n",
    "            config['eval_settings']['pickle_file'] = results_pickle+ 'results.pkl'\n",
    "            config['eval_settings']['results_file']= csv_for_results\n",
    "            \n",
    "#             #\"\"\"\n",
    "#             #skipping max because I already ran it\n",
    "#             if num_images not in [64, '64'] and 'monocolor' not in run_str:\n",
    "#                 print('Skipping')\n",
    "#                 continue\n",
    "#             #\"\"\"\n",
    "\n",
    "            #save yml\n",
    "            new_yml_file = run_dir_name+run_str+'.yml'\n",
    "            with open(new_yml_file, 'w') as fo:\n",
    "                    yaml.dump(config,fo)   \n",
    "\n",
    "            #---------- actually run training too--------------\n",
    "            !python3 pytorch_train_and_eval_reid_2.py --config_file {new_yml_file}\n",
    "\n",
    "            # Save model to wandb file location to prevent overwriting\n",
    "            !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "            with open(config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "                results = pickle.load(fi)  \n",
    "\n",
    "            # Write out run summary to results tracking document\n",
    "            results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "            results_df.loc[len(results_df)] = {'run_str': run_str,\n",
    "                                                'wandb_id':results['wandb_id'],\n",
    "                                                'num_ids':num_ids,\n",
    "                                                'num_images_per_id':num_images,\n",
    "                                                'total_training_images':len(pd.read_csv(train_file)),\n",
    "                                                'batch_size':config['data_settings']['batch_size'],\n",
    "                                                'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                                'train_loss':results['train_loss'],\n",
    "                                                'valid_loss':results['valid_loss'],\n",
    "                                                '1NN':results['1NN_acc'],\n",
    "                                                '3NN':results['3NN_acc'],\n",
    "                                                'training_file':train_file,\n",
    "                                                'reference_file':reference_file,\n",
    "                                                'query_file':query_file,\n",
    "                                                'start_time':results['start_time'],\n",
    "                                                'train_time':results['train_time'],\n",
    "                                                'stop_epoch':results['stop_epoch']}\n",
    "            results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c200c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61825432",
   "metadata": {},
   "source": [
    "#### In event of run failure to record automatically\n",
    "Use below code to save run details from results.pickle even if there was an issue in your run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad9149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "    results = pickle.load(fi)   \n",
    "\n",
    "# Write out run summary to results tracking document\n",
    "results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "results_df.loc[len(results_df)] = {'run_str': run_str,\n",
    "                                    'wandb_id':results['wandb_id'],\n",
    "                                    'num_ids':num_ids,\n",
    "                                    'num_images_per_id':num_images,\n",
    "                                    'total_training_images':len(pd.read_csv(train_file)),\n",
    "                                    'batch_size':config['data_settings']['batch_size'],\n",
    "                                    'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                    'train_loss':results['train_loss'],\n",
    "                                    'valid_loss':results['valid_loss'],\n",
    "                                    '1NN':results['1NN_acc'],\n",
    "                                    '3NN':results['3NN_acc'],\n",
    "                                    'training_file':train_file,\n",
    "                                    'reference_file':reference_file,\n",
    "                                    'query_file':query_file,\n",
    "                                    'start_time':results['start_time'],\n",
    "                                    'train_time':results['train_time'],\n",
    "                                    'stop_epoch':results['stop_epoch']}\n",
    "results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
