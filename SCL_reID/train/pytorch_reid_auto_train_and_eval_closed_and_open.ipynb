{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df272772",
   "metadata": {},
   "source": [
    "# A Notebook to iterate through a set of training CSV's and train a SCL ReID model, evaluate model, and save results to a line in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7158d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Necessary Imports\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2caf0d4",
   "metadata": {},
   "source": [
    "## Correct filepaths in csvs (if needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569dcdfd",
   "metadata": {},
   "source": [
    "# Iterate through training files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd184456",
   "metadata": {},
   "source": [
    "### Specify Training and Evaluation Files \n",
    "This method iterates through a folder of training csvs and evaluates them on the same test (query) file using the same reference file each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34eba983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir containing training csvs to start working with \n",
    "dir = '/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2'\n",
    "\n",
    "test_file = '/home/gsantiago/summer_bee_data/closed_test_batch1/summer_bee_dataset_closed_test_bee_sample_num_None.csv'\n",
    "#test on all of batch 2 (open set) gonna also test on all of batch 1 \n",
    "\n",
    "valid_file = '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch1/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'\n",
    "# run valid on smaller subset of test_set to speed training \n",
    "\n",
    "#reference knn on smaller subset of batch 1\n",
    "reference_file = '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch1/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'\n",
    "\n",
    "\n",
    "results_file = '../yml_and_csv_files/Few_shot_expirament_results_tracking.csv'\n",
    "\n",
    "file_paths = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499f8a08",
   "metadata": {},
   "source": [
    "#### Get list of training files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36fe7c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2/summer_bee_dataset_closed_train_bee_4_ids_batch2_sample_num_4.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2/summer_bee_dataset_closed_train_bee_4_ids_batch2_sample_num_8.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2/summer_bee_dataset_closed_train_bee_4_ids_batch2_sample_num_16.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2/summer_bee_dataset_closed_train_bee_4_ids_batch2_sample_num_20.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2/summer_bee_dataset_closed_train_bee_4_ids_batch2_sample_num_max.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2/summer_bee_dataset_closed_train_bee_4_ids_batch2_sample_num_2.csv\n"
     ]
    }
   ],
   "source": [
    "# Get file list \n",
    "for root, dirs, files in os.walk(dir):\n",
    "    files = files\n",
    "for f in files:\n",
    "    print(root+r'/'+f)\n",
    "    train_file = root+r'/'+f\n",
    "    file_paths.append(train_file)\n",
    "#     continue\n",
    "\n",
    "#files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF setting two folders to train at once: \n",
    "\n",
    "# dir2 = '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2'\n",
    "# files2 = []\n",
    "# for root2, dirs2, files2 in os.walk(dir2):\n",
    "#     files2 = files2\n",
    "# for f2 in files2:\n",
    "#     #print(root+r'/'+f2)\n",
    "#     train_file2 = root2+r'/'+f2\n",
    "#     file_paths.append(train_file2)\n",
    "# #     continue\n",
    "\n",
    "# print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd93bbf0-e573-4ab6-a157-5b589c64eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375cea6",
   "metadata": {},
   "source": [
    "## Loop through files, train and eval model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c923c",
   "metadata": {},
   "source": [
    "Code may need to be modified for differnt training sets such as\n",
    "* run_str parsing to include important variables such as num_ids\n",
    "* the path to your template yml config file\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745415dc-6f87-4521-ae3a-f1492eb39356",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_max.csv']#,'/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cd4046-6108-476d-b80b-479a51a7e2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10264\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_paths[0])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3fe0480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels  64\n",
      "2024-10-23 11:17:03.349364: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-23 11:17:04.221365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "finished imports\n",
      "beginning execution\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlqmeyers\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/wandb/run-20241023_111707-icojrprj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomfy-bee-29\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/vit_finetuning64_ids_batch1_sample_num_max\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/vit_finetuning64_ids_batch1_sample_num_max/runs/icojrprj\u001b[0m\n",
      "Date and time when this experiment was started: 24-10-23 11:17\n",
      "Data Settings:\n",
      "{'aug_p': 0.3, 'batch_size': 64, 'crop_height': None, 'crop_left': None, 'crop_top': None, 'crop_width': None, 'cropped': False, 'datafiles': {'gallery': '/home/lmeyers/ReID_complete/summer_2023_reid_galleries_closed.csv', 'query': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv', 'reference': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_02.csv', 'test': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv', 'train': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_max.csv', 'valid': ''}, 'dataset': 'summer_2023', 'fname_col': 'new_filepath', 'gallery_id': 'gallery_id', 'image_id_col': 'image_id', 'input_size': [250, 250], 'iteration_id': 'iteration_id', 'label_col': 'ID', 'n_distractors': 9, 'percent_reference': 0.1, 'percent_valid': 0.2, 'sample_reference': True, 'sample_valid': True, 'split_type': 'closed'}\n",
      "Train Settings:\n",
      "{'checkpoint_to_load': 100, 'early_stop_consecutive_epochs': 50, 'early_stopping': True, 'early_stopping_metric': 'valid_loss', 'finetune_epochs': 25, 'gpu': 1, 'learning_rate': 0.001, 'margin': 0.2, 'num_epochs': 200, 'print_k': 10, 'resume_from_saved': True, 'save_checkpoint_freq': 50, 'wandb_dir_path': '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/', 'wandb_entity_name': 'meyers_luke_lab', 'wandb_project_name': 'vit_finetuning64_ids_batch1_sample_num_max', 'wandb_run_id': None}\n",
      "Model Settings:\n",
      "{'latent_dim': 128, 'model_class': 'vit_reid', 'model_path': '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/64_ids_batch1_sample_num_max.pth', 'num_labels': '64'}\n",
      "Using GPU 1\n",
      "Creating train and valid dataloaders...\n",
      "Using 2053 samples for validation set\n",
      "8211 total training samples\n",
      "initializing evaluation dataloaders...\n",
      "Using 1087 samples for reference set\n",
      "9779 total test samples\n",
      "Batch image shape: torch.Size([64, 3, 250, 250])\n",
      "Batch label shape: torch.Size([64])\n",
      "Building model....\n",
      "Resuming training from saved epoch: 100\n",
      "Loading saved model /home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/wandb/run-20241018_093254-5eva7juu/files/64_ids_batch1_sample_num_max.pth\n",
      "Loss: TripletMarginLoss(\n",
      "  (distance): CosineSimilarity()\n",
      "  (reducer): AvgNonZeroReducer()\n",
      ")\n",
      "Getting ViT feature extractor...\n",
      "Found device: cuda\n",
      "Training model head...\n",
      "[101,   129] train_loss: 0.9460 | val_loss: 0.0754\n",
      "evaluating on test set after 101 epochs\n",
      "using model from epoch 100 with loss: 0.07537106715608388\n",
      "Reference (or Train) Loss: 0.0890\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0868\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8987\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8883\n",
      "[102,   129] train_loss: 0.9596 | val_loss: 0.0810\n",
      "[103,   129] train_loss: 1.0117 | val_loss: 0.0796\n",
      "[104,   129] train_loss: 0.9892 | val_loss: 0.0786\n",
      "[105,   129] train_loss: 0.9907 | val_loss: 0.0823\n",
      "[106,   129] train_loss: 0.9216 | val_loss: 0.0713\n",
      "evaluating on test set after 106 epochs\n",
      "using model from epoch 105 with loss: 0.071314936154522\n",
      "Reference (or Train) Loss: 0.0908\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0871\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.891\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8784\n",
      "[107,   129] train_loss: 0.9403 | val_loss: 0.0732\n",
      "[108,   129] train_loss: 0.9551 | val_loss: 0.0755\n",
      "[109,   129] train_loss: 0.9368 | val_loss: 0.0774\n",
      "[110,   129] train_loss: 0.9334 | val_loss: 0.0800\n",
      "[111,   129] train_loss: 0.9259 | val_loss: 0.0800\n",
      "evaluating on test set after 111 epochs\n",
      "using model from epoch 105 with loss: 0.071314936154522\n",
      "Reference (or Train) Loss: 0.0934\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0892\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8234\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8115\n",
      "[112,   129] train_loss: 0.9683 | val_loss: 0.0742\n",
      "[113,   129] train_loss: 0.9474 | val_loss: 0.0781\n",
      "[114,   129] train_loss: 0.9062 | val_loss: 0.0747\n",
      "[115,   129] train_loss: 0.9402 | val_loss: 0.0800\n",
      "[116,   129] train_loss: 0.9351 | val_loss: 0.0735\n",
      "evaluating on test set after 116 epochs\n",
      "using model from epoch 105 with loss: 0.071314936154522\n",
      "Reference (or Train) Loss: 0.0942\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0883\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.894\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8781\n",
      "[117,   129] train_loss: 0.8983 | val_loss: 0.0780\n",
      "[118,   129] train_loss: 0.9441 | val_loss: 0.0742\n",
      "[119,   129] train_loss: 0.9631 | val_loss: 0.0701\n",
      "[121,   129] train_loss: 0.8711 | val_loss: 0.0813\n",
      "evaluating on test set after 121 epochs\n",
      "using model from epoch 118 with loss: 0.07013013045070693\n",
      "Reference (or Train) Loss: 0.0913\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0867\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8905\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8807\n",
      "[122,   129] train_loss: 0.8970 | val_loss: 0.0795\n",
      "[123,   129] train_loss: 0.9548 | val_loss: 0.0692\n",
      "[124,   129] train_loss: 0.9351 | val_loss: 0.0727\n",
      "[125,   129] train_loss: 0.9428 | val_loss: 0.0737\n",
      "[126,   129] train_loss: 0.8850 | val_loss: 0.0738\n",
      "evaluating on test set after 126 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0893\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0865\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8943\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8762\n",
      "[127,   129] train_loss: 0.9354 | val_loss: 0.0738\n",
      "[128,   129] train_loss: 1.0014 | val_loss: 0.0778\n",
      "[129,   129] train_loss: 0.9011 | val_loss: 0.0848\n",
      "[130,   129] train_loss: 1.0163 | val_loss: 0.0725\n",
      "[131,   129] train_loss: 0.9519 | val_loss: 0.0798\n",
      "evaluating on test set after 131 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0896\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0872\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8699\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8596\n",
      "[132,   129] train_loss: 0.9862 | val_loss: 0.0847\n",
      "[133,   129] train_loss: 0.9822 | val_loss: 0.0777\n",
      "[134,   129] train_loss: 1.0170 | val_loss: 0.0806\n",
      "[135,   129] train_loss: 0.9738 | val_loss: 0.0756\n",
      "[136,   129] train_loss: 0.9943 | val_loss: 0.0846\n",
      "evaluating on test set after 136 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0913\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0873\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8701\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8578\n",
      "[137,   129] train_loss: 1.0061 | val_loss: 0.0773\n",
      "[138,   129] train_loss: 1.0196 | val_loss: 0.0765\n",
      "[139,   129] train_loss: 0.9428 | val_loss: 0.0811\n",
      "[140,   129] train_loss: 0.9157 | val_loss: 0.0749\n",
      "[141,   129] train_loss: 0.9709 | val_loss: 0.0818\n",
      "evaluating on test set after 141 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0936\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0870\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8619\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8594\n",
      "[142,   129] train_loss: 0.9684 | val_loss: 0.0818\n",
      "[143,   129] train_loss: 1.0464 | val_loss: 0.0860\n",
      "[144,   129] train_loss: 1.0225 | val_loss: 0.0776\n",
      "[145,   129] train_loss: 0.9928 | val_loss: 0.0756\n",
      "[146,   129] train_loss: 0.9975 | val_loss: 0.0754\n",
      "evaluating on test set after 146 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0927\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0876\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8767\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8652\n",
      "[147,   129] train_loss: 0.9582 | val_loss: 0.0761\n",
      "[148,   129] train_loss: 1.0106 | val_loss: 0.0772\n",
      "[149,   129] train_loss: 0.9895 | val_loss: 0.0736\n",
      "[150,   129] train_loss: 0.9638 | val_loss: 0.0776\n",
      "Saving checkpoint 150\n",
      "[151,   129] train_loss: 0.9372 | val_loss: 0.0791\n",
      "evaluating on test set after 151 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0948\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0883\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8577\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8564\n",
      "[152,   129] train_loss: 0.9636 | val_loss: 0.0752\n",
      "[153,   129] train_loss: 0.9533 | val_loss: 0.0817\n",
      "[154,   129] train_loss: 1.0107 | val_loss: 0.0830\n",
      "[155,   129] train_loss: 1.0477 | val_loss: 0.0774\n",
      "[156,   129] train_loss: 0.9843 | val_loss: 0.0776\n",
      "evaluating on test set after 156 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0921\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0878\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8678\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8637\n",
      "[157,   129] train_loss: 1.0177 | val_loss: 0.0773\n",
      "[158,   129] train_loss: 0.9962 | val_loss: 0.0748\n",
      "[159,   129] train_loss: 0.9998 | val_loss: 0.0801\n",
      "[160,   129] train_loss: 0.9419 | val_loss: 0.0735\n",
      "[161,   129] train_loss: 0.9719 | val_loss: 0.0776\n",
      "evaluating on test set after 161 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0903\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0884\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8944\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8861\n",
      "[162,   129] train_loss: 0.9928 | val_loss: 0.0809\n",
      "[163,   129] train_loss: 1.0210 | val_loss: 0.0802\n",
      "[164,   129] train_loss: 1.0394 | val_loss: 0.0830\n",
      "[165,   129] train_loss: 1.0067 | val_loss: 0.0760\n",
      "[166,   129] train_loss: 1.0314 | val_loss: 0.0843\n",
      "evaluating on test set after 166 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0942\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0874\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8726\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8651\n",
      "[167,   129] train_loss: 0.9830 | val_loss: 0.0779\n",
      "[168,   129] train_loss: 0.9615 | val_loss: 0.0733\n",
      "[169,   129] train_loss: 0.9405 | val_loss: 0.0801\n",
      "[170,   129] train_loss: 0.9530 | val_loss: 0.0735\n",
      "[171,   129] train_loss: 0.9653 | val_loss: 0.0777\n",
      "evaluating on test set after 171 epochs\n",
      "using model from epoch 122 with loss: 0.06922103755641729\n",
      "Reference (or Train) Loss: 0.0921\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0870\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8725\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8615\n",
      "[172,   129] train_loss: 1.0166 | val_loss: 0.0723\n",
      "[173,   129] train_loss: 0.9749 | val_loss: 0.0886\n",
      "Early stopping at epoch 173 due to no improvement in valid_loss for 50 consecutive epochs\n",
      "using epoch 122 with loss 0.06922103755641729 for eval\n",
      "Total train time: 343.3093053380648min\n",
      "Evaluating model...\n",
      "generating embeddings\n",
      "Reference (or Train) Loss: 0.0938\n",
      "Reference size: (1087, 128)\n",
      "Test (or Query) Loss: 0.0886\n",
      "Test (or Query) size: (9779, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.8427\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.8228\n",
      "\n",
      "Per label 3NN test accuracy:\n",
      "65\t0.86\n",
      "66\t0.96\n",
      "67\t0.96\n",
      "68\t0.78\n",
      "69\t0.91\n",
      "70\t0.63\n",
      "71\t0.77\n",
      "72\t0.88\n",
      "73\t0.91\n",
      "74\t0.62\n",
      "75\t0.92\n",
      "76\t0.82\n",
      "77\t0.93\n",
      "78\t0.87\n",
      "79\t0.76\n",
      "80\t0.72\n",
      "81\t0.88\n",
      "82\t0.90\n",
      "83\t0.91\n",
      "84\t0.90\n",
      "85\t0.93\n",
      "86\t0.68\n",
      "87\t0.93\n",
      "88\t0.93\n",
      "89\t0.16\n",
      "90\t0.95\n",
      "91\t0.89\n",
      "92\t0.90\n",
      "93\t0.82\n",
      "94\t0.91\n",
      "95\t0.96\n",
      "96\t0.75\n",
      "97\t0.90\n",
      "98\t0.95\n",
      "99\t0.91\n",
      "100\t0.81\n",
      "101\t0.85\n",
      "102\t0.97\n",
      "103\t0.93\n",
      "104\t0.46\n",
      "105\t0.17\n",
      "106\t0.91\n",
      "107\t0.81\n",
      "108\t0.70\n",
      "109\t0.79\n",
      "110\t0.66\n",
      "111\t0.74\n",
      "112\t0.37\n",
      "113\t0.81\n",
      "114\t0.00\n",
      "115\t0.83\n",
      "116\t0.88\n",
      "117\t0.43\n",
      "118\t0.85\n",
      "119\t0.74\n",
      "120\t0.75\n",
      "121\t0.78\n",
      "122\t0.55\n",
      "123\t0.92\n",
      "124\t0.09\n",
      "125\t0.75\n",
      "126\t0.43\n",
      "127\t0.59\n",
      "128\t0.71\n",
      "\n",
      "Printing Confusion Matrix:\n",
      "[[160   1   0 ...   0   0   0]\n",
      " [  0  76   0 ...   0   0   0]\n",
      " [  0   0 214 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  29   0   0]\n",
      " [  0   0   0 ...   0  37   1]\n",
      " [  0   0   0 ...   0   0  70]]\n",
      "{'1NN_acc': 0.8427, '3NN_acc': 0.8228, 'label_list': array([ 65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]), 'knn_class': array([0.8556, 0.962 , 0.9596, 0.7768, 0.9134, 0.6341, 0.7734, 0.8838,\n",
      "       0.9102, 0.6154, 0.9167, 0.8214, 0.9341, 0.8658, 0.7625, 0.7216,\n",
      "       0.8779, 0.8986, 0.913 , 0.8966, 0.9286, 0.6765, 0.9297, 0.927 ,\n",
      "       0.1591, 0.9474, 0.8876, 0.8971, 0.8193, 0.9094, 0.9641, 0.7475,\n",
      "       0.9043, 0.9531, 0.9126, 0.8068, 0.8473, 0.9682, 0.9286, 0.4643,\n",
      "       0.1695, 0.9128, 0.8148, 0.7037, 0.7864, 0.6562, 0.7407, 0.3737,\n",
      "       0.8138, 0.    , 0.8284, 0.8764, 0.4268, 0.8495, 0.7416, 0.747 ,\n",
      "       0.7829, 0.5522, 0.9157, 0.0851, 0.7528, 0.4328, 0.5873, 0.7071]), 'knn_conf': array([[160,   1,   0, ...,   0,   0,   0],\n",
      "       [  0,  76,   0, ...,   0,   0,   0],\n",
      "       [  0,   0, 214, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,  29,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,  37,   1],\n",
      "       [  0,   0,   0, ...,   0,   0,  70]]), 'train_loss': 0.0}\n",
      "icojrprj\n",
      "Results saved to pickle file\n",
      "Saving model...\n",
      "Finished\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.005 MB of 0.015 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train loss ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: triplet_num ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  valid loss ‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñá‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       epoch 172\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train loss 0.11182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: triplet_num 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  valid loss 0.08858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mcomfy-bee-29\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/vit_finetuning64_ids_batch1_sample_num_max/runs/icojrprj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_max/wandb/run-20241023_111707-icojrprj/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If testing multiple lengths of training, add multiple #'s to epochs_to_test\n",
    "epochs_to_test = [200]\n",
    "\n",
    "#Loops through number of epochs\n",
    "for n in epochs_to_test: \n",
    "    \n",
    "    #Loops through files\n",
    "    for i in range(len(file_paths)):\n",
    "        train_file = file_paths[i] #train file path\n",
    "       \n",
    "        if i == 0: #for training on different batches at the same time\\\n",
    "            reference_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_02.csv'\n",
    "            test_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv'\n",
    "        else:\n",
    "            reference_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_02.csv'\n",
    "            test_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_max.csv'\n",
    "\n",
    "\n",
    "        ##----------------Filter particular runs (if needed)----------\n",
    "        #if num_images == 'max':\n",
    "\n",
    "        ##---------- Initilize new config .yml for new training file---------------\n",
    "        \n",
    "        #open config yaml to update experiment params\n",
    "        with open('../yml_and_csv_files/reid_template.yml', 'r') as fo:\n",
    "            config = yaml.safe_load(fo)\n",
    "        \n",
    "        #------------ Initilize and make new dir for each training set-----\n",
    "\n",
    "        # Saves wandb folder, checkpoints, and outputs to folder named based on run string, which should contain attrs that differntiate run\n",
    "        run_str = os.path.basename(train_file)[34:-4] #MAY NEED TO MODIFY BASED ON CSV NAME\n",
    "        run_dir_name = os.path.join(config['train_settings']['wandb_dir_path'],run_str+'/')\n",
    "        if not os.path.exists(run_dir_name):\n",
    "            os.mkdir(run_dir_name)\n",
    "        split_parts = run_str.rsplit('_', 1) #String parse csv name\n",
    "        if len(split_parts) > 1: # Check if there is at least one underscore in the string\n",
    "            # Get the substring after the last underscore\n",
    "            num_images = split_parts[1]\n",
    "            num_ids = split_parts[0][:2]\n",
    "        else:\n",
    "            # Handle the case where there are no underscores in the string\n",
    "            num_images = run_str \n",
    "        \n",
    "        # Saves wandb folder, checkpoints, and outputs to folder named based on run string, which should contain attrs that differntiate run\n",
    "        wandb_name =  config['train_settings']['wandb_project_name']+run_str #set wandb_dir_name\n",
    "        \n",
    "        #Update params\n",
    "        config['model_settings']['num_labels']= num_ids\n",
    "        print('Num labels ',num_ids)\n",
    "\n",
    "        #Check if batch size needs to be updated\n",
    "        df = pd.read_csv(train_file)\n",
    "        if config['data_settings']['batch_size'] > len(df):\n",
    "            config['data_settings']['batch_size'] = len(df)\n",
    "            print('Updated batch to contain all Data. Size = ',len(df))\n",
    "        \n",
    "        #Check if print_k needs to be updated for small dataset\n",
    "        print_k = config['train_settings']['print_k']\n",
    "        if print_k > len(df)/config['data_settings']['batch_size']:\n",
    "            print_k = len(df)/config['data_settings']['batch_size']\n",
    "            config['train_settings']['print_k'] = print_k\n",
    "            print('Updating print_k to contain whole epoch. Num_batches =',print_k)\n",
    "        \n",
    "        #Testing a differnt num of epochs based on loop\n",
    "        config['train_settings']['num_epochs'] = n\n",
    "\n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_file\n",
    "        config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "        #config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = test_file\n",
    "        config['data_settings']['datafiles']['valid']= ''\n",
    "        config['data_settings']['datafiles']['query']= test_file\n",
    "\n",
    "        #update Model path\n",
    "        config['model_settings']['model_path'] = os.path.join(run_dir_name,run_str+'.pth')\n",
    "\n",
    "        #update pickle_file to prevent being overwritten\n",
    "        pickle_file = os.path.join(run_dir_name,'results.pkl')\n",
    "        config['eval_settings']['pickle_file'] = pickle_file\n",
    "        csv_file = os.path.join(run_dir_name,\"results.csv\")\n",
    "        config['eval_settings']['results_file'] = csv_file\n",
    "\n",
    "        #update wandb_project_name\n",
    "        config['train_settings']['wandb_project_name'] = wandb_name\n",
    "        config['train_settings']['wandb_dir_path'] = run_dir_name #this should make a seperate wandb folder for runs\n",
    "\n",
    "        #save yml\n",
    "        new_yml_file = run_dir_name+run_str+'.yml'\n",
    "        with open(new_yml_file, 'w') as fo:\n",
    "                yaml.dump(config,fo)   \n",
    "\n",
    "        #---------- actually run training too--------------\n",
    "        !python pytorch_finetune_and_eval_reid.py --config_file {new_yml_file}\n",
    "\n",
    "        # Save model to wandb file location to prevent overwriting\n",
    "        # new dir in wandb/ will be generated each training run\n",
    "        !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "        # read python results from pickle file, \n",
    "        with open(config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "            results = pickle.load(fi)  \n",
    "            \n",
    "        # Write out run summary to results tracking document\n",
    "       \n",
    "        results_dict =  {'run_str': run_str,\n",
    "                                            'wandb_id':results['wandb_id'],\n",
    "                                            'num_ids':num_ids,\n",
    "                                            'num_images_per_id':num_images,\n",
    "                                            'total_training_images':len(pd.read_csv(train_file))-(len(pd.read_csv(train_file))*config['data_settings']['percent_valid']),\n",
    "                                            'batch_size':config['data_settings']['batch_size'],\n",
    "                                            'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                            'train_loss':results['train_loss'],\n",
    "                                            'valid_loss':results['valid_loss'],\n",
    "                                            '1NN':results['1NN_acc'],\n",
    "                                            '3NN':results['3NN_acc'],\n",
    "                                            'training_file':train_file,\n",
    "                                            'reference_file':reference_file,\n",
    "                                            'query_file':test_file,\n",
    "                                            'start_time':results['start_time'],\n",
    "                                            'train_time':results['train_time'],\n",
    "                                            'unfreeze_epochs': results['unfreeze_epoch'],\n",
    "                                            'stop_epoch':results['stop_epoch']}\n",
    "        \n",
    "        if not os.path.exists(config['eval_settings']['results_file']):\n",
    "            !touch {config['eval_settings']['results_file']}\n",
    "            results_df = pd.DataFrame(results_dict,index=[0])\n",
    "            results_df.to_csv(config['eval_settings']['results_file'])\n",
    "        else:\n",
    "            #read df and append row\n",
    "            results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "            results_df.loc[len(results_df)] = results_dict\n",
    "            results_df.to_csv(config['eval_settings']['results_file'],index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc0b48",
   "metadata": {},
   "source": [
    "## If folder contains csvs from both batches: run below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06a4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_max.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_16.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_04.csv\n",
      "32 4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './reid_template.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(num_images, num_ids)\n\u001b[1;32m    102\u001b[0m  \u001b[38;5;66;03m#open config yaml to update experiment params\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./reid_template.yml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fo:\n\u001b[1;32m    104\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(fo)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#Update params\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#num labels should be taken from pandas because batch1!=batch2\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './reid_template.yml'"
     ]
    }
   ],
   "source": [
    "# if you want only train on batch1, add the string after the asterisk\n",
    "#here im training on 32 ids batch1\n",
    "directory_of_csvs ='/home/gsantiago/summer_bee_data/open_sets/new*04*batch1'\n",
    "\n",
    "sample_num_of_interest = \"sample_num_\"\n",
    "\n",
    "#this variable is for creating a folder that will contain the files we want\n",
    "type_of_train_wanted = '/home/gsantiago/ReID_model_training/new_auto_train_eval/models_trained/'\n",
    "#type_of_train_wanted = '/home/lmeyers/ReID_complete/few_shot_experiments/new_training_models/'\n",
    "\n",
    "#wandb_dir_name is the directory where eveything wandb saves or whatevah\n",
    "wandb_dir_name = \"/home/gsantiago/ReID_model_training/new_auto_train_eval/models_trained/\"\n",
    "\n",
    "results_pickle=\"/home/gsantiago/ReID_model_training/new_auto_train_eval/\"\n",
    "\n",
    "csv_for_results = \"/home/gsantiago/ReID_model_training/new_auto_train_eval/Few_shot_expirament_results_tracking.csv\"\n",
    "epochs_to_test = [1500]\n",
    "label_col = 'reID'\n",
    "gpu_id = '1'\n",
    "\n",
    "\n",
    "\n",
    "## optional!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "## REMOVE LATER\n",
    "\n",
    "\n",
    "non_trained_models=[\n",
    "\n",
    "# '/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_16.csv',\n",
    "\n",
    "'/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_32.csv',\n",
    "\n",
    "'/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_64.csv'\n",
    "\n",
    "]\n",
    "if not os.path.exists(type_of_train_wanted):\n",
    "    os.mkdir(type_of_train_wanted)\n",
    "\n",
    "\n",
    "\n",
    "for n in epochs_to_test:\n",
    "    #loops through every file that starts with that string\n",
    "    for split in glob(directory_of_csvs):\n",
    "        #print(split)\n",
    "        \n",
    "        #get number of batch to reference and test on the other\n",
    "        \n",
    "        batch = split.split('_')[-1]\n",
    "        #print(batch)\n",
    "        #print(os.path.dirname(split))\n",
    "        parent_dir = os.path.dirname(split)\n",
    "        \n",
    "        if batch == \"batch1\":\n",
    "            query_file = parent_dir +'/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_query_64_ids_batch2.csv'\n",
    "            reference_file = parent_dir +'/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_reference_64_ids_batch2.csv'\n",
    "            valid_file = parent_dir +'/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_valid_64_ids_batch2.csv'\n",
    "        else:\n",
    "            query_file = parent_dir +'/open_reference_query_testing_batch1/summer_bee_dataset_closed_test_bee_query_64_ids_batch1.csv'\n",
    "            reference_file = parent_dir +'/open_reference_query_testing_batch1/summer_bee_dataset_closed_test_bee_reference_64_ids_batch1.csv'\n",
    "            valid_file = parent_dir +'/open_reference_query_testing_batch1/summer_bee_dataset_closed_test_bee_valid_64_ids_batch1.csv'\n",
    "        split +='/*'+sample_num_of_interest+'*'\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #this loops through each directory and takes the csv with the sample num of interest\n",
    "        # if you want to loop through every sample num, sample_num should be \"sample_num\"\n",
    "        for csv in glob(split):\n",
    "            if csv not in non_trained_models:\n",
    "                print(csv)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            train_file = csv\n",
    "            wandb_name = os.path.basename(os.path.dirname(train_file))\n",
    "            run_str = os.path.basename(train_file)[:-4]\n",
    "            run_dir_name = type_of_train_wanted+run_str+'/'\n",
    "            #print(wandb_name)\n",
    "            if not os.path.exists(run_dir_name):\n",
    "                os.mkdir(run_dir_name)\n",
    "            #print(run_dir_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            split_parts = train_file.split('/')[-1].split('_')\n",
    "            #print(split_parts)\n",
    "            # Check if there is at least one underscore in the string\n",
    "            if len(split_parts) > 1:\n",
    "                # Get the substring after the last underscore\n",
    "                num_images = split_parts[-1].replace('.csv', '')\n",
    "                num_ids = split_parts[6]\n",
    "            else:\n",
    "                # Handle the case where there are no underscores in the string\n",
    "                num_images = run_str\n",
    "\n",
    "            print(num_images, num_ids)\n",
    "\n",
    "             #open config yaml to update experiment params\n",
    "            with open('./reid_template.yml', 'r') as fo:\n",
    "                config = yaml.safe_load(fo)\n",
    "\n",
    "            #Update params\n",
    "\n",
    "            #num labels should be taken from pandas because batch1!=batch2\n",
    "\n",
    "            df = pd.read_csv(train_file)\n",
    "            num_labels = df['ID'].nunique()\n",
    "\n",
    "            config['model_settings']['num_labels']= num_labels\n",
    "            \n",
    "            \n",
    "            #ubdating label_col\n",
    "            config['data_settings']['label_col']=  label_col\n",
    "            \n",
    "            #gpu\n",
    "            config['train_settings']['gpu'] = gpu_id\n",
    "            print('Num labels ',num_labels)\n",
    "\n",
    "            #Check if batch size needs to be updated\n",
    "\n",
    "            config['data_settings']['batch_size'] = 64\n",
    "            if config['data_settings']['batch_size'] > len(df):\n",
    "                config['data_settings']['batch_size'] = len(df)\n",
    "                print('Updated batch to contain all Data. Size = ',len(df))\n",
    "\n",
    "            #Check if print_k needs to be updated for small dataset\n",
    "            print_k = config['train_settings']['print_k']\n",
    "            if print_k > len(df)/config['data_settings']['batch_size']:\n",
    "                print_k = len(df)/config['data_settings']['batch_size']\n",
    "                config['train_settings']['print_k'] = print_k\n",
    "                print('Updating print_k to contain whole epoch. Num_batches =',print_k)\n",
    "\n",
    "            #Testing a differnt num epochs (EXPIRAMENT HERE)\n",
    "            config['train_settings']['num_epochs'] = n\n",
    "\n",
    "            #updating datafiles\n",
    "            config['data_settings']['datafiles']['train']=train_file\n",
    "            config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "\n",
    "            #config['data_settings']['datafiles']['train']=train_csv\n",
    "            config['data_settings']['datafiles']['test'] = query_file\n",
    "            config['data_settings']['datafiles']['valid']= valid_file \n",
    "            config['data_settings']['datafiles']['query']= query_file\n",
    "\n",
    "            #update Model path\n",
    "            config['model_settings']['model_path'] = run_dir_name+run_str+'.pth'\n",
    "\n",
    "            #update wandb_project_name\n",
    "            config['train_settings']['wandb_project_name'] = wandb_name\n",
    "            config['train_settings']['wandb_dir_path'] = wandb_dir_name + run_str #this should make a seperate wandb folder for runs\n",
    "            \n",
    "            #pickle_config_file\n",
    "            config['eval_settings']['pickle_file'] = results_pickle+ 'results.pkl'\n",
    "            config['eval_settings']['results_file']= csv_for_results\n",
    "            \n",
    "#             #\"\"\"\n",
    "#             #skipping max because I already ran it\n",
    "#             if num_images not in [64, '64'] and 'monocolor' not in run_str:\n",
    "#                 print('Skipping')\n",
    "#                 continue\n",
    "#             #\"\"\"\n",
    "\n",
    "            #save yml\n",
    "            new_yml_file = run_dir_name+run_str+'.yml'\n",
    "            with open(new_yml_file, 'w') as fo:\n",
    "                    yaml.dump(config,fo)   \n",
    "\n",
    "            #---------- actually run training too--------------\n",
    "            !python3 pytorch_train_and_eval_reid_2.py --config_file {new_yml_file}\n",
    "\n",
    "            # Save model to wandb file location to prevent overwriting\n",
    "            !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "            with open(config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "                results = pickle.load(fi)  \n",
    "\n",
    "            # Write out run summary to results tracking document\n",
    "            results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "            results_df.loc[len(results_df)] = {'run_str': run_str,\n",
    "                                                'wandb_id':results['wandb_id'],\n",
    "                                                'num_ids':num_ids,\n",
    "                                                'num_images_per_id':num_images,\n",
    "                                                'total_training_images':len(pd.read_csv(train_file)),\n",
    "                                                'batch_size':config['data_settings']['batch_size'],\n",
    "                                                'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                                'train_loss':results['train_loss'],\n",
    "                                                'valid_loss':results['valid_loss'],\n",
    "                                                '1NN':results['1NN_acc'],\n",
    "                                                '3NN':results['3NN_acc'],\n",
    "                                                'training_file':train_file,\n",
    "                                                'reference_file':reference_file,\n",
    "                                                'query_file':query_file,\n",
    "                                                'start_time':results['start_time'],\n",
    "                                                'train_time':results['train_time'],\n",
    "                                                'stop_epoch':results['stop_epoch']}\n",
    "            results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c200c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61825432",
   "metadata": {},
   "source": [
    "#### In event of run failure to record automatically\n",
    "Use below code to save run details from results.pickle even if there was an issue in your run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad9149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "    results = pickle.load(fi)   \n",
    "\n",
    "# Write out run summary to results tracking document\n",
    "results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "results_df.loc[len(results_df)] = {'run_str': run_str,\n",
    "                                    'wandb_id':results['wandb_id'],\n",
    "                                    'num_ids':num_ids,\n",
    "                                    'num_images_per_id':num_images,\n",
    "                                    'total_training_images':len(pd.read_csv(train_file)),\n",
    "                                    'batch_size':config['data_settings']['batch_size'],\n",
    "                                    'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                    'train_loss':results['train_loss'],\n",
    "                                    'valid_loss':results['valid_loss'],\n",
    "                                    '1NN':results['1NN_acc'],\n",
    "                                    '3NN':results['3NN_acc'],\n",
    "                                    'training_file':train_file,\n",
    "                                    'reference_file':reference_file,\n",
    "                                    'query_file':query_file,\n",
    "                                    'start_time':results['start_time'],\n",
    "                                    'train_time':results['train_time'],\n",
    "                                    'stop_epoch':results['stop_epoch']}\n",
    "results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
