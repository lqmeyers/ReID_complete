{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958be40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbce334-e33e-4aba-9098-03ed18210fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-18 12:23:33.449328: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-18 12:23:34.196123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished imports\n",
      "beginning execution\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/home/lmeyers/ReID_complete/SCL_reID/utils/\")\n",
    "\n",
    "from pytorch_eval_reid import eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3f121",
   "metadata": {},
   "source": [
    "## Reference train set, query test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5c61e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3318361773.py, line 135)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 135\u001b[0;36m\u001b[0m\n\u001b[0;31m    if not os.path.exists(config['eval_settings']['results_file']):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "directory_of_models_trained = '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/checkpoints/'\n",
    "# directory_for_ymls = './new_configs_reference_train_query_test/'\n",
    "# directory_for_jsons = './jsons_for_results/'\n",
    "yml = \"/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/64_ids_batch1_sample_num_64.yml\"\n",
    "\n",
    "gpu_id = '1'\n",
    "# if(not os.path.exists(directory_for_ymls)):\n",
    "#     os.mkdir(directory_for_ymls)\n",
    "    \n",
    "# if(not os.path.exists(directory_for_jsons)):\n",
    "#     os.mkdir(directory_for_jsons)\n",
    "\n",
    "\n",
    "for model_file in glob(directory_of_models_trained+\"*.pth\"):\n",
    "\n",
    "#         reference_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_02.csv'\n",
    "#         test_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv'\n",
    "#         # else:\n",
    "#         #     reference_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_02.csv'\n",
    "#         #     test_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_max.csv'\n",
    "\n",
    "\n",
    "#         ##----------------Filter particular runs (if needed)----------\n",
    "#         #if num_images == 'max':\n",
    "\n",
    "#         ##---------- create a temp yml to eval all checkpoints---------------\n",
    "        \n",
    "    #open config yaml to update experiment params\n",
    "    with open(yml, 'r') as fo:\n",
    "        config = yaml.safe_load(fo)\n",
    "    \n",
    "    model_name = os.path.basename(model_file)\n",
    "    run_dir_name = os.path.dirname(model_file)\n",
    "    num_epochs = model_name[:-4]\n",
    "    print(num_epochs)\n",
    "#         #------------ Initilize and make new dir for each training set-----\n",
    "    train_file = config['data_settings'][\"datafiles\"][\"train\"]\n",
    "#         # Saves wandb folder, checkpoints, and outputs to folder named based on run string, which should contain attrs that differntiate run\n",
    "    run_str = os.path.basename(train_file)[34:-4] #MAY NEED TO MODIFY BASED ON CSV NAME\n",
    "#         run_dir_name = os.path.join(config['train_settings']['wandb_dir_path'],run_str+'/')\n",
    "#         if not os.path.exists(run_dir_name):\n",
    "#             os.mkdir(run_dir_name)\n",
    "#         split_parts = run_str.rsplit('_', 1) #String parse csv name\n",
    "#         if len(split_parts) > 1: # Check if there is at least one underscore in the string\n",
    "#             # Get the substring after the last underscore\n",
    "#             num_images = split_parts[1]\n",
    "#             num_ids = split_parts[0][:2]\n",
    "#         else:\n",
    "#             # Handle the case where there are no underscores in the string\n",
    "#             num_images = run_str \n",
    "        \n",
    "#         # Saves wandb folder, checkpoints, and outputs to folder named based on run string, which should contain attrs that differntiate run\n",
    "    wandb_name =  config['train_settings']['wandb_project_name']+run_str #set wandb_dir_name\n",
    "        \n",
    "#         #Update params\n",
    "#         config['model_settings']['num_labels']= num_ids\n",
    "#         print('Num labels ',num_ids)\n",
    "\n",
    "#         #Check if batch size needs to be updated\n",
    "#         df = pd.read_csv(train_file)\n",
    "#         if config['data_settings']['batch_size'] > len(df):\n",
    "#             config['data_settings']['batch_size'] = len(df)\n",
    "#             print('Updated batch to contain all Data. Size = ',len(df))\n",
    "        \n",
    "#         #Check if print_k needs to be updated for small dataset\n",
    "#         print_k = config['train_settings']['print_k']\n",
    "#         if print_k > len(df)/config['data_settings']['batch_size']:\n",
    "#             print_k = len(df)/config['data_settings']['batch_size']\n",
    "#             config['train_settings']['print_k'] = print_k\n",
    "#             print('Updating print_k to contain whole epoch. Num_batches =',print_k)\n",
    "        \n",
    "    #Testing a differnt num of epochs based on loop\n",
    "    config['train_settings']['num_epochs'] = num_epochs\n",
    "\n",
    "#         #updating datafiles\n",
    "#         config['data_settings']['datafiles']['train']=train_file\n",
    "#         config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "#         #config['data_settings']['datafiles']['train']=train_csv\n",
    "#         config['data_settings']['datafiles']['test'] = test_file\n",
    "#         config['data_settings']['datafiles']['valid']= ''\n",
    "#         config['data_settings']['datafiles']['query']= test_file\n",
    "\n",
    "    #update Model path\n",
    "    config['model_settings']['model_path'] = model_file\n",
    "\n",
    "#         #update pickle_file to prevent being overwritten\n",
    "    pickle_file = os.path.join(run_dir_name,'results.pkl')\n",
    "    config['eval_settings']['pickle_file'] = pickle_file\n",
    "    csv_file = os.path.join(run_dir_name,\"results.csv\")\n",
    "    config['eval_settings']['results_file'] = csv_file\n",
    "\n",
    "#         #update wandb_project_name\n",
    "#         config['train_settings']['wandb_project_name'] = wandb_name\n",
    "#         config['train_settings']['wandb_dir_path'] = run_dir_name #this should make a seperate wandb folder for runs\n",
    "\n",
    "#         #save yml\n",
    "    new_yml_file = os.path.join(run_dir_name,\"checkpoint_eval.yml\")\n",
    "   \n",
    "    with open(new_yml_file, 'w') as fo:\n",
    "            yaml.dump(config,fo)   \n",
    "        \n",
    "\n",
    "    #---------- actually run eval--------------\n",
    "    # !python pytorch_train_and_eval_reid_2.py --config_file {new_yml_file}\n",
    "    eval(new_yml_file)\n",
    "\n",
    "#         # Save model to wandb file location to prevent overwriting\n",
    "#         # new dir in wandb/ will be generated each training run\n",
    "#         !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "    # read python results from pickle file, \n",
    "    with open(config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "        results = pickle.load(fi)  \n",
    "\n",
    "    # Write out run summary to results tracking document\n",
    "\n",
    "    results_dict =  {'run_str': run_str,\n",
    "                                        # 'wandb_id':results['wandb_id'],\n",
    "                                        'num_ids':num_ids,\n",
    "                                        'num_images_per_id':num_images,\n",
    "                                        'total_training_images':len(pd.read_csv(train_file))-(len(pd.read_csv(train_file))*config['data_settings']['percent_valid']),\n",
    "                                        'batch_size':config['data_settings']['batch_size'],\n",
    "                                        'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                        # 'train_loss':results['train_loss'],\n",
    "                                        # 'valid_loss':results['valid_loss'],\n",
    "                                        '1NN':results['1NN_acc'],\n",
    "                                        '3NN':results['3NN_acc'],\n",
    "                                        'training_file':train_file,\n",
    "                                        'query_file':config['data_settings'][\"datafiles\"][\"test\"],\n",
    "                                        # 'start_time':results['start_time'],\n",
    "                                        # 'train_time':results['train_time'],\n",
    "                                        # 'stop_epoch':results['stop_epoch']}\n",
    "                    }\n",
    "\n",
    "    if not os.path.exists(config['eval_settings']['results_file']):\n",
    "        !touch {config['eval_settings']['results_file']}\n",
    "        results_df = pd.DataFrame(results_dict,index=[0])\n",
    "        results_df.to_csv(config['eval_settings']['results_file'])\n",
    "    else:\n",
    "        #read df and append row\n",
    "        results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "        results_df.loc[len(results_df)] = results_dict\n",
    "        results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a569fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1NN_acc\n",
      "3NN_acc\n",
      "label_list\n",
      "knn_class\n",
      "knn_conf\n"
     ]
    }
   ],
   "source": [
    " with open(yml_config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "        results = pickle.load(fi)  \n",
    "        \n",
    "    \n",
    "# Convert ndarray to list\n",
    "\n",
    "for key in results.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c094d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
