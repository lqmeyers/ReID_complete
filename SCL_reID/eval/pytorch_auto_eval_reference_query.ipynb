{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958be40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbce334-e33e-4aba-9098-03ed18210fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-18 12:29:25.341567: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-18 12:29:26.083149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished imports\n",
      "beginning execution\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/home/lmeyers/ReID_complete/SCL_reID/utils/\")\n",
    "\n",
    "from pytorch_eval_reid import eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3f121",
   "metadata": {},
   "source": [
    "## Reference train set, query test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5c61e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "Date and time when this experiment was started: 24-09-18 12:29\n",
      "Data Settings:\n",
      "{'aug_p': 0.3, 'batch_size': 64, 'crop_height': None, 'crop_left': None, 'crop_top': None, 'crop_width': None, 'cropped': False, 'datafiles': {'gallery': '/home/lmeyers/ReID_complete/summer_2023_reid_galleries_closed.csv', 'query': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv', 'reference': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_02.csv', 'test': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv', 'train': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_64.csv', 'valid': ''}, 'dataset': 'summer_2023', 'fname_col': 'new_filepath', 'gallery_id': 'gallery_id', 'image_id_col': 'image_id', 'input_size': [250, 250], 'iteration_id': 'iteration_id', 'label_col': 'ID', 'n_distractors': 9, 'percent_valid': 0.2, 'sample_reference': True, 'sample_valid': True, 'split_type': 'closed'}\n",
      "Model Settings:\n",
      "{'latent_dim': 128, 'model_class': 'vit_reid', 'model_path': '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/checkpoints/1400.pth', 'num_labels': '64'}\n",
      "Using GPU 1\n",
      "Building model.... /home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/checkpoints/1400.pth\n",
      "Loss: TripletMarginLoss(\n",
      "  (distance): CosineSimilarity()\n",
      "  (reducer): AvgNonZeroReducer()\n",
      ")\n",
      "Getting ViT feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found device: cuda\n",
      "Evaluating model...\n",
      "Using 2173 samples for reference set\n",
      "8693 total test samples\n",
      "generating embeddings\n",
      "Reference (or Train) Loss: 0.2061\n",
      "Reference size: (2173, 128)\n",
      "Test (or Query) Loss: 0.2015\n",
      "Test (or Query) size: (8693, 128)\n",
      "Training kNN classifier with k=1\n",
      "1NN test accuracy: 0.0276\n",
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.0298\n",
      "\n",
      "Per label 3NN test accuracy:\n",
      "65\t0.09\n",
      "66\t0.00\n",
      "67\t0.03\n",
      "68\t0.02\n",
      "69\t0.40\n",
      "70\t0.00\n",
      "71\t0.00\n",
      "72\t0.00\n",
      "73\t0.04\n",
      "74\t0.00\n",
      "75\t0.01\n",
      "76\t0.00\n",
      "77\t0.00\n",
      "78\t0.03\n",
      "79\t0.00\n",
      "80\t0.11\n",
      "81\t0.11\n",
      "82\t0.00\n",
      "83\t0.01\n",
      "84\t0.00\n",
      "85\t0.00\n",
      "86\t0.00\n",
      "87\t0.10\n",
      "88\t0.00\n",
      "89\t0.00\n",
      "90\t0.10\n",
      "91\t0.00\n",
      "92\t0.00\n",
      "93\t0.02\n",
      "94\t0.00\n",
      "95\t0.00\n",
      "96\t0.00\n",
      "97\t0.00\n",
      "98\t0.00\n",
      "99\t0.00\n",
      "100\t0.00\n",
      "101\t0.01\n",
      "102\t0.00\n",
      "103\t0.00\n",
      "104\t0.00\n",
      "105\t0.00\n",
      "106\t0.00\n",
      "107\t0.00\n",
      "108\t0.00\n",
      "109\t0.00\n",
      "110\t0.00\n",
      "111\t0.00\n",
      "112\t0.00\n",
      "113\t0.00\n",
      "114\t0.00\n",
      "115\t0.00\n",
      "116\t0.00\n",
      "117\t0.00\n",
      "118\t0.00\n",
      "119\t0.00\n",
      "120\t0.00\n",
      "121\t0.00\n",
      "122\t0.00\n",
      "123\t0.00\n",
      "124\t0.00\n",
      "125\t0.00\n",
      "126\t0.00\n",
      "127\t0.00\n",
      "128\t0.00\n",
      "\n",
      "Printing Confusion Matrix:\n",
      "[[15  1  9 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [10  3  7 ...  0  0  0]\n",
      " ...\n",
      " [ 2  0  1 ...  0  0  0]\n",
      " [11  0  6 ...  0  0  0]\n",
      " [ 5  1  3 ...  0  0  0]]\n",
      "Total eval time: 2.7057247002919516min\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m\n\u001b[1;32m    114\u001b[0m     results \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fi)  \n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Write out run summary to results tracking document\u001b[39;00m\n\u001b[1;32m    118\u001b[0m results_dict \u001b[38;5;241m=\u001b[39m  {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_str\u001b[39m\u001b[38;5;124m'\u001b[39m: run_str,\n\u001b[1;32m    119\u001b[0m                                     \u001b[38;5;66;03m# 'wandb_id':results['wandb_id'],\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_ids\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mnum_ids\u001b[49m,\n\u001b[1;32m    121\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_images_per_id\u001b[39m\u001b[38;5;124m'\u001b[39m:num_images,\n\u001b[1;32m    122\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_training_images\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mlen\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(train_file))\u001b[38;5;241m-\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(train_file))\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_settings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercent_valid\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    123\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m:config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_settings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    124\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m:config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_settings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    125\u001b[0m                                     \u001b[38;5;66;03m# 'train_loss':results['train_loss'],\u001b[39;00m\n\u001b[1;32m    126\u001b[0m                                     \u001b[38;5;66;03m# 'valid_loss':results['valid_loss'],\u001b[39;00m\n\u001b[1;32m    127\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1NN\u001b[39m\u001b[38;5;124m'\u001b[39m:results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1NN_acc\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    128\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3NN\u001b[39m\u001b[38;5;124m'\u001b[39m:results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3NN_acc\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    129\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_file\u001b[39m\u001b[38;5;124m'\u001b[39m:train_file,\n\u001b[1;32m    130\u001b[0m                                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_file\u001b[39m\u001b[38;5;124m'\u001b[39m:config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_settings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatafiles\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    131\u001b[0m                                     \u001b[38;5;66;03m# 'start_time':results['start_time'],\u001b[39;00m\n\u001b[1;32m    132\u001b[0m                                     \u001b[38;5;66;03m# 'train_time':results['train_time'],\u001b[39;00m\n\u001b[1;32m    133\u001b[0m                                     \u001b[38;5;66;03m# 'stop_epoch':results['stop_epoch']}\u001b[39;00m\n\u001b[1;32m    134\u001b[0m                 }\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_settings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults_file\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    137\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtouch \u001b[39m\u001b[38;5;132;01m{config['eval_settings']['results_file']}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_ids' is not defined"
     ]
    }
   ],
   "source": [
    "directory_of_models_trained = '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/checkpoints/'\n",
    "# directory_for_ymls = './new_configs_reference_train_query_test/'\n",
    "# directory_for_jsons = './jsons_for_results/'\n",
    "yml = \"/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/64_ids_batch1_sample_num_64.yml\"\n",
    "\n",
    "gpu_id = '1'\n",
    "# if(not os.path.exists(directory_for_ymls)):\n",
    "#     os.mkdir(directory_for_ymls)\n",
    "    \n",
    "# if(not os.path.exists(directory_for_jsons)):\n",
    "#     os.mkdir(directory_for_jsons)\n",
    "\n",
    "\n",
    "for model_file in glob(directory_of_models_trained+\"*.pth\"):\n",
    "\n",
    "#         reference_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_02.csv'\n",
    "#         test_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv'\n",
    "#         # else:\n",
    "#         #     reference_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_02.csv'\n",
    "#         #     test_file = '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_max.csv'\n",
    "\n",
    "\n",
    "#         ##----------------Filter particular runs (if needed)----------\n",
    "#         #if num_images == 'max':\n",
    "\n",
    "#         ##---------- create a temp yml to eval all checkpoints---------------\n",
    "        \n",
    "    #open config yaml to update experiment params\n",
    "    with open(yml, 'r') as fo:\n",
    "        config = yaml.safe_load(fo)\n",
    "    \n",
    "    model_name = os.path.basename(model_file)\n",
    "    run_dir_name = os.path.dirname(model_file)\n",
    "    num_epochs = model_name[:-4]\n",
    "    print(num_epochs)\n",
    "#         #------------ Initilize and make new dir for each training set-----\n",
    "    train_file = config['data_settings'][\"datafiles\"][\"train\"]\n",
    "#         # Saves wandb folder, checkpoints, and outputs to folder named based on run string, which should contain attrs that differntiate run\n",
    "    run_str = os.path.basename(train_file)[34:-4] #MAY NEED TO MODIFY BASED ON CSV NAME\n",
    "#         run_dir_name = os.path.join(config['train_settings']['wandb_dir_path'],run_str+'/')\n",
    "#         if not os.path.exists(run_dir_name):\n",
    "#             os.mkdir(run_dir_name)\n",
    "#         split_parts = run_str.rsplit('_', 1) #String parse csv name\n",
    "#         if len(split_parts) > 1: # Check if there is at least one underscore in the string\n",
    "#             # Get the substring after the last underscore\n",
    "#             num_images = split_parts[1]\n",
    "#             num_ids = split_parts[0][:2]\n",
    "#         else:\n",
    "#             # Handle the case where there are no underscores in the string\n",
    "#             num_images = run_str \n",
    "        \n",
    "#         # Saves wandb folder, checkpoints, and outputs to folder named based on run string, which should contain attrs that differntiate run\n",
    "    wandb_name =  config['train_settings']['wandb_project_name']+run_str #set wandb_dir_name\n",
    "        \n",
    "#         #Update params\n",
    "#         config['model_settings']['num_labels']= num_ids\n",
    "#         print('Num labels ',num_ids)\n",
    "\n",
    "#         #Check if batch size needs to be updated\n",
    "#         df = pd.read_csv(train_file)\n",
    "#         if config['data_settings']['batch_size'] > len(df):\n",
    "#             config['data_settings']['batch_size'] = len(df)\n",
    "#             print('Updated batch to contain all Data. Size = ',len(df))\n",
    "        \n",
    "#         #Check if print_k needs to be updated for small dataset\n",
    "#         print_k = config['train_settings']['print_k']\n",
    "#         if print_k > len(df)/config['data_settings']['batch_size']:\n",
    "#             print_k = len(df)/config['data_settings']['batch_size']\n",
    "#             config['train_settings']['print_k'] = print_k\n",
    "#             print('Updating print_k to contain whole epoch. Num_batches =',print_k)\n",
    "        \n",
    "    #Testing a differnt num of epochs based on loop\n",
    "    config['train_settings']['num_epochs'] = num_epochs\n",
    "\n",
    "#         #updating datafiles\n",
    "#         config['data_settings']['datafiles']['train']=train_file\n",
    "#         config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "#         #config['data_settings']['datafiles']['train']=train_csv\n",
    "#         config['data_settings']['datafiles']['test'] = test_file\n",
    "#         config['data_settings']['datafiles']['valid']= ''\n",
    "#         config['data_settings']['datafiles']['query']= test_file\n",
    "\n",
    "    #update Model path\n",
    "    config['model_settings']['model_path'] = model_file\n",
    "\n",
    "#         #update pickle_file to prevent being overwritten\n",
    "    pickle_file = os.path.join(run_dir_name,'results.pkl')\n",
    "    config['eval_settings']['pickle_file'] = pickle_file\n",
    "    csv_file = os.path.join(run_dir_name,\"results.csv\")\n",
    "    config['eval_settings']['results_file'] = csv_file\n",
    "\n",
    "#         #update wandb_project_name\n",
    "#         config['train_settings']['wandb_project_name'] = wandb_name\n",
    "#         config['train_settings']['wandb_dir_path'] = run_dir_name #this should make a seperate wandb folder for runs\n",
    "\n",
    "#         #save yml\n",
    "    new_yml_file = os.path.join(run_dir_name,\"checkpoint_eval.yml\")\n",
    "   \n",
    "    with open(new_yml_file, 'w') as fo:\n",
    "            yaml.dump(config,fo)   \n",
    "        \n",
    "\n",
    "    #---------- actually run eval--------------\n",
    "    # !python pytorch_train_and_eval_reid_2.py --config_file {new_yml_file}\n",
    "    eval(new_yml_file)\n",
    "\n",
    "#         # Save model to wandb file location to prevent overwriting\n",
    "#         # new dir in wandb/ will be generated each training run\n",
    "#         !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "    # read python results from pickle file, \n",
    "    with open(config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "        results = pickle.load(fi)  \n",
    "\n",
    "    # Write out run summary to results tracking document\n",
    "\n",
    "    results_dict =  {'run_str': run_str,\n",
    "                                        # 'wandb_id':results['wandb_id'],\n",
    "                                        'num_ids':num_ids,\n",
    "                                        'num_images_per_id':num_images,\n",
    "                                        'total_training_images':len(pd.read_csv(train_file))-(len(pd.read_csv(train_file))*config['data_settings']['percent_valid']),\n",
    "                                        'batch_size':config['data_settings']['batch_size'],\n",
    "                                        'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                        # 'train_loss':results['train_loss'],\n",
    "                                        # 'valid_loss':results['valid_loss'],\n",
    "                                        '1NN':results['1NN_acc'],\n",
    "                                        '3NN':results['3NN_acc'],\n",
    "                                        'training_file':train_file,\n",
    "                                        'query_file':config['data_settings'][\"datafiles\"][\"test\"],\n",
    "                                        # 'start_time':results['start_time'],\n",
    "                                        # 'train_time':results['train_time'],\n",
    "                                        # 'stop_epoch':results['stop_epoch']}\n",
    "                    }\n",
    "\n",
    "    if not os.path.exists(config['eval_settings']['results_file']):\n",
    "        !touch {config['eval_settings']['results_file']}\n",
    "        results_df = pd.DataFrame(results_dict,index=[0])\n",
    "        results_df.to_csv(config['eval_settings']['results_file'])\n",
    "    else:\n",
    "        #read df and append row\n",
    "        results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "        results_df.loc[len(results_df)] = results_dict\n",
    "        results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a569fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1NN_acc\n",
      "3NN_acc\n",
      "label_list\n",
      "knn_class\n",
      "knn_conf\n"
     ]
    }
   ],
   "source": [
    " with open(yml_config['eval_settings']['pickle_file'],'rb') as fi:\n",
    "        results = pickle.load(fi)  \n",
    "        \n",
    "    \n",
    "# Convert ndarray to list\n",
    "\n",
    "for key in results.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c094d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
