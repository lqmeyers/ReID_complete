{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-25 10:21:53.741197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 10:21:54.499471: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as Image2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "import yaml\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from transformers import ViTFeatureExtractor\n",
    "from SCL_reID.utils.pytorch_data import *\n",
    "from SCL_reID.models.pytorch_models import *\n",
    "from pytorch_metric_learning import losses, miners\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO PERFORM KNN EVALUATION\n",
    "#\n",
    "def knn_evaluation(train_images, train_labels, test_images, test_labels, n_neighbors, per_class=True, conf_matrix=True):\n",
    "    # BUILD KNN MODEL AND PREDICT\n",
    "    results = {}\n",
    "    print(f'Training kNN classifier with k={n_neighbors}')\n",
    "    my_knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric='cosine')\n",
    "    my_knn.fit(train_images, train_labels)\n",
    "    knn_pred = my_knn.predict(test_images)\n",
    "    knn_acc = np.round(np.sum([1 for pred, label in zip(knn_pred, test_labels) if pred == label])/test_labels.shape[0],4)\n",
    "    print(f'{n_neighbors}NN test accuracy: {knn_acc}')\n",
    "    # store results\n",
    "    results['n_neighbors'] = n_neighbors\n",
    "    results['knn'] = knn_acc\n",
    "    label_list = np.unique(train_labels)\n",
    "    results['label_list'] = label_list\n",
    "    if per_class:\n",
    "        knn_class = np.zeros(len(label_list))\n",
    "        print(f'\\nPer label {n_neighbors}NN test accuracy:')\n",
    "        for k, label in enumerate(label_list):\n",
    "            mask = test_labels == label\n",
    "            knn_class[k] = np.round(np.sum(knn_pred[mask]==test_labels[mask])/np.sum(mask),4)\n",
    "            print(f'{label}\\t{knn_class[k]:.2f}')\n",
    "        # store results\n",
    "        results['knn_class'] = knn_class\n",
    "    if conf_matrix:\n",
    "        knn_conf = confusion_matrix(test_labels, knn_pred)\n",
    "        results['knn_conf'] = knn_conf\n",
    "        print('\\nPrinting Confusion Matrix:')\n",
    "        print(results['knn_conf'])\n",
    "    return results\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date and time when this experiment was started: 24-09-25 10:21\n",
      "Date and time when this experiment was started: 24-09-25 10:21\n",
      "Data Settings:\n",
      "{'aug_p': 0.3, 'batch_size': 64, 'crop_height': None, 'crop_left': None, 'crop_top': None, 'crop_width': None, 'cropped': False, 'datafiles': {'gallery': '/home/lmeyers/ReID_complete/summer_2023_reid_galleries_closed.csv', 'query': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv', 'reference': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_02.csv', 'test': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2/summer_bee_dataset_open_train_bee_64_ids_batch2_sample_num_max.csv', 'train': '/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch1/summer_bee_dataset_open_train_bee_64_ids_batch1_sample_num_64.csv', 'valid': ''}, 'dataset': 'summer_2023', 'fname_col': 'new_filepath', 'gallery_id': 'gallery_id', 'image_id_col': 'image_id', 'input_size': [250, 250], 'iteration_id': 'iteration_id', 'label_col': 'ID', 'n_distractors': 9, 'percent_valid': 0.2, 'sample_reference': True, 'sample_valid': True, 'split_type': 'closed'}\n",
      "Train Settings:\n",
      "{'checkpoint_to_load': None, 'early_stop_consecutive_epochs': 1000, 'early_stopping': True, 'gpu': 1, 'learning_rate': 0.001, 'margin': 0.2, 'num_epochs': 1500, 'print_k': 10, 'save_checkpoint_freq': 50, 'wandb_dir_path': '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/', 'wandb_entity_name': 'meyers_luke_lab', 'wandb_project_name': 'vit_baselines_test64_ids_batch1_sample_num_64', 'wandb_resume': False, 'wandb_run_id': None}\n",
      "Model Settings:\n",
      "{'latent_dim': 128, 'model_class': 'vit_reid', 'model_path': '/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/64_ids_batch1_sample_num_64.pth', 'num_labels': '64'}\n"
     ]
    }
   ],
   "source": [
    "#load config file params:\n",
    "config_file = \"/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/64_ids_batch1_sample_num_64.yml\"\n",
    "verbose = True\n",
    "\n",
    "try:\n",
    "    with open(config_file) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    model_config = config['model_settings'] # settings for model building\n",
    "    train_config = config['train_settings'] # settings for model training\n",
    "    data_config = config['data_settings'] # settings for data loading\n",
    "    eval_config = config['eval_settings'] # settings for evaluation\n",
    "    torch_seed = config['torch_seed']\n",
    "    verbose = config['verbose']\n",
    "except Exception as e:\n",
    "    print('ERROR - unable to open experiment config file. Terminating.')\n",
    "    print('Exception msg:',e)\n",
    "if verbose:\n",
    "    # ADD PRINT OF DATE AND TIME\n",
    "    now = datetime.now() # current date and time\n",
    "    dt = now.strftime(\"%y-%m-%d %H:%M\")\n",
    "    print(f'Date and time when this experiment was started: {dt}')\n",
    "    print(f'Date and time when this experiment was started: {dt}')\n",
    "    print(\"Data Settings:\")\n",
    "    print(data_config)\n",
    "    print(\"Train Settings:\")\n",
    "    print(train_config)\n",
    "    print(\"Model Settings:\")\n",
    "    print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0,\"/home/lmeyers/ReID_complete/SCL_reID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tweak config file data to test different scenarios\n",
    "# data_config['datafiles']['reference'] = '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch1/summer_bee_dataset_closed_train_bee_balanced_batch1_sample_num_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ViTForReID' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/lmeyers/contrastive_learning_new_training/64_ids_batch1_sample_num_64/checkpoints/1400.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m=\u001b[39m ViTForReID()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m(model_path)\u001b[38;5;66;03m#ViTForReID(load_saved=True,model_path=model_path)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#model = torch.load('/home/lmeyers/ReID_complete/checkpoints/300.pth')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ViTForReID' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "from SCL_reID.models.pytorch_models import *\n",
    "from SCL_reID.utils import *\n",
    "\n",
    "model = ViTForReID()\n",
    "model.load(model_path)#ViTForReID(load_saved=True,model_path=model_path)\n",
    "\n",
    "#model = torch.load('/home/lmeyers/ReID_complete/checkpoints/300.pth')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "if verbose:\n",
    "    print(f'Device: {device}')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = miners.MultiSimilarityMiner()\n",
    "loss_fn = losses.TripletMarginLoss(train_config['margin'], distance = CosineSimilarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################################################\n",
    "# FUNCTION TO GET EMBEDDINGS AND LABELS FOR EVALUATING MODEL\n",
    "def get_embeddings(model, dataloader, loss_fn, miner, device, feature_extractor=None):\n",
    "    embeddings = []\n",
    "    all_labels = []\n",
    "    loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for k, batch in enumerate(dataloader):\n",
    "            if feature_extractor is None:\n",
    "                images = batch['image'].to(device)\n",
    "            else:\n",
    "                images = [transforms.functional.to_pil_image(x) for x in batch['image']]\n",
    "                images = np.concatenate([feature_extractor(x)['pixel_values'] for x in images])\n",
    "                images = torch.tensor(images, dtype=torch.float).to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(images)\n",
    "            hard_pairs = miner(outputs, labels)\n",
    "            loss += loss_fn(outputs, labels, hard_pairs).detach().cpu().numpy()\n",
    "            embeddings.append(outputs.detach().cpu().numpy())\n",
    "            all_labels += list(labels.detach().cpu().numpy())\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    all_labels = np.array(all_labels)\n",
    "    loss/=k\n",
    "    return embeddings, all_labels, loss\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ViT feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load VIT feature extractor if needed\n",
    "\n",
    "# print('Getting ViT feature extractor...')\n",
    "# model_name = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForReID(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (reid): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to eval \n",
    "if verbose:\n",
    "    print('Evaluating model...')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating dataloaders from dataset\n",
      "Using 2173 samples for reference set\n",
      "8693 total test samples\n",
      "Got_reference_embeddings\n",
      "(2173, 128)\n",
      "Got test embeddings\n",
      "(8693, 128)\n",
      "Reference (or Train) Loss: 0.2061\n",
      "Test (or Query) Loss: 0.2015\n"
     ]
    }
   ],
   "source": [
    "#get dataloaders and embeddings \n",
    "print(\"generating dataloaders from dataset\")\n",
    "\n",
    "test_dataloader, reference_dataloader = get_dataset(data_config, 'test',generate_valid=True) #generate valid automatically\n",
    "\n",
    "reference_embeddings, reference_labels, reference_loss = get_embeddings(model, reference_dataloader, loss_fn, miner, device, feature_extractor)\n",
    "print('Got_reference_embeddings')\n",
    "print(reference_embeddings.shape)\n",
    "test_embeddings, test_labels, test_loss = get_embeddings(model, test_dataloader, loss_fn, miner, device, feature_extractor)\n",
    "print('Got test embeddings')\n",
    "print(test_embeddings.shape)\n",
    "\n",
    "\n",
    "print(f'Reference (or Train) Loss: {reference_loss:.4f}')\n",
    "print(f'Test (or Query) Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2\n",
      "/home/lmeyers/summer_bee_data_reextract/new_open_max_ids_batch2\n"
     ]
    }
   ],
   "source": [
    "reference_data_batch = os.path.dirname(data_config['datafiles']['reference'])#[-1:]\n",
    "print(reference_data_batch)\n",
    "query_data_batch = os.path.dirname(data_config['datafiles']['query'])#[-1:]\n",
    "print(query_data_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_to_b2 = {10: 74, 11: 75, 15: 79, 14: 78, 12: 76, 16: 80, 13: 77, 9: 73, 18: 82, 19: 83, 23: 87, 22: 86, 20: 84, 24: 88, 21: 85, 17: 81, 50: 114, 51: 115, 55: 119, 54: 118, 52: 116, 56: 120, 53: 117, 49: 113, 42: 106, 43: 107, 47: 111, 46: 110, 44: 108, 48: 112, 45: 109, 41: 105, 26: 90, 27: 91, 31: 95, 30: 94, 28: 92, 32: 96, 29: 93, 25: 89, 58: 122, 59: 123, 63: 127, 62: 126, 60: 124, 64: 128, 61: 125, 57: 121, 34: 98, 35: 99, 39: 103, 38: 102, 36: 100, 40: 104, 37: 101, 33: 97, 2: 66, 3: 67, 7: 71, 6: 70, 68: 68, 8: 72, 5: 69, 1: 65}\n",
    "b2_to_b1 = {74: 10, 75: 11, 79: 15, 78: 14, 76: 12, 80: 16, 77: 13, 73: 9, 82: 18, 83: 19, 87: 23, 86: 22, 84: 20, 88: 24, 85: 21, 81: 17, 114: 50, 115: 51, 119: 55, 118: 54, 116: 52, 120: 56, 117: 53, 113: 49, 106: 42, 107: 43, 111: 47, 110: 46, 108: 44, 112: 48, 109: 45, 105: 41, 90: 26, 91: 27, 95: 31, 94: 30, 92: 28, 96: 32, 93: 29, 89: 25, 122: 58, 123: 59, 127: 63, 126: 62, 124: 60, 128: 64, 125: 61, 121: 57, 98: 34, 99: 35, 103: 39, 102: 38, 100: 36, 104: 40, 101: 37, 97: 33, 66: 2, 67: 3, 71: 7, 70: 6, 68: 68, 72: 8, 69: 5, 65: 1}\n",
    "\n",
    "reference_data_batch = os.path.dirname(data_config['datafiles']['reference'])[-1:]\n",
    "query_data_batch = os.path.dirname(data_config['datafiles']['query'])[-1:]\n",
    "\n",
    "\n",
    "if reference_data_batch != query_data_batch and data_config['label_col'] != 'color_num':\n",
    "    if reference_data_batch > query_data_batch:\n",
    "        for i in range(len(test_labels)):\n",
    "            test_labels[i] = b1_to_b2[test_labels[i]]\n",
    "    else: \n",
    "        for i in range(len(test_labels)):\n",
    "            test_labels[i] = b2_to_b1[test_labels[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kNN classifier with k=3\n",
      "3NN test accuracy: 0.033\n",
      "\n",
      "Per label 3NN test accuracy:\n",
      "65\t0.11\n",
      "66\t0.00\n",
      "67\t0.01\n",
      "68\t0.00\n",
      "69\t0.01\n",
      "70\t0.00\n",
      "71\t0.00\n",
      "72\t0.12\n",
      "73\t0.16\n",
      "74\t0.00\n",
      "75\t0.05\n",
      "76\t0.11\n",
      "77\t0.01\n",
      "78\t0.24\n",
      "79\t0.00\n",
      "80\t0.03\n",
      "81\t0.17\n",
      "82\t0.08\n",
      "83\t0.03\n",
      "84\t0.00\n",
      "85\t0.00\n",
      "86\t0.00\n",
      "87\t0.03\n",
      "88\t0.00\n",
      "89\t0.00\n",
      "90\t0.07\n",
      "91\t0.00\n",
      "92\t0.03\n",
      "93\t0.08\n",
      "94\t0.00\n",
      "95\t0.00\n",
      "96\t0.00\n",
      "97\t0.00\n",
      "98\t0.02\n",
      "99\t0.00\n",
      "100\t0.00\n",
      "101\t0.00\n",
      "102\t0.00\n",
      "103\t0.00\n",
      "104\t0.00\n",
      "105\t0.00\n",
      "106\t0.00\n",
      "107\t0.00\n",
      "108\t0.00\n",
      "109\t0.00\n",
      "110\t0.00\n",
      "111\t0.00\n",
      "112\t0.00\n",
      "113\t0.00\n",
      "114\t0.00\n",
      "115\t0.00\n",
      "116\t0.00\n",
      "117\t0.00\n",
      "118\t0.00\n",
      "119\t0.00\n",
      "120\t0.00\n",
      "121\t0.07\n",
      "122\t0.00\n",
      "123\t0.00\n",
      "124\t0.00\n",
      "125\t0.00\n",
      "126\t0.00\n",
      "127\t0.00\n",
      "128\t0.00\n",
      "\n",
      "Printing Confusion Matrix:\n",
      "[[19  0  0 ...  0  0  0]\n",
      " [14  0  0 ...  0  0  0]\n",
      " [34  0  1 ...  0  0  0]\n",
      " ...\n",
      " [ 7  0  0 ...  0  0  0]\n",
      " [ 5  0  0 ...  0  0  0]\n",
      " [ 8  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = knn_evaluation(reference_embeddings, reference_labels, test_embeddings, test_labels, \n",
    "                        eval_config['n_neighbors'], eval_config['per_class'], eval_config['conf_matrix'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
