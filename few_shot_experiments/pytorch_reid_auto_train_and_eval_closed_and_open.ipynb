{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a95221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d54ee",
   "metadata": {},
   "source": [
    "# closed set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bbc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = '/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2'\n",
    "#dir = '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2'\n",
    "dir = '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2'\n",
    "#dir to start working with \n",
    "\n",
    "#make new wandb project based on dir name\n",
    "\n",
    "\n",
    "test_file = '/home/gsantiago/summer_bee_data/open_sets/open_max_ids_batch1/summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv'\n",
    "#test on all of batch 2 (open set) gonna also test on all of batch 1 \n",
    "\n",
    "valid_file = '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch1/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'\n",
    "# run valid on smaller subset of test_set to speed training \n",
    "\n",
    "reference_file = '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch1/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'\n",
    "#reference knn on smaller subset of batch 1\n",
    "\n",
    "results_file = '/home/lmeyers/ReID_complete/few_shot_experiments/Few_shot_expirament_results_tracking.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897a1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_max.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_16.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_2.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_4.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_20.csv\n",
      "/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_8.csv\n"
     ]
    }
   ],
   "source": [
    "# Get file list \n",
    "for root, dirs, files in os.walk(dir):\n",
    "    files = files\n",
    "for f in files:\n",
    "    print(root+r'/'+f)\n",
    "    train_file = root+r'/'+f\n",
    "#     continue\n",
    "\n",
    "#files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e5a0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_4.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_2.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_8.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_16.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_20.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_max.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_max.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_16.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_2.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_4.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_20.csv'\n",
      " 'summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_8.csv']\n"
     ]
    }
   ],
   "source": [
    "dir2 = '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2'\n",
    "files2 = []\n",
    "for root2, dirs2, files2 in os.walk(dir2):\n",
    "    files2 = files2\n",
    "for f2 in files2:\n",
    "    #print(root+r'/'+f2)\n",
    "    train_file2 = root2+r'/'+f2\n",
    "#     continue\n",
    "\n",
    "files = np.concatenate((files,files2))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e737d9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels  8\n",
      "2023-10-20 17:38:10.994168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 17:38:12.141749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "finished imports\n",
      "beginning execution\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlqmeyers\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/lmeyers/ReID_complete/few_shot_experiments/8_ids_monocolor_batch2_sample_num_max/wandb/run-20231020_173816-njb7ranp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeachy-durian-15\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/closed_sets_8_ids_monocolor_batch2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/closed_sets_8_ids_monocolor_batch2/runs/njb7ranp\u001b[0m\n",
      "Date and time when this experiment was started: 23-10-20 17:38\n",
      "Data Settings:\n",
      "{'aug_p': 0.3, 'batch_size': 64, 'crop_height': None, 'crop_left': None, 'crop_top': None, 'crop_width': None, 'cropped': False, 'datafiles': {'gallery': '/home/lmeyers/ReID_complete/summer_2023_reid_galleries_closed.csv', 'query': '/home/gsantiago/summer_bee_data/open_sets/open_max_ids_batch1/summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv', 'reference': '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch1/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv', 'test': '/home/gsantiago/summer_bee_data/open_sets/open_max_ids_batch1/summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv', 'train': '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_max.csv', 'valid': '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch1/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'}, 'dataset': 'summer_2023', 'fname_col': 'filepath', 'gallery_id': 'gallery_id', 'image_id_col': 'image_id', 'input_size': [250, 250], 'iteration_id': 'iteration_id', 'label_col': 'color_num', 'n_distractors': 9, 'split_type': 'closed'}\n",
      "Train Settings:\n",
      "{'checkpoint_to_load': None, 'gpu': 1, 'learning_rate': 0.001, 'margin': 0.2, 'num_epochs': 300, 'print_k': 10, 'save_checkpoint_freq': 50, 'wandb_dir_path': '/home/lmeyers/ReID_complete/few_shot_experiments/8_ids_monocolor_batch2_sample_num_max/', 'wandb_entity_name': 'meyers_luke_lab', 'wandb_project_name': 'closed_sets_8_ids_monocolor_batch2', 'wandb_resume': False, 'wandb_run_id': None}\n",
      "Model Settings:\n",
      "{'latent_dim': 128, 'model_class': 'resnet50_conv3', 'model_path': './8_ids_monocolor_batch2_sample_num_max/8_ids_monocolor_batch2_sample_num_max.pth', 'num_labels': '8'}\n",
      "Using GPU 1\n",
      "Creating train and valid dataloaders...\n",
      "Batch image shape: torch.Size([64, 3, 250, 250])\n",
      "Batch label shape: torch.Size([64])\n",
      "Building model....\n",
      "Loss: TripletMarginLoss(\n",
      "  (distance): CosineSimilarity()\n",
      "  (reducer): AvgNonZeroReducer()\n",
      ")\n",
      "Found device: cuda\n",
      "Training model...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lmeyers/ReID_complete/pytorch_train_and_eval_reid.py\", line 328, in <module>\n",
      "    train_and_eval(args.config_file)\n",
      "  File \"/home/lmeyers/ReID_complete/pytorch_train_and_eval_reid.py\", line 232, in train_and_eval\n",
      "    hard_pairs = miner(outputs, labels)\n",
      "  File \"/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/pytorch_metric_learning/miners/base_miner.py\", line 54, in forward\n",
      "    mining_output = self.mine(embeddings, labels, ref_emb, ref_labels)\n",
      "  File \"/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/pytorch_metric_learning/miners/multi_similarity_miner.py\", line 17, in mine\n",
      "    a1, p, a2, n = lmu.get_all_pairs_indices(labels, ref_labels)\n",
      "  File \"/home/lmeyers/anaconda3/envs/mlenv/lib/python3.9/site-packages/pytorch_metric_learning/utils/loss_and_miner_utils.py\", line 50, in get_all_pairs_indices\n",
      "    a1_idx, p_idx = torch.where(matches)\n",
      "KeyboardInterrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
      "Num labels  8\n",
      "2023-10-20 17:38:40.056789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 17:38:41.139562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "finished imports\n",
      "beginning execution\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlqmeyers\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "epochs_to_test = [300,600,1500]\n",
    "for n in epochs_to_test: \n",
    "    for i in range(len(files)):\n",
    "        #train file path\n",
    "        train_file = root+r'/'+files[i]\n",
    "        wandb_name = os.path.basename(root)\n",
    "        # initilize and make new dir for run\n",
    "        #run_num = '0'+str(i) #I've decided this makes it harder\n",
    "        run_str = os.path.basename(files[i])[36:-4]\n",
    "        run_dir_name = run_str+'/'\n",
    "        if not os.path.exists(run_dir_name):\n",
    "            os.mkdir(run_dir_name)\n",
    "\n",
    "        split_parts = run_str.rsplit('_', 1)\n",
    "        # Check if there is at least one underscore in the string\n",
    "        if len(split_parts) > 1:\n",
    "            # Get the substring after the last underscore\n",
    "            num_images = split_parts[1]\n",
    "            num_ids = split_parts[-1]\n",
    "        else:\n",
    "            # Handle the case where there are no underscores in the string\n",
    "            num_images = run_str\n",
    "\n",
    "        #filter particular runs (if needed)\n",
    "        if num_images == 'max':\n",
    "\n",
    "            ##---------- Initilize new config .yml for new training file---------------\n",
    "\n",
    "            #open config yaml to update experiment params\n",
    "            with open('/home/lmeyers/ReID_complete/reid_template.yml', 'r') as fo:\n",
    "                config = yaml.safe_load(fo)\n",
    "            \n",
    "            #Update params\n",
    "            config['model_settings']['num_labels']= run_str[0]\n",
    "            print('Num labels ',run_str[0])\n",
    "\n",
    "            #Check if batch size needs to be updated\n",
    "            df = pd.read_csv(train_file)\n",
    "            if config['data_settings']['batch_size'] > len(df):\n",
    "                config['data_settings']['batch_size'] = len(df)\n",
    "                print('Updated batch to contain all Data. Size = ',len(df))\n",
    "            \n",
    "            #Testing a differnt num epochs\n",
    "            config['train_settings']['num_epochs'] = n\n",
    "\n",
    "            #updating datafiles\n",
    "            config['data_settings']['datafiles']['train']=train_file\n",
    "            config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "\n",
    "            #config['data_settings']['datafiles']['train']=train_csv\n",
    "            config['data_settings']['datafiles']['test'] = test_file\n",
    "            config['data_settings']['datafiles']['valid']= valid_file \n",
    "            config['data_settings']['datafiles']['query']= test_file\n",
    "\n",
    "            #update Model path\n",
    "            config['model_settings']['model_path'] = './'+run_dir_name+run_str+'.pth'\n",
    "\n",
    "            #update wandb_project_name\n",
    "            config['train_settings']['wandb_project_name'] = wandb_name\n",
    "            config['train_settings']['wandb_dir_path'] = '/home/lmeyers/ReID_complete/few_shot_experiments/'+ run_dir_name #this should make a seperate wandb folder for runs\n",
    "\n",
    "            #save yml\n",
    "            new_yml_file = './'+run_dir_name+run_str+'.yml'\n",
    "            with open(new_yml_file, 'w') as fo:\n",
    "                    yaml.dump(config,fo)   \n",
    "\n",
    "            #---------- actually run training too--------------\n",
    "            !python /home/lmeyers/ReID_complete/pytorch_train_and_eval_reid.py --config_file {new_yml_file}\n",
    "\n",
    "            # Save model to wandb file location to prevent overwriting\n",
    "            !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "            with open('/home/lmeyers/ReID_complete/results.pkl','rb') as fi:\n",
    "                results = pickle.load(fi)  \n",
    "            \n",
    "            # Write out run summary to results tracking document\n",
    "            results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "            results_df.loc[len(results_df)] = {'run_str': run_str,\n",
    "                                               'wandb_id':results['wandb_id'],\n",
    "                                               'num_ids':num_ids,\n",
    "                                               'num_images_per_id':num_images,\n",
    "                                               'total_training_images':len(pd.read_csv(train_file)),\n",
    "                                               'batch_size':config['data_settings']['batch_size'],\n",
    "                                               'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                               'train_loss':results['train_loss'],\n",
    "                                               'valid_loss':results['valid_loss'],\n",
    "                                               '1NN':results['1NN_acc'],\n",
    "                                               '3NN':results['3NN_acc'],\n",
    "                                               'training_file':train_file,\n",
    "                                               'reference_file':reference_file,\n",
    "                                               'query_file':test_file,\n",
    "                                               'start_time':results['start_time'],\n",
    "                                               'train_time':results['train_time']}\n",
    "            results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3c3300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closed_sets_max_ids_batch1\n",
      "closed_sets_max_ids_batch2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we can specify what we want to train. If we want everything, remove max, it should work\n",
    "directory = '/home/gsantiago/summer_bee_data/closed_sets_max*'\n",
    "closed_set_directory = '/home/gsantiago/summer_bee_data/closed_test_'\n",
    "closed_test = \"/home/gsantiago/summer_bee_data/closed_test_\"\n",
    "\n",
    "model_path = '/home/gsantiago/ReID_model_training/model_path/'\n",
    "for path in glob(directory):\n",
    "    \n",
    "    \n",
    "    batch_number = path.split('_')[-1]\n",
    "    \n",
    "    all_csv_directories = path+'/*'\n",
    "    full_dir_name = path.split('/')[-1]\n",
    "#     print(full_dir_name)\n",
    "#     continue\n",
    "    type_of_split = full_dir_name.replace('closed_sets', '').replace(batch_number,'')\n",
    "    #open_set_for_test = open_sets+\"open\"+type_of_split\n",
    "\n",
    "    #print(batch_number)\n",
    "    \n",
    "    closed_for_this_test= closed_test+  batch_number+'/*'\n",
    "    \n",
    "#     if(batch_number == 'batch1'):\n",
    "        \n",
    "#         #open_set_for_test = open_set_for_test+'batch2/'+\"summer_bee_dataset_open_train_bee_balanced_batch2_sample_num_max.csv\"\n",
    "#     else:\n",
    "#         #open_set_for_test = open_set_for_test+'batch1/'+\"summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv\"\n",
    "        \n",
    "#     print(open_set_for_test)\n",
    "    \n",
    "    with open('./pytorch_train_and_eval_reid.yml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    for train_csv in glob(all_csv_directories):\n",
    "        #print('train_csv')\n",
    "        #print(train_csv)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #changing train folder\n",
    "#         print(closed_for_this_test)\n",
    "#         print(glob(closed_for_this_test))\n",
    "        train_df = pd.read_csv(train_csv)\n",
    "        num_labels = train_df['ID'].nunique()\n",
    "        #print(num_labels)\n",
    "        \n",
    "        #batch setting\n",
    "        config['data_settings']['batch_size']= 64\n",
    "        \n",
    "        #updating labels\n",
    "        \n",
    "        config['model_settings']['num_labels']=num_labels\n",
    "        \n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['reference']= train_csv\n",
    "  \n",
    "\n",
    "        #config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = glob(closed_for_this_test)[0]\n",
    "        config['data_settings']['datafiles']['valid']= glob(closed_for_this_test)[0]\n",
    "        config['data_settings']['datafiles']['query']= glob(closed_for_this_test)[0]\n",
    "        \n",
    "        model_directory =(model_path+\"closed/\"+full_dir_name+'/')\n",
    "        \n",
    "        if not os.path.exists(model_directory):\n",
    "            os.makedirs(model_directory)\n",
    "        config['model_settings']['model_path']= model_directory\n",
    "        \n",
    "        config_name = train_csv.split('/')[-1].replace('.csv', '')\n",
    "        print(config['data_settings']['datafiles'])\n",
    "\n",
    "        new_yml_file = \"./different_yml_configs/closed/\"+config_name+'_config.yml'\n",
    "        output_file = \"./all_outputs/\"+(new_yml_file.split('/')[-1].replace('_config.yml', '_output.txt'))\n",
    "       \n",
    "        with open(new_yml_file, 'w') as f:\n",
    "            yaml.dump(config,f)\n",
    "        \n",
    "        !python pytorch_train_and_eval_reid.py --config_file {new_yml_file} > {output_file} \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f014d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d2bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13c236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c1bc2ca",
   "metadata": {},
   "source": [
    "# open set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv\n",
      "64\n",
      "2023-10-15 22:44:31.603382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 22:44:32.942356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231015_224436-pfstr74p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdaily-plant-25\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/pfstr74p\u001b[0m\n",
      "/home/gsantiago/ReID_model_training/pytorch_train_and_eval_reid.py:77: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  knn_class[k] = np.round(np.sum(knn_pred[mask]==test_labels[mask])/np.sum(mask),4)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.005 MB of 0.012 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss 0.14135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdaily-plant-25\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/pfstr74p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231015_224436-pfstr74p/logs\u001b[0m\n",
      "train_csv\n",
      "64\n",
      "2023-10-15 23:53:43.592262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 23:53:45.135061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231015_235349-b64kuzyj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-voice-26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/b64kuzyj\u001b[0m\n",
      "/home/gsantiago/ReID_model_training/pytorch_train_and_eval_reid.py:77: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  knn_class[k] = np.round(np.sum(knn_pred[mask]==test_labels[mask])/np.sum(mask),4)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.005 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñà‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss 0.16833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mefficient-voice-26\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/b64kuzyj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231015_235349-b64kuzyj/logs\u001b[0m\n",
      "train_csv\n",
      "64\n",
      "2023-10-16 00:45:50.898888: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 00:45:52.091201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231016_004555-9myro3ut\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdandy-voice-27\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/9myro3ut\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we can specify what we want to train. If we want everything, remove max, it should work\n",
    "directory = '/home/gsantiago/summer_bee_data/open_sets/open_max*'\n",
    "open_set_directory = '/home/gsantiago/summer_bee_data/open_sets/'\n",
    "open_test = \"/home/gsantiago/summer_bee_data/open_sets/\"\n",
    "\n",
    "model_path = '/home/gsantiago/ReID_model_training/model_path/'\n",
    "for path in glob(directory):\n",
    "    \n",
    "    #print(path)\n",
    "    batch_number = path.split('_')[-1]\n",
    "    #print(batch_number)\n",
    "    \n",
    "    all_csv_directories = path+'/*'\n",
    "    #print(glob(all_csv_directories))\n",
    "    full_dir_name = path.split('/')[-1]\n",
    "    #print(full_dir_name)\n",
    "    type_of_split = full_dir_name.replace(batch_number,'')\n",
    "    #print(\"type_of_split\")\n",
    "    #print(type_of_split)\n",
    "   \n",
    "\n",
    "    #print(batch_number)\n",
    "    \n",
    "    #open_for_this_test= open_test+  batch_number+'/*'\n",
    "    \n",
    "    if(batch_number == 'batch1'):\n",
    "        \n",
    "        open_set_for_test = open_test+type_of_split+'batch2/'+\"summer_bee_dataset_open_train_bee_balanced_batch2_sample_num_max.csv\"\n",
    "        \n",
    "    else:\n",
    "        open_set_for_test = open_test+type_of_split+'batch1/'+\"summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv\"\n",
    "        \n",
    "    #print(open_set_for_test)\n",
    "    \n",
    "    with open('./pytorch_train_and_eval_reid.yml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    #print(glob(all_csv_directories))\n",
    "    for train_csv in glob(all_csv_directories):\n",
    "        print('train_csv')\n",
    "        #print(train_csv)\n",
    "\n",
    "\n",
    "        # optional, skipping  the finished cvs\n",
    "        \n",
    "        #changing train folder\n",
    "        #print(\"open_set_for_test\")\n",
    "        #print(open_set_for_test)\n",
    "        #print(glob(open_set_for_test))\n",
    "        train_df = pd.read_csv(train_csv)\n",
    "        num_labels = train_df['ID'].nunique()\n",
    "        print(num_labels)\n",
    "        \n",
    "        #batch setting\n",
    "        config['data_settings']['batch_size']= 64\n",
    "        \n",
    "        #updating labels\n",
    "        \n",
    "        config['model_settings']['num_labels']=num_labels\n",
    "        \n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = open_set_for_test\n",
    "        config['data_settings']['datafiles']['valid']= open_set_for_test\n",
    "        config['data_settings']['datafiles']['reference']= train_csv\n",
    "        config['data_settings']['datafiles']['query']= open_set_for_test\n",
    "        \n",
    "        \n",
    "                \n",
    "        model_directory =(model_path+\"open/\"+full_dir_name+'/')\n",
    "        train_model_directory = model_directory+'train/'\n",
    "        eval_model_directory = model_directory +'eval/'\n",
    "        \n",
    "        if not os.path.exists(model_directory):\n",
    "            os.makedirs(model_directory)\n",
    "            os.makedirs(train_model_directory)\n",
    "            os.makedirs(eval_model_directory)\n",
    "            \n",
    "        #sample size for model, saved as name\n",
    "        \n",
    "        sample_num = train_csv.split('_')[-1].replace(\".csv\",'_samples.pth')\n",
    "            \n",
    "        config['model_settings']['model_path']= train_model_directory+sample_num\n",
    "        config['eval_settings']['model_path'] = eval_model_directory +sample_num\n",
    "        \n",
    "\n",
    "        \n",
    "        config_name = train_csv.split('/')[-1].replace('.csv', '')\n",
    "        #print(config['data_settings']['datafiles'])\n",
    "\n",
    "        new_yml_file = \"./different_yml_configs/open/\"+config_name+'_config.yml'\n",
    "        output_file = \"./all_outputs/\"+(new_yml_file.split('/')[-1].replace('_config.yml', '_output.txt'))\n",
    "       \n",
    "        with open(new_yml_file, 'w') as f:\n",
    "            yaml.dump(config,f)\n",
    "        \n",
    "        !python pytorch_train_and_eval_reid.py --config_file {new_yml_file} > {output_file} \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ec5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
