{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a95221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d54ee",
   "metadata": {},
   "source": [
    "# Choose Folder Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bbc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = '/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2'\n",
    "#dir = '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2'\n",
    "dir = '/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1' #'/home/gsantiago/summer_bee_data/open_sets/new_open_max_ids_batch1' # '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2'\n",
    "#dir to start working with \n",
    "\n",
    "#make new wandb project based on dir name\n",
    "\n",
    "\n",
    "test_file = '/home/gsantiago/summer_bee_data/open_sets/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_query_64_ids_batch2.csv' # '/home/gsantiago/summer_bee_data/closed_test_batch1/summer_bee_dataset_closed_test_bee_sample_num_None.csv'\n",
    "#test on all of batch 2 (open set) gonna also test on all of batch 1 \n",
    "\n",
    "valid_file = '/home/gsantiago/summer_bee_data/open_sets/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_valid_64_ids_batch2.csv' #/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch2/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'\n",
    "# run valid on smaller subset of test_set to speed training \n",
    "\n",
    "reference_file = '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch2/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'\n",
    "#reference knn on smaller subset of batch 1\n",
    "\n",
    "results_file = '/home/lmeyers/ReID_complete/color_detect_experiments/Color_detect_few_shot_expirament_results_tracking.csv'\n",
    "\n",
    "file_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "897a1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_max.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_16.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_04.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_32.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_08.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_64.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_02.csv\n"
     ]
    }
   ],
   "source": [
    "# Get file list \n",
    "\n",
    "for root, dirs, files in os.walk(dir):\n",
    "    files = files\n",
    "for f in files:\n",
    "    print(root+r'/'+f)\n",
    "    train_file = root+r'/'+f\n",
    "    file_paths.append(train_file)\n",
    "#     continue\n",
    "\n",
    "#files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b7625",
   "metadata": {},
   "source": [
    "## If setting up two folders to train one after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_4.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_2.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_8.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_16.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_20.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_max.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_max.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_16.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_2.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_4.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_20.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_8.csv']\n"
     ]
    }
   ],
   "source": [
    "dir2 = '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2'\n",
    "files2 = []\n",
    "for root2, dirs2, files2 in os.walk(dir2):\n",
    "    files2 = files2\n",
    "for f2 in files2:\n",
    "    #print(root+r'/'+f2)\n",
    "    train_file2 = root2+r'/'+f2\n",
    "    file_paths.append(train_file2)\n",
    "#     continue\n",
    "\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40567111",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c6a97",
   "metadata": {},
   "source": [
    "Change num epochs to test and any run filters (if statement just above with(open(new_yml_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737d9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', 'ids', 'batch1', 'sample', 'num', 'max']\n",
      "max\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '16']\n",
      "16\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '04']\n",
      "04\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '32']\n",
      "32\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '08']\n",
      "08\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '64']\n",
      "64\n",
      "4\n",
      "Updating print_k to contain whole epoch. Num_batches = 4\n",
      "Number of Epochs 1550\n",
      "2023-11-19 17:08:40.361833: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-19 17:08:41.545957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlqmeyers\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/lmeyers/ReID_complete/color_detect_experiments/4_ids_batch1_sample_num_64/wandb/run-20231119_170845-qp31iqb7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfluent-sun-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/color_detect_new_open_04_ids_all_colors_batch1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/color_detect_new_open_04_ids_all_colors_batch1/runs/qp31iqb7\u001b[0m\n",
      "Date and time when this experiment was started: 23-11-19 17:08\n",
      "Data Settings:\n",
      "{'aug_p': 0.3, 'batch_size': 64, 'crop_height': None, 'crop_left': None, 'crop_top': None, 'crop_width': None, 'cropped': False, 'datafiles': {'color_map': '/home/lmeyers/ReID_complete/summer_2023_v3_color_map_w_order.json', 'gallery': '/home/lmeyers/ReID_complete/summer_2023_reid_galleries_closed.csv', 'query': '/home/gsantiago/summer_bee_data/open_sets/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_query_64_ids_batch2.csv', 'reference': '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch2/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv', 'test': '/home/gsantiago/summer_bee_data/open_sets/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_query_64_ids_batch2.csv', 'train': '/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_64.csv', 'valid': '/home/gsantiago/summer_bee_data/open_sets/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_valid_64_ids_batch2.csv'}, 'dataset': 'summer_2023', 'fname_col': 'filepath', 'gallery_id': 'gallery_id', 'image_id_col': 'image_id', 'input_size': [250, 250], 'iteration_id': 'iteration_id', 'label_col': 'color_num', 'n_distractors': 9, 'split_type': 'closed'}\n",
      "Train Settings:\n",
      "{'checkpoint_to_load': None, 'early_stop_consecutive_epochs': 1000, 'early_stopping': True, 'gpu': 0, 'learning_rate': 0.001, 'margin': 0.2, 'num_epochs': 1550, 'pos_weight': 7, 'print_k': 4, 'save_checkpoint_freq': 100, 'wandb_dir_path': '/home/lmeyers/ReID_complete/color_detect_experiments/4_ids_batch1_sample_num_64/', 'wandb_entity_name': 'meyers_luke_lab', 'wandb_project_name': 'color_detect_new_open_04_ids_all_colors_batch1', 'wandb_resume': False, 'wandb_run_id': None}\n",
      "Model Settings:\n",
      "{'latent_dim': 128, 'model_class': 'resnet50_conv3', 'model_path': './4_ids_batch1_sample_num_64/4_ids_batch1_sample_num_64.pth', 'num_labels': 8}\n",
      "Using GPU 0\n",
      "Creating train and valid dataloaders...\n",
      "Batch image shape: torch.Size([64, 3, 250, 250])\n",
      "Batch label shape: torch.Size([64, 16])\n",
      "Loss: BCEWithLogitsLoss() Positive Weight: 7\n",
      "Device: cuda\n",
      "Training model...\n",
      "[1,     4] loss: 1.2058 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3866 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "Saving checkpoint 0\n",
      "[2,     4] loss: 1.1986 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3867 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[3,     4] loss: 1.1942 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3870 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[4,     4] loss: 1.1902 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3869 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[5,     4] loss: 1.1860 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3872 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[6,     4] loss: 1.1824 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3876 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[7,     4] loss: 1.1786 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3878 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[8,     4] loss: 1.1743 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3881 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[9,     4] loss: 1.1704 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3885 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[10,     4] loss: 1.1666 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3888 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[11,     4] loss: 1.1632 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3895 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[12,     4] loss: 1.1588 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3898 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[13,     4] loss: 1.1554 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3901 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[14,     4] loss: 1.1509 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3908 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[15,     4] loss: 1.1474 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3911 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[16,     4] loss: 1.1433 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3916 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[17,     4] loss: 1.1398 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3921 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[18,     4] loss: 1.1363 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3930 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[19,     4] loss: 1.1326 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3932 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[20,     4] loss: 1.1290 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3940 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[21,     4] loss: 1.1252 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3947 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[22,     4] loss: 1.1218 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3953 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[23,     4] loss: 1.1180 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3966 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[24,     4] loss: 1.1149 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3974 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[25,     4] loss: 1.1117 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3980 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[26,     4] loss: 1.1080 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3984 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[27,     4] loss: 1.1052 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3993 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[28,     4] loss: 1.1009 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.3997 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[29,     4] loss: 1.0981 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4011 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[30,     4] loss: 1.0944 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4021 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[31,     4] loss: 1.0914 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4025 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[32,     4] loss: 1.0884 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4038 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[33,     4] loss: 1.0843 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4048 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[34,     4] loss: 1.0814 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4055 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[35,     4] loss: 1.0782 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4059 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[36,     4] loss: 1.0752 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4074 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[37,     4] loss: 1.0731 | train_acc: 0.8730 | train_recall: 0.0000| val_loss: 1.4070 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[38,     4] loss: 1.0696 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4086 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[39,     4] loss: 1.0656 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4088 | val_acc: 0.8749 | val_recall: 0.0010 | f1: 0.0013011150294914842\n",
      "[40,     4] loss: 1.0637 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4101 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[41,     4] loss: 1.0605 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4118 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[42,     4] loss: 1.0571 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4106 | val_acc: 0.8750 | val_recall: 0.0000 | f1: 0.0\n",
      "[43,     4] loss: 1.0536 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4110 | val_acc: 0.8749 | val_recall: 0.0039 | f1: 0.012924655340611935\n",
      "[44,     4] loss: 1.0507 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4121 | val_acc: 0.8744 | val_recall: 0.0039 | f1: 0.004545446950942278\n",
      "[45,     4] loss: 1.0473 | train_acc: 0.8760 | train_recall: 0.0078| val_loss: 1.4120 | val_acc: 0.8750 | val_recall: 0.0029 | f1: 0.009067357517778873\n",
      "[46,     4] loss: 1.0442 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4130 | val_acc: 0.8744 | val_recall: 0.0039 | f1: 0.009067324921488762\n",
      "[47,     4] loss: 1.0420 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4140 | val_acc: 0.8743 | val_recall: 0.0078 | f1: 0.021541208028793335\n",
      "[48,     4] loss: 1.0390 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4144 | val_acc: 0.8745 | val_recall: 0.0098 | f1: 0.025027303025126457\n",
      "[49,     4] loss: 1.0361 | train_acc: 0.8750 | train_recall: 0.0000| val_loss: 1.4137 | val_acc: 0.8746 | val_recall: 0.0117 | f1: 0.03602629154920578\n",
      "[50,     4] loss: 1.0336 | train_acc: 0.8760 | train_recall: 0.0078| val_loss: 1.4163 | val_acc: 0.8750 | val_recall: 0.0107 | f1: 0.03820960596203804\n",
      "[51,     4] loss: 1.0301 | train_acc: 0.8760 | train_recall: 0.0078| val_loss: 1.4170 | val_acc: 0.8748 | val_recall: 0.0117 | f1: 0.034463707357645035\n",
      "[52,     4] loss: 1.0270 | train_acc: 0.8760 | train_recall: 0.0078| val_loss: 1.4182 | val_acc: 0.8751 | val_recall: 0.0127 | f1: 0.037586573511362076\n",
      "[53,     4] loss: 1.0239 | train_acc: 0.8779 | train_recall: 0.0234| val_loss: 1.4169 | val_acc: 0.8743 | val_recall: 0.0234 | f1: 0.06699248403310776\n",
      "[54,     4] loss: 1.0208 | train_acc: 0.8789 | train_recall: 0.0312| val_loss: 1.4165 | val_acc: 0.8735 | val_recall: 0.0254 | f1: 0.06789295375347137\n",
      "[55,     4] loss: 1.0177 | train_acc: 0.8818 | train_recall: 0.0547| val_loss: 1.4173 | val_acc: 0.8737 | val_recall: 0.0352 | f1: 0.09423718601465225\n",
      "[56,     4] loss: 1.0150 | train_acc: 0.8789 | train_recall: 0.0391| val_loss: 1.4204 | val_acc: 0.8734 | val_recall: 0.0303 | f1: 0.0869198590517044\n",
      "[57,     4] loss: 1.0120 | train_acc: 0.8779 | train_recall: 0.0234| val_loss: 1.4209 | val_acc: 0.8719 | val_recall: 0.0303 | f1: 0.06999021023511887\n",
      "[58,     4] loss: 1.0094 | train_acc: 0.8877 | train_recall: 0.1016| val_loss: 1.4208 | val_acc: 0.8721 | val_recall: 0.0400 | f1: 0.10378509014844894\n",
      "[59,     4] loss: 1.0061 | train_acc: 0.8867 | train_recall: 0.0938| val_loss: 1.4208 | val_acc: 0.8712 | val_recall: 0.0488 | f1: 0.11436252295970917\n",
      "[60,     4] loss: 1.0053 | train_acc: 0.8818 | train_recall: 0.0703| val_loss: 1.4247 | val_acc: 0.8700 | val_recall: 0.0342 | f1: 0.07922816276550293\n",
      "[61,     4] loss: 1.0014 | train_acc: 0.8965 | train_recall: 0.1719| val_loss: 1.4226 | val_acc: 0.8679 | val_recall: 0.0498 | f1: 0.10834692418575287\n",
      "[62,     4] loss: 0.9992 | train_acc: 0.8770 | train_recall: 0.1172| val_loss: 1.4233 | val_acc: 0.8691 | val_recall: 0.0605 | f1: 0.14171746373176575\n",
      "[63,     4] loss: 0.9952 | train_acc: 0.8916 | train_recall: 0.1484| val_loss: 1.4211 | val_acc: 0.8673 | val_recall: 0.0840 | f1: 0.17712274193763733\n",
      "[64,     4] loss: 0.9925 | train_acc: 0.9004 | train_recall: 0.2031| val_loss: 1.4233 | val_acc: 0.8660 | val_recall: 0.0732 | f1: 0.1449616402387619\n",
      "[65,     4] loss: 0.9897 | train_acc: 0.9004 | train_recall: 0.2266| val_loss: 1.4261 | val_acc: 0.8662 | val_recall: 0.0869 | f1: 0.18050312995910645\n",
      "[66,     4] loss: 0.9875 | train_acc: 0.9150 | train_recall: 0.3281| val_loss: 1.4249 | val_acc: 0.8643 | val_recall: 0.0938 | f1: 0.19391973316669464\n",
      "[67,     4] loss: 0.9850 | train_acc: 0.9141 | train_recall: 0.3750| val_loss: 1.4293 | val_acc: 0.8601 | val_recall: 0.1016 | f1: 0.19458334147930145\n",
      "[68,     4] loss: 0.9819 | train_acc: 0.9160 | train_recall: 0.4062| val_loss: 1.4276 | val_acc: 0.8618 | val_recall: 0.1221 | f1: 0.23793596029281616\n",
      "[69,     4] loss: 0.9792 | train_acc: 0.9150 | train_recall: 0.3281| val_loss: 1.4263 | val_acc: 0.8589 | val_recall: 0.1289 | f1: 0.24288074672222137\n",
      "[70,     4] loss: 0.9781 | train_acc: 0.9102 | train_recall: 0.4609| val_loss: 1.4287 | val_acc: 0.8491 | val_recall: 0.1445 | f1: 0.25538352131843567\n",
      "[71,     4] loss: 0.9750 | train_acc: 0.9229 | train_recall: 0.4609| val_loss: 1.4271 | val_acc: 0.8523 | val_recall: 0.1514 | f1: 0.2742421329021454\n",
      "[72,     4] loss: 0.9717 | train_acc: 0.9248 | train_recall: 0.6016| val_loss: 1.4295 | val_acc: 0.8505 | val_recall: 0.1680 | f1: 0.2759772539138794\n",
      "[73,     4] loss: 0.9685 | train_acc: 0.9541 | train_recall: 0.7422| val_loss: 1.4311 | val_acc: 0.8401 | val_recall: 0.1738 | f1: 0.27769702672958374\n",
      "[74,     4] loss: 0.9666 | train_acc: 0.9414 | train_recall: 0.5859| val_loss: 1.4326 | val_acc: 0.8429 | val_recall: 0.1934 | f1: 0.31434375047683716\n",
      "[75,     4] loss: 0.9629 | train_acc: 0.9453 | train_recall: 0.7891| val_loss: 1.4310 | val_acc: 0.8328 | val_recall: 0.2090 | f1: 0.3185470998287201\n",
      "[76,     4] loss: 0.9614 | train_acc: 0.9395 | train_recall: 0.8359| val_loss: 1.4321 | val_acc: 0.8292 | val_recall: 0.2061 | f1: 0.3049773573875427\n",
      "[77,     4] loss: 0.9576 | train_acc: 0.9473 | train_recall: 0.8594| val_loss: 1.4323 | val_acc: 0.8267 | val_recall: 0.2217 | f1: 0.32000595331192017\n",
      "[78,     4] loss: 0.9549 | train_acc: 0.9512 | train_recall: 0.8281| val_loss: 1.4295 | val_acc: 0.8275 | val_recall: 0.2197 | f1: 0.325973778963089\n",
      "[79,     4] loss: 0.9530 | train_acc: 0.9463 | train_recall: 0.8984| val_loss: 1.4340 | val_acc: 0.8190 | val_recall: 0.2441 | f1: 0.3287249207496643\n",
      "[80,     4] loss: 0.9497 | train_acc: 0.9453 | train_recall: 0.8672| val_loss: 1.4320 | val_acc: 0.8190 | val_recall: 0.2383 | f1: 0.3227497637271881\n",
      "[81,     4] loss: 0.9477 | train_acc: 0.9453 | train_recall: 0.9375| val_loss: 1.4363 | val_acc: 0.8105 | val_recall: 0.2480 | f1: 0.3431062698364258\n",
      "[82,     4] loss: 0.9452 | train_acc: 0.9561 | train_recall: 0.9609| val_loss: 1.4358 | val_acc: 0.8173 | val_recall: 0.2520 | f1: 0.35795876383781433\n",
      "[83,     4] loss: 0.9424 | train_acc: 0.9570 | train_recall: 1.0000| val_loss: 1.4359 | val_acc: 0.8005 | val_recall: 0.2852 | f1: 0.35813602805137634\n",
      "[84,     4] loss: 0.9406 | train_acc: 0.9414 | train_recall: 0.8906| val_loss: 1.4368 | val_acc: 0.7969 | val_recall: 0.2705 | f1: 0.3403768539428711\n",
      "[85,     4] loss: 0.9385 | train_acc: 0.9434 | train_recall: 0.9688| val_loss: 1.4374 | val_acc: 0.7975 | val_recall: 0.2764 | f1: 0.3489932119846344\n",
      "[86,     4] loss: 0.9349 | train_acc: 0.9463 | train_recall: 0.9609| val_loss: 1.4372 | val_acc: 0.7944 | val_recall: 0.2803 | f1: 0.35454270243644714\n",
      "[87,     4] loss: 0.9327 | train_acc: 0.9521 | train_recall: 0.9453| val_loss: 1.4392 | val_acc: 0.7960 | val_recall: 0.2949 | f1: 0.36344724893569946\n",
      "[88,     4] loss: 0.9310 | train_acc: 0.9502 | train_recall: 0.9453| val_loss: 1.4392 | val_acc: 0.7886 | val_recall: 0.2959 | f1: 0.35462120175361633\n",
      "[89,     4] loss: 0.9275 | train_acc: 0.9434 | train_recall: 0.9609| val_loss: 1.4417 | val_acc: 0.7761 | val_recall: 0.3066 | f1: 0.35019248723983765\n",
      "[90,     4] loss: 0.9271 | train_acc: 0.9365 | train_recall: 0.9531| val_loss: 1.4382 | val_acc: 0.7827 | val_recall: 0.2969 | f1: 0.35593265295028687\n",
      "[91,     4] loss: 0.9243 | train_acc: 0.9395 | train_recall: 0.9766| val_loss: 1.4412 | val_acc: 0.7771 | val_recall: 0.3154 | f1: 0.3581699728965759\n",
      "[92,     4] loss: 0.9222 | train_acc: 0.9482 | train_recall: 0.9297| val_loss: 1.4392 | val_acc: 0.7856 | val_recall: 0.3076 | f1: 0.36003798246383667\n",
      "[93,     4] loss: 0.9192 | train_acc: 0.9326 | train_recall: 0.9766| val_loss: 1.4454 | val_acc: 0.7693 | val_recall: 0.3340 | f1: 0.36446651816368103\n",
      "[94,     4] loss: 0.9173 | train_acc: 0.9414 | train_recall: 0.9766| val_loss: 1.4451 | val_acc: 0.7720 | val_recall: 0.3301 | f1: 0.3638169467449188\n",
      "[95,     4] loss: 0.9131 | train_acc: 0.9424 | train_recall: 0.9844| val_loss: 1.4451 | val_acc: 0.7771 | val_recall: 0.3418 | f1: 0.377104252576828\n",
      "[96,     4] loss: 0.9119 | train_acc: 0.9346 | train_recall: 1.0000| val_loss: 1.4430 | val_acc: 0.7740 | val_recall: 0.3379 | f1: 0.37281718850135803\n",
      "[97,     4] loss: 0.9088 | train_acc: 0.9336 | train_recall: 1.0000| val_loss: 1.4457 | val_acc: 0.7731 | val_recall: 0.3389 | f1: 0.366104394197464\n",
      "[98,     4] loss: 0.9083 | train_acc: 0.9297 | train_recall: 0.9844| val_loss: 1.4492 | val_acc: 0.7557 | val_recall: 0.3457 | f1: 0.3473409116268158\n",
      "[99,     4] loss: 0.9051 | train_acc: 0.9443 | train_recall: 1.0000| val_loss: 1.4450 | val_acc: 0.7710 | val_recall: 0.3320 | f1: 0.36271917819976807\n",
      "[100,     4] loss: 0.9026 | train_acc: 0.9463 | train_recall: 1.0000| val_loss: 1.4436 | val_acc: 0.7811 | val_recall: 0.3281 | f1: 0.3665813207626343\n",
      "[101,     4] loss: 0.8995 | train_acc: 0.9336 | train_recall: 1.0000| val_loss: 1.4492 | val_acc: 0.7662 | val_recall: 0.3330 | f1: 0.3542390465736389\n",
      "Saving checkpoint 100\n",
      "[102,     4] loss: 0.8980 | train_acc: 0.9365 | train_recall: 0.9922| val_loss: 1.4425 | val_acc: 0.7751 | val_recall: 0.3301 | f1: 0.3652344346046448\n",
      "[103,     4] loss: 0.8951 | train_acc: 0.9248 | train_recall: 0.9844| val_loss: 1.4520 | val_acc: 0.7657 | val_recall: 0.3389 | f1: 0.3644081652164459\n",
      "[104,     4] loss: 0.8931 | train_acc: 0.9336 | train_recall: 0.9844| val_loss: 1.4485 | val_acc: 0.7633 | val_recall: 0.3467 | f1: 0.36258116364479065\n",
      "[105,     4] loss: 0.8894 | train_acc: 0.9277 | train_recall: 1.0000| val_loss: 1.4522 | val_acc: 0.7651 | val_recall: 0.3457 | f1: 0.368068665266037\n",
      "[106,     4] loss: 0.8874 | train_acc: 0.9297 | train_recall: 1.0000| val_loss: 1.4504 | val_acc: 0.7593 | val_recall: 0.3428 | f1: 0.3618227541446686\n",
      "[107,     4] loss: 0.8846 | train_acc: 0.9404 | train_recall: 1.0000| val_loss: 1.4505 | val_acc: 0.7640 | val_recall: 0.3398 | f1: 0.36152660846710205\n",
      "[108,     4] loss: 0.8836 | train_acc: 0.9258 | train_recall: 1.0000| val_loss: 1.4520 | val_acc: 0.7643 | val_recall: 0.3418 | f1: 0.3629941940307617\n",
      "[109,     4] loss: 0.8818 | train_acc: 0.9434 | train_recall: 1.0000| val_loss: 1.4523 | val_acc: 0.7587 | val_recall: 0.3418 | f1: 0.3583448827266693\n",
      "[110,     4] loss: 0.8797 | train_acc: 0.9297 | train_recall: 0.9531| val_loss: 1.4532 | val_acc: 0.7588 | val_recall: 0.3477 | f1: 0.36579400300979614\n",
      "[111,     4] loss: 0.8767 | train_acc: 0.9365 | train_recall: 1.0000| val_loss: 1.4544 | val_acc: 0.7606 | val_recall: 0.3447 | f1: 0.36114269495010376\n",
      "[112,     4] loss: 0.8759 | train_acc: 0.9346 | train_recall: 0.9844| val_loss: 1.4537 | val_acc: 0.7570 | val_recall: 0.3525 | f1: 0.3572045564651489\n",
      "[113,     4] loss: 0.8715 | train_acc: 0.9434 | train_recall: 1.0000| val_loss: 1.4531 | val_acc: 0.7590 | val_recall: 0.3535 | f1: 0.3658973276615143\n",
      "[114,     4] loss: 0.8700 | train_acc: 0.9238 | train_recall: 1.0000| val_loss: 1.4516 | val_acc: 0.7604 | val_recall: 0.3574 | f1: 0.36691585183143616\n",
      "[115,     4] loss: 0.8674 | train_acc: 0.9492 | train_recall: 1.0000| val_loss: 1.4579 | val_acc: 0.7561 | val_recall: 0.3623 | f1: 0.3669392764568329\n",
      "[116,     4] loss: 0.8658 | train_acc: 0.9375 | train_recall: 1.0000| val_loss: 1.4563 | val_acc: 0.7587 | val_recall: 0.3584 | f1: 0.3626212775707245\n",
      "[117,     4] loss: 0.8619 | train_acc: 0.9395 | train_recall: 1.0000| val_loss: 1.4607 | val_acc: 0.7515 | val_recall: 0.3623 | f1: 0.3581025302410126\n",
      "[118,     4] loss: 0.8608 | train_acc: 0.9326 | train_recall: 1.0000| val_loss: 1.4575 | val_acc: 0.7576 | val_recall: 0.3555 | f1: 0.3591703772544861\n",
      "[119,     4] loss: 0.8574 | train_acc: 0.9443 | train_recall: 1.0000| val_loss: 1.4554 | val_acc: 0.7565 | val_recall: 0.3564 | f1: 0.35642778873443604\n",
      "[120,     4] loss: 0.8569 | train_acc: 0.9473 | train_recall: 1.0000| val_loss: 1.4583 | val_acc: 0.7585 | val_recall: 0.3574 | f1: 0.3648650050163269\n",
      "[121,     4] loss: 0.8552 | train_acc: 0.9209 | train_recall: 1.0000| val_loss: 1.4612 | val_acc: 0.7600 | val_recall: 0.3525 | f1: 0.36394307017326355\n",
      "[122,     4] loss: 0.8523 | train_acc: 0.9297 | train_recall: 1.0000| val_loss: 1.4626 | val_acc: 0.7522 | val_recall: 0.3701 | f1: 0.3591674268245697\n",
      "[123,     4] loss: 0.8510 | train_acc: 0.9453 | train_recall: 1.0000| val_loss: 1.4596 | val_acc: 0.7573 | val_recall: 0.3672 | f1: 0.3649112284183502\n",
      "[124,     4] loss: 0.8480 | train_acc: 0.9365 | train_recall: 1.0000| val_loss: 1.4582 | val_acc: 0.7589 | val_recall: 0.3545 | f1: 0.3640051484107971\n",
      "[125,     4] loss: 0.8460 | train_acc: 0.9424 | train_recall: 1.0000| val_loss: 1.4637 | val_acc: 0.7534 | val_recall: 0.3662 | f1: 0.36092767119407654\n",
      "[126,     4] loss: 0.8433 | train_acc: 0.9443 | train_recall: 1.0000| val_loss: 1.4587 | val_acc: 0.7620 | val_recall: 0.3662 | f1: 0.37106335163116455\n",
      "[127,     4] loss: 0.8416 | train_acc: 0.9385 | train_recall: 1.0000| val_loss: 1.4577 | val_acc: 0.7605 | val_recall: 0.3652 | f1: 0.37006986141204834\n",
      "[128,     4] loss: 0.8406 | train_acc: 0.9346 | train_recall: 1.0000| val_loss: 1.4660 | val_acc: 0.7601 | val_recall: 0.3613 | f1: 0.36440885066986084\n",
      "[129,     4] loss: 0.8376 | train_acc: 0.9492 | train_recall: 1.0000| val_loss: 1.4641 | val_acc: 0.7546 | val_recall: 0.3662 | f1: 0.3647858500480652\n",
      "[130,     4] loss: 0.8357 | train_acc: 0.9424 | train_recall: 1.0000| val_loss: 1.4650 | val_acc: 0.7513 | val_recall: 0.3711 | f1: 0.3595975935459137\n",
      "[131,     4] loss: 0.8338 | train_acc: 0.9346 | train_recall: 1.0000| val_loss: 1.4668 | val_acc: 0.7555 | val_recall: 0.3594 | f1: 0.3576104938983917\n",
      "[132,     4] loss: 0.8318 | train_acc: 0.9492 | train_recall: 1.0000| val_loss: 1.4660 | val_acc: 0.7532 | val_recall: 0.3691 | f1: 0.3611631393432617\n",
      "[133,     4] loss: 0.8292 | train_acc: 0.9375 | train_recall: 1.0000| val_loss: 1.4623 | val_acc: 0.7582 | val_recall: 0.3623 | f1: 0.356579065322876\n",
      "[134,     4] loss: 0.8277 | train_acc: 0.9434 | train_recall: 0.9922| val_loss: 1.4644 | val_acc: 0.7574 | val_recall: 0.3574 | f1: 0.36187854409217834\n",
      "[135,     4] loss: 0.8256 | train_acc: 0.9258 | train_recall: 1.0000| val_loss: 1.4678 | val_acc: 0.7626 | val_recall: 0.3584 | f1: 0.3642362654209137\n",
      "[136,     4] loss: 0.8242 | train_acc: 0.9297 | train_recall: 1.0000| val_loss: 1.4691 | val_acc: 0.7567 | val_recall: 0.3555 | f1: 0.3554266691207886\n",
      "[137,     4] loss: 0.8209 | train_acc: 0.9219 | train_recall: 1.0000| val_loss: 1.4716 | val_acc: 0.7604 | val_recall: 0.3516 | f1: 0.35842111706733704\n",
      "[138,     4] loss: 0.8183 | train_acc: 0.9385 | train_recall: 1.0000| val_loss: 1.4685 | val_acc: 0.7583 | val_recall: 0.3564 | f1: 0.35628780722618103\n",
      "[139,     4] loss: 0.8168 | train_acc: 0.9189 | train_recall: 1.0000| val_loss: 1.4660 | val_acc: 0.7639 | val_recall: 0.3555 | f1: 0.36480435729026794\n",
      "[140,     4] loss: 0.8153 | train_acc: 0.9316 | train_recall: 1.0000| val_loss: 1.4735 | val_acc: 0.7543 | val_recall: 0.3652 | f1: 0.35766303539276123\n",
      "[141,     4] loss: 0.8115 | train_acc: 0.9336 | train_recall: 1.0000| val_loss: 1.4747 | val_acc: 0.7551 | val_recall: 0.3701 | f1: 0.3655913770198822\n",
      "[142,     4] loss: 0.8100 | train_acc: 0.9404 | train_recall: 1.0000| val_loss: 1.4678 | val_acc: 0.7629 | val_recall: 0.3623 | f1: 0.3696857988834381\n",
      "[143,     4] loss: 0.8093 | train_acc: 0.9258 | train_recall: 1.0000| val_loss: 1.4706 | val_acc: 0.7585 | val_recall: 0.3652 | f1: 0.3656906187534332\n",
      "[144,     4] loss: 0.8064 | train_acc: 0.9336 | train_recall: 1.0000| val_loss: 1.4734 | val_acc: 0.7542 | val_recall: 0.3652 | f1: 0.3639048635959625\n",
      "[145,     4] loss: 0.8063 | train_acc: 0.9336 | train_recall: 0.9844| val_loss: 1.4733 | val_acc: 0.7549 | val_recall: 0.3564 | f1: 0.3618863821029663\n",
      "[146,     4] loss: 0.8027 | train_acc: 0.9326 | train_recall: 1.0000| val_loss: 1.4755 | val_acc: 0.7595 | val_recall: 0.3594 | f1: 0.3631713390350342\n",
      "[147,     4] loss: 0.8024 | train_acc: 0.9395 | train_recall: 1.0000| val_loss: 1.4748 | val_acc: 0.7397 | val_recall: 0.3799 | f1: 0.3435553014278412\n",
      "[148,     4] loss: 0.8024 | train_acc: 0.9346 | train_recall: 0.9844| val_loss: 1.4751 | val_acc: 0.7545 | val_recall: 0.3672 | f1: 0.3595445454120636\n",
      "[149,     4] loss: 0.7966 | train_acc: 0.9463 | train_recall: 1.0000| val_loss: 1.4730 | val_acc: 0.7666 | val_recall: 0.3574 | f1: 0.36985185742378235\n",
      "[150,     4] loss: 0.7943 | train_acc: 0.9395 | train_recall: 1.0000| val_loss: 1.4731 | val_acc: 0.7620 | val_recall: 0.3564 | f1: 0.3623916804790497\n",
      "[151,     4] loss: 0.7930 | train_acc: 0.9404 | train_recall: 1.0000| val_loss: 1.4742 | val_acc: 0.7582 | val_recall: 0.3623 | f1: 0.3620752990245819\n",
      "[152,     4] loss: 0.7936 | train_acc: 0.9395 | train_recall: 1.0000| val_loss: 1.4826 | val_acc: 0.7552 | val_recall: 0.3652 | f1: 0.35534098744392395\n",
      "[153,     4] loss: 0.7930 | train_acc: 0.9355 | train_recall: 0.9688| val_loss: 1.4792 | val_acc: 0.7504 | val_recall: 0.3750 | f1: 0.3529348373413086\n",
      "[154,     4] loss: 0.7902 | train_acc: 0.9307 | train_recall: 1.0000| val_loss: 1.4833 | val_acc: 0.7535 | val_recall: 0.3809 | f1: 0.36387118697166443\n",
      "[155,     4] loss: 0.7877 | train_acc: 0.9346 | train_recall: 1.0000| val_loss: 1.4763 | val_acc: 0.7621 | val_recall: 0.3740 | f1: 0.3724548816680908\n",
      "[156,     4] loss: 0.7843 | train_acc: 0.9355 | train_recall: 1.0000| val_loss: 1.4887 | val_acc: 0.7490 | val_recall: 0.3750 | f1: 0.35540565848350525\n",
      "[157,     4] loss: 0.7827 | train_acc: 0.9365 | train_recall: 1.0000| val_loss: 1.4803 | val_acc: 0.7581 | val_recall: 0.3643 | f1: 0.3613441586494446\n",
      "[158,     4] loss: 0.7846 | train_acc: 0.9326 | train_recall: 0.9688| val_loss: 1.4836 | val_acc: 0.7617 | val_recall: 0.3584 | f1: 0.36939293146133423\n",
      "[159,     4] loss: 0.7789 | train_acc: 0.9355 | train_recall: 1.0000| val_loss: 1.4816 | val_acc: 0.7639 | val_recall: 0.3564 | f1: 0.3695010542869568\n",
      "[160,     4] loss: 0.7776 | train_acc: 0.9424 | train_recall: 1.0000| val_loss: 1.4833 | val_acc: 0.7502 | val_recall: 0.3564 | f1: 0.3489472568035126\n",
      "[161,     4] loss: 0.7738 | train_acc: 0.9355 | train_recall: 1.0000| val_loss: 1.4837 | val_acc: 0.7527 | val_recall: 0.3594 | f1: 0.3524657189846039\n",
      "[162,     4] loss: 0.7730 | train_acc: 0.9336 | train_recall: 1.0000| val_loss: 1.4852 | val_acc: 0.7609 | val_recall: 0.3672 | f1: 0.3667851686477661\n",
      "[163,     4] loss: 0.7718 | train_acc: 0.9336 | train_recall: 1.0000| val_loss: 1.4866 | val_acc: 0.7528 | val_recall: 0.3711 | f1: 0.36134666204452515\n",
      "[164,     4] loss: 0.7689 | train_acc: 0.9414 | train_recall: 1.0000| val_loss: 1.4789 | val_acc: 0.7537 | val_recall: 0.3672 | f1: 0.3607673943042755\n",
      "[165,     4] loss: 0.7659 | train_acc: 0.9521 | train_recall: 1.0000| val_loss: 1.4805 | val_acc: 0.7633 | val_recall: 0.3662 | f1: 0.37140876054763794\n",
      "[166,     4] loss: 0.7656 | train_acc: 0.9424 | train_recall: 1.0000| val_loss: 1.4858 | val_acc: 0.7634 | val_recall: 0.3662 | f1: 0.3737301230430603\n",
      "[167,     4] loss: 0.7634 | train_acc: 0.9404 | train_recall: 1.0000| val_loss: 1.4940 | val_acc: 0.7570 | val_recall: 0.3730 | f1: 0.3655433654785156\n",
      "[168,     4] loss: 0.7608 | train_acc: 0.9404 | train_recall: 1.0000| val_loss: 1.4868 | val_acc: 0.7573 | val_recall: 0.3672 | f1: 0.3657359182834625\n",
      "[169,     4] loss: 0.7607 | train_acc: 0.9375 | train_recall: 1.0000| val_loss: 1.4829 | val_acc: 0.7615 | val_recall: 0.3623 | f1: 0.3662930130958557\n",
      "[170,     4] loss: 0.7573 | train_acc: 0.9385 | train_recall: 1.0000| val_loss: 1.4861 | val_acc: 0.7592 | val_recall: 0.3584 | f1: 0.3641836941242218\n",
      "[171,     4] loss: 0.7543 | train_acc: 0.9443 | train_recall: 1.0000| val_loss: 1.4871 | val_acc: 0.7600 | val_recall: 0.3574 | f1: 0.36213260889053345\n",
      "[172,     4] loss: 0.7542 | train_acc: 0.9414 | train_recall: 1.0000| val_loss: 1.4866 | val_acc: 0.7592 | val_recall: 0.3535 | f1: 0.35843732953071594\n",
      "[173,     4] loss: 0.7520 | train_acc: 0.9316 | train_recall: 1.0000| val_loss: 1.4891 | val_acc: 0.7609 | val_recall: 0.3545 | f1: 0.36313596367836\n",
      "[174,     4] loss: 0.7495 | train_acc: 0.9385 | train_recall: 1.0000| val_loss: 1.4960 | val_acc: 0.7566 | val_recall: 0.3623 | f1: 0.35860586166381836\n",
      "[175,     4] loss: 0.7486 | train_acc: 0.9385 | train_recall: 1.0000| val_loss: 1.4935 | val_acc: 0.7590 | val_recall: 0.3643 | f1: 0.3637557625770569\n",
      "[176,     4] loss: 0.7478 | train_acc: 0.9375 | train_recall: 0.9922| val_loss: 1.4936 | val_acc: 0.7628 | val_recall: 0.3594 | f1: 0.365545392036438\n",
      "[177,     4] loss: 0.7460 | train_acc: 0.9541 | train_recall: 1.0000| val_loss: 1.4967 | val_acc: 0.7582 | val_recall: 0.3643 | f1: 0.3604886531829834\n",
      "[178,     4] loss: 0.7432 | train_acc: 0.9521 | train_recall: 1.0000| val_loss: 1.4894 | val_acc: 0.7632 | val_recall: 0.3633 | f1: 0.3674669861793518\n",
      "[179,     4] loss: 0.7409 | train_acc: 0.9492 | train_recall: 1.0000| val_loss: 1.4919 | val_acc: 0.7637 | val_recall: 0.3584 | f1: 0.36849889159202576\n",
      "[180,     4] loss: 0.7392 | train_acc: 0.9561 | train_recall: 1.0000| val_loss: 1.4993 | val_acc: 0.7600 | val_recall: 0.3594 | f1: 0.3654099702835083\n",
      "[181,     4] loss: 0.7383 | train_acc: 0.9414 | train_recall: 1.0000| val_loss: 1.4954 | val_acc: 0.7606 | val_recall: 0.3652 | f1: 0.3664567470550537\n",
      "[182,     4] loss: 0.7362 | train_acc: 0.9395 | train_recall: 1.0000| val_loss: 1.4930 | val_acc: 0.7594 | val_recall: 0.3564 | f1: 0.3598308563232422\n",
      "[183,     4] loss: 0.7348 | train_acc: 0.9521 | train_recall: 1.0000| val_loss: 1.5012 | val_acc: 0.7590 | val_recall: 0.3613 | f1: 0.36194610595703125\n",
      "[184,     4] loss: 0.7326 | train_acc: 0.9355 | train_recall: 1.0000| val_loss: 1.5002 | val_acc: 0.7565 | val_recall: 0.3506 | f1: 0.35318514704704285\n",
      "[185,     4] loss: 0.7314 | train_acc: 0.9482 | train_recall: 1.0000| val_loss: 1.4952 | val_acc: 0.7673 | val_recall: 0.3545 | f1: 0.3681718409061432\n",
      "[186,     4] loss: 0.7287 | train_acc: 0.9609 | train_recall: 1.0000| val_loss: 1.4982 | val_acc: 0.7681 | val_recall: 0.3613 | f1: 0.374208927154541\n",
      "[187,     4] loss: 0.7277 | train_acc: 0.9434 | train_recall: 1.0000| val_loss: 1.4993 | val_acc: 0.7639 | val_recall: 0.3652 | f1: 0.365859717130661\n",
      "[188,     4] loss: 0.7264 | train_acc: 0.9512 | train_recall: 1.0000| val_loss: 1.4950 | val_acc: 0.7664 | val_recall: 0.3604 | f1: 0.36993587017059326\n",
      "[189,     4] loss: 0.7253 | train_acc: 0.9482 | train_recall: 1.0000| val_loss: 1.4950 | val_acc: 0.7712 | val_recall: 0.3652 | f1: 0.37897202372550964\n",
      "[190,     4] loss: 0.7222 | train_acc: 0.9590 | train_recall: 1.0000| val_loss: 1.4984 | val_acc: 0.7711 | val_recall: 0.3623 | f1: 0.38338518142700195\n",
      "[191,     4] loss: 0.7208 | train_acc: 0.9580 | train_recall: 1.0000| val_loss: 1.4942 | val_acc: 0.7754 | val_recall: 0.3594 | f1: 0.3850158452987671\n",
      "[192,     4] loss: 0.7191 | train_acc: 0.9688 | train_recall: 1.0000| val_loss: 1.4946 | val_acc: 0.7737 | val_recall: 0.3604 | f1: 0.37952572107315063\n",
      "[193,     4] loss: 0.7168 | train_acc: 0.9609 | train_recall: 1.0000| val_loss: 1.5010 | val_acc: 0.7683 | val_recall: 0.3662 | f1: 0.37880218029022217\n",
      "[194,     4] loss: 0.7153 | train_acc: 0.9658 | train_recall: 1.0000| val_loss: 1.4989 | val_acc: 0.7719 | val_recall: 0.3633 | f1: 0.3827396333217621\n",
      "[195,     4] loss: 0.7161 | train_acc: 0.9639 | train_recall: 1.0000| val_loss: 1.4997 | val_acc: 0.7708 | val_recall: 0.3623 | f1: 0.3764266073703766\n",
      "[196,     4] loss: 0.7114 | train_acc: 0.9463 | train_recall: 1.0000| val_loss: 1.5112 | val_acc: 0.7601 | val_recall: 0.3535 | f1: 0.36868324875831604\n",
      "[197,     4] loss: 0.7119 | train_acc: 0.9512 | train_recall: 1.0000| val_loss: 1.5082 | val_acc: 0.7695 | val_recall: 0.3564 | f1: 0.37998515367507935\n",
      "[198,     4] loss: 0.7085 | train_acc: 0.9580 | train_recall: 1.0000| val_loss: 1.5016 | val_acc: 0.7723 | val_recall: 0.3418 | f1: 0.3771737515926361\n",
      "[199,     4] loss: 0.7074 | train_acc: 0.9688 | train_recall: 1.0000| val_loss: 1.5059 | val_acc: 0.7748 | val_recall: 0.3438 | f1: 0.3779110014438629\n",
      "[200,     4] loss: 0.7067 | train_acc: 0.9629 | train_recall: 1.0000| val_loss: 1.5083 | val_acc: 0.7709 | val_recall: 0.3408 | f1: 0.37166091799736023\n",
      "[201,     4] loss: 0.7036 | train_acc: 0.9648 | train_recall: 1.0000| val_loss: 1.5058 | val_acc: 0.7743 | val_recall: 0.3369 | f1: 0.37982165813446045\n",
      "Saving checkpoint 200\n",
      "[202,     4] loss: 0.7014 | train_acc: 0.9512 | train_recall: 1.0000| val_loss: 1.5040 | val_acc: 0.7754 | val_recall: 0.3408 | f1: 0.38143622875213623\n",
      "[203,     4] loss: 0.7023 | train_acc: 0.9736 | train_recall: 1.0000| val_loss: 1.5086 | val_acc: 0.7714 | val_recall: 0.3398 | f1: 0.3735153377056122\n",
      "[204,     4] loss: 0.6982 | train_acc: 0.9697 | train_recall: 1.0000| val_loss: 1.5098 | val_acc: 0.7728 | val_recall: 0.3428 | f1: 0.37525680661201477\n",
      "[205,     4] loss: 0.6978 | train_acc: 0.9736 | train_recall: 1.0000| val_loss: 1.5095 | val_acc: 0.7765 | val_recall: 0.3428 | f1: 0.3788587152957916\n",
      "[206,     4] loss: 0.6942 | train_acc: 0.9785 | train_recall: 1.0000| val_loss: 1.5093 | val_acc: 0.7775 | val_recall: 0.3428 | f1: 0.3810192346572876\n",
      "[207,     4] loss: 0.6962 | train_acc: 0.9766 | train_recall: 1.0000| val_loss: 1.5146 | val_acc: 0.7721 | val_recall: 0.3438 | f1: 0.37453579902648926\n",
      "[208,     4] loss: 0.6924 | train_acc: 0.9883 | train_recall: 1.0000| val_loss: 1.5064 | val_acc: 0.7811 | val_recall: 0.3389 | f1: 0.3867521584033966\n",
      "[209,     4] loss: 0.6910 | train_acc: 0.9688 | train_recall: 1.0000| val_loss: 1.5054 | val_acc: 0.7860 | val_recall: 0.3438 | f1: 0.39487922191619873\n",
      "[210,     4] loss: 0.6896 | train_acc: 0.9727 | train_recall: 1.0000| val_loss: 1.5115 | val_acc: 0.7817 | val_recall: 0.3545 | f1: 0.3917474150657654\n",
      "[211,     4] loss: 0.6885 | train_acc: 0.9775 | train_recall: 1.0000| val_loss: 1.5119 | val_acc: 0.7781 | val_recall: 0.3574 | f1: 0.38766318559646606\n",
      "[212,     4] loss: 0.6853 | train_acc: 0.9795 | train_recall: 1.0000| val_loss: 1.5109 | val_acc: 0.7805 | val_recall: 0.3535 | f1: 0.3861243426799774\n",
      "[213,     4] loss: 0.6843 | train_acc: 0.9795 | train_recall: 1.0000| val_loss: 1.5141 | val_acc: 0.7767 | val_recall: 0.3516 | f1: 0.38344794511795044\n",
      "[214,     4] loss: 0.6836 | train_acc: 0.9824 | train_recall: 1.0000| val_loss: 1.5121 | val_acc: 0.7778 | val_recall: 0.3438 | f1: 0.3783847689628601\n",
      "[215,     4] loss: 0.6815 | train_acc: 0.9883 | train_recall: 1.0000| val_loss: 1.5151 | val_acc: 0.7803 | val_recall: 0.3350 | f1: 0.3803509771823883\n",
      "[216,     4] loss: 0.6794 | train_acc: 0.9697 | train_recall: 1.0000| val_loss: 1.5163 | val_acc: 0.7787 | val_recall: 0.3428 | f1: 0.37811264395713806\n",
      "[217,     4] loss: 0.6801 | train_acc: 0.9854 | train_recall: 1.0000| val_loss: 1.5230 | val_acc: 0.7747 | val_recall: 0.3379 | f1: 0.37390053272247314\n",
      "[218,     4] loss: 0.6768 | train_acc: 0.9805 | train_recall: 1.0000| val_loss: 1.5203 | val_acc: 0.7798 | val_recall: 0.3379 | f1: 0.3811856806278229\n",
      "[219,     4] loss: 0.6759 | train_acc: 0.9893 | train_recall: 1.0000| val_loss: 1.5176 | val_acc: 0.7783 | val_recall: 0.3418 | f1: 0.3840844929218292\n",
      "[220,     4] loss: 0.6756 | train_acc: 0.9854 | train_recall: 1.0000| val_loss: 1.5208 | val_acc: 0.7745 | val_recall: 0.3428 | f1: 0.377956360578537\n",
      "[221,     4] loss: 0.6724 | train_acc: 0.9785 | train_recall: 1.0000| val_loss: 1.5252 | val_acc: 0.7747 | val_recall: 0.3516 | f1: 0.37893763184547424\n",
      "[222,     4] loss: 0.6716 | train_acc: 0.9834 | train_recall: 1.0000| val_loss: 1.5202 | val_acc: 0.7789 | val_recall: 0.3477 | f1: 0.37944668531417847\n",
      "[223,     4] loss: 0.6709 | train_acc: 0.9863 | train_recall: 1.0000| val_loss: 1.5219 | val_acc: 0.7798 | val_recall: 0.3438 | f1: 0.38585034012794495\n",
      "[224,     4] loss: 0.6695 | train_acc: 0.9922 | train_recall: 1.0000| val_loss: 1.5222 | val_acc: 0.7816 | val_recall: 0.3457 | f1: 0.38625866174697876\n",
      "[225,     4] loss: 0.6663 | train_acc: 0.9922 | train_recall: 1.0000| val_loss: 1.5242 | val_acc: 0.7814 | val_recall: 0.3418 | f1: 0.3828984200954437\n",
      "[226,     4] loss: 0.6656 | train_acc: 0.9883 | train_recall: 1.0000| val_loss: 1.5146 | val_acc: 0.7864 | val_recall: 0.3438 | f1: 0.3943774700164795\n",
      "[227,     4] loss: 0.6636 | train_acc: 0.9971 | train_recall: 1.0000| val_loss: 1.5199 | val_acc: 0.7841 | val_recall: 0.3467 | f1: 0.39605244994163513\n",
      "[228,     4] loss: 0.6616 | train_acc: 0.9951 | train_recall: 1.0000| val_loss: 1.5216 | val_acc: 0.7834 | val_recall: 0.3369 | f1: 0.38998156785964966\n",
      "[229,     4] loss: 0.6613 | train_acc: 0.9922 | train_recall: 1.0000| val_loss: 1.5255 | val_acc: 0.7786 | val_recall: 0.3291 | f1: 0.37731391191482544\n",
      "[230,     4] loss: 0.6595 | train_acc: 0.9814 | train_recall: 1.0000| val_loss: 1.5281 | val_acc: 0.7795 | val_recall: 0.3340 | f1: 0.38341906666755676\n",
      "[231,     4] loss: 0.6584 | train_acc: 0.9912 | train_recall: 1.0000| val_loss: 1.5264 | val_acc: 0.7822 | val_recall: 0.3398 | f1: 0.3857421278953552\n",
      "[232,     4] loss: 0.6581 | train_acc: 0.9941 | train_recall: 1.0000| val_loss: 1.5210 | val_acc: 0.7871 | val_recall: 0.3428 | f1: 0.39234498143196106\n",
      "[233,     4] loss: 0.6589 | train_acc: 0.9805 | train_recall: 0.9844| val_loss: 1.5292 | val_acc: 0.7799 | val_recall: 0.3359 | f1: 0.3834633231163025\n",
      "[234,     4] loss: 0.6582 | train_acc: 0.9863 | train_recall: 1.0000| val_loss: 1.5281 | val_acc: 0.7855 | val_recall: 0.3398 | f1: 0.389870285987854\n",
      "[235,     4] loss: 0.6566 | train_acc: 0.9717 | train_recall: 1.0000| val_loss: 1.5358 | val_acc: 0.7744 | val_recall: 0.3398 | f1: 0.37480801343917847\n",
      "[236,     4] loss: 0.6548 | train_acc: 0.9863 | train_recall: 1.0000| val_loss: 1.5280 | val_acc: 0.7869 | val_recall: 0.3457 | f1: 0.39312121272087097\n",
      "[237,     4] loss: 0.6512 | train_acc: 0.9883 | train_recall: 1.0000| val_loss: 1.5311 | val_acc: 0.7795 | val_recall: 0.3281 | f1: 0.37623047828674316\n",
      "[238,     4] loss: 0.6507 | train_acc: 0.9893 | train_recall: 1.0000| val_loss: 1.5282 | val_acc: 0.7798 | val_recall: 0.3457 | f1: 0.3859240710735321\n",
      "[239,     4] loss: 0.6503 | train_acc: 0.9883 | train_recall: 1.0000| val_loss: 1.5273 | val_acc: 0.7831 | val_recall: 0.3486 | f1: 0.3932318091392517\n",
      "[240,     4] loss: 0.6464 | train_acc: 0.9922 | train_recall: 1.0000| val_loss: 1.5338 | val_acc: 0.7865 | val_recall: 0.3389 | f1: 0.3909102976322174\n",
      "[241,     4] loss: 0.6458 | train_acc: 0.9922 | train_recall: 1.0000| val_loss: 1.5382 | val_acc: 0.7781 | val_recall: 0.3525 | f1: 0.38729599118232727\n",
      "[242,     4] loss: 0.6437 | train_acc: 0.9971 | train_recall: 1.0000| val_loss: 1.5319 | val_acc: 0.7837 | val_recall: 0.3291 | f1: 0.3855869770050049\n",
      "[243,     4] loss: 0.6428 | train_acc: 0.9961 | train_recall: 1.0000| val_loss: 1.5269 | val_acc: 0.7865 | val_recall: 0.3291 | f1: 0.3909102976322174\n",
      "[244,     4] loss: 0.6394 | train_acc: 0.9961 | train_recall: 1.0000| val_loss: 1.5312 | val_acc: 0.7821 | val_recall: 0.3428 | f1: 0.3925609290599823\n",
      "[245,     4] loss: 0.6392 | train_acc: 0.9912 | train_recall: 1.0000| val_loss: 1.5321 | val_acc: 0.7844 | val_recall: 0.3438 | f1: 0.3941313922405243\n",
      "[246,     4] loss: 0.6366 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.5380 | val_acc: 0.7812 | val_recall: 0.3350 | f1: 0.38046690821647644\n",
      "[247,     4] loss: 0.6346 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.5380 | val_acc: 0.7811 | val_recall: 0.3398 | f1: 0.3847953677177429\n",
      "[248,     4] loss: 0.6328 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.5363 | val_acc: 0.7856 | val_recall: 0.3379 | f1: 0.3930053412914276\n",
      "[249,     4] loss: 0.6325 | train_acc: 0.9922 | train_recall: 1.0000| val_loss: 1.5340 | val_acc: 0.7877 | val_recall: 0.3398 | f1: 0.3960074484348297\n",
      "[250,     4] loss: 0.6330 | train_acc: 0.9922 | train_recall: 1.0000| val_loss: 1.5459 | val_acc: 0.7848 | val_recall: 0.3242 | f1: 0.38442206382751465\n",
      "[251,     4] loss: 0.6340 | train_acc: 0.9902 | train_recall: 0.9844| val_loss: 1.5379 | val_acc: 0.7850 | val_recall: 0.3428 | f1: 0.3922698497772217\n",
      "[252,     4] loss: 0.6352 | train_acc: 0.9912 | train_recall: 1.0000| val_loss: 1.5327 | val_acc: 0.7893 | val_recall: 0.3320 | f1: 0.3994857966899872\n",
      "[253,     4] loss: 0.6337 | train_acc: 0.9805 | train_recall: 1.0000| val_loss: 1.5347 | val_acc: 0.7858 | val_recall: 0.3408 | f1: 0.3953956365585327\n",
      "[254,     4] loss: 0.6315 | train_acc: 0.9990 | train_recall: 1.0000| val_loss: 1.5381 | val_acc: 0.7645 | val_recall: 0.3359 | f1: 0.35723841190338135\n",
      "[255,     4] loss: 0.6305 | train_acc: 0.9922 | train_recall: 0.9844| val_loss: 1.5350 | val_acc: 0.7834 | val_recall: 0.3408 | f1: 0.395356148481369\n",
      "[256,     4] loss: 0.6281 | train_acc: 0.9824 | train_recall: 1.0000| val_loss: 1.5315 | val_acc: 0.7947 | val_recall: 0.3350 | f1: 0.40271610021591187\n",
      "[257,     4] loss: 0.6267 | train_acc: 0.9814 | train_recall: 1.0000| val_loss: 1.5459 | val_acc: 0.7898 | val_recall: 0.3330 | f1: 0.39425623416900635\n",
      "[258,     4] loss: 0.6281 | train_acc: 0.9863 | train_recall: 1.0000| val_loss: 1.5378 | val_acc: 0.7964 | val_recall: 0.3271 | f1: 0.40329840779304504\n",
      "[259,     4] loss: 0.6263 | train_acc: 0.9932 | train_recall: 1.0000| val_loss: 1.5325 | val_acc: 0.7976 | val_recall: 0.3252 | f1: 0.4070774018764496\n",
      "[260,     4] loss: 0.6194 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5452 | val_acc: 0.7860 | val_recall: 0.3408 | f1: 0.39524415135383606\n",
      "[261,     4] loss: 0.6180 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5528 | val_acc: 0.7760 | val_recall: 0.3438 | f1: 0.3785400688648224\n",
      "[262,     4] loss: 0.6174 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.5431 | val_acc: 0.7878 | val_recall: 0.3311 | f1: 0.3943784832954407\n",
      "[263,     4] loss: 0.6149 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5440 | val_acc: 0.7976 | val_recall: 0.3193 | f1: 0.40163514018058777\n",
      "[264,     4] loss: 0.6153 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.5496 | val_acc: 0.7932 | val_recall: 0.3291 | f1: 0.39962002635002136\n",
      "[265,     4] loss: 0.6125 | train_acc: 0.9961 | train_recall: 1.0000| val_loss: 1.5479 | val_acc: 0.7916 | val_recall: 0.3340 | f1: 0.40413138270378113\n",
      "[266,     4] loss: 0.6108 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5385 | val_acc: 0.7959 | val_recall: 0.3271 | f1: 0.4025093615055084\n",
      "[267,     4] loss: 0.6096 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5460 | val_acc: 0.7970 | val_recall: 0.3281 | f1: 0.4010109305381775\n",
      "[268,     4] loss: 0.6098 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5517 | val_acc: 0.7905 | val_recall: 0.3301 | f1: 0.39251071214675903\n",
      "[269,     4] loss: 0.6051 | train_acc: 0.9961 | train_recall: 1.0000| val_loss: 1.5475 | val_acc: 0.7898 | val_recall: 0.3350 | f1: 0.3971818685531616\n",
      "[270,     4] loss: 0.6027 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5483 | val_acc: 0.7935 | val_recall: 0.3311 | f1: 0.3978992700576782\n",
      "[271,     4] loss: 0.6024 | train_acc: 0.9941 | train_recall: 1.0000| val_loss: 1.5428 | val_acc: 0.7943 | val_recall: 0.3281 | f1: 0.3994675576686859\n",
      "[272,     4] loss: 0.6023 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5428 | val_acc: 0.7958 | val_recall: 0.3398 | f1: 0.40593597292900085\n",
      "[273,     4] loss: 0.6010 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5508 | val_acc: 0.7899 | val_recall: 0.3418 | f1: 0.3997455835342407\n",
      "[274,     4] loss: 0.5980 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5516 | val_acc: 0.7906 | val_recall: 0.3359 | f1: 0.3936285674571991\n",
      "[275,     4] loss: 0.5962 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5498 | val_acc: 0.7964 | val_recall: 0.3359 | f1: 0.40209904313087463\n",
      "[276,     4] loss: 0.5976 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5498 | val_acc: 0.7939 | val_recall: 0.3340 | f1: 0.39843571186065674\n",
      "[277,     4] loss: 0.5953 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5540 | val_acc: 0.7928 | val_recall: 0.3291 | f1: 0.3953695595264435\n",
      "[278,     4] loss: 0.5932 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5575 | val_acc: 0.7917 | val_recall: 0.3359 | f1: 0.39541611075401306\n",
      "[279,     4] loss: 0.5939 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5513 | val_acc: 0.7997 | val_recall: 0.3350 | f1: 0.41127604246139526\n",
      "[280,     4] loss: 0.5930 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5569 | val_acc: 0.7958 | val_recall: 0.3154 | f1: 0.3937079608440399\n",
      "[281,     4] loss: 0.6024 | train_acc: 0.9873 | train_recall: 1.0000| val_loss: 1.5533 | val_acc: 0.7903 | val_recall: 0.3311 | f1: 0.39669594168663025\n",
      "[282,     4] loss: 0.5912 | train_acc: 0.9941 | train_recall: 1.0000| val_loss: 1.5574 | val_acc: 0.7863 | val_recall: 0.3486 | f1: 0.4003612995147705\n",
      "[283,     4] loss: 0.5917 | train_acc: 0.9922 | train_recall: 1.0000| val_loss: 1.5583 | val_acc: 0.7932 | val_recall: 0.3340 | f1: 0.40722253918647766\n",
      "[284,     4] loss: 0.5882 | train_acc: 0.9961 | train_recall: 0.9844| val_loss: 1.5587 | val_acc: 0.7960 | val_recall: 0.3096 | f1: 0.3928150236606598\n",
      "[285,     4] loss: 0.5869 | train_acc: 0.9961 | train_recall: 1.0000| val_loss: 1.5549 | val_acc: 0.7993 | val_recall: 0.3154 | f1: 0.3991086483001709\n",
      "[286,     4] loss: 0.5868 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5571 | val_acc: 0.7990 | val_recall: 0.3145 | f1: 0.39979544281959534\n",
      "[287,     4] loss: 0.5843 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.5628 | val_acc: 0.7986 | val_recall: 0.3232 | f1: 0.4046693444252014\n",
      "[288,     4] loss: 0.5843 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5560 | val_acc: 0.8009 | val_recall: 0.3291 | f1: 0.41039371490478516\n",
      "[289,     4] loss: 0.5790 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5498 | val_acc: 0.8021 | val_recall: 0.3271 | f1: 0.4098324477672577\n",
      "[290,     4] loss: 0.5805 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5518 | val_acc: 0.8016 | val_recall: 0.3320 | f1: 0.40922728180885315\n",
      "[291,     4] loss: 0.5773 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5576 | val_acc: 0.7997 | val_recall: 0.3359 | f1: 0.4071663022041321\n",
      "[292,     4] loss: 0.5761 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5545 | val_acc: 0.8018 | val_recall: 0.3408 | f1: 0.41302311420440674\n",
      "[293,     4] loss: 0.5758 | train_acc: 0.9961 | train_recall: 1.0000| val_loss: 1.5579 | val_acc: 0.8033 | val_recall: 0.3379 | f1: 0.41699326038360596\n",
      "[294,     4] loss: 0.5742 | train_acc: 0.9971 | train_recall: 1.0000| val_loss: 1.5679 | val_acc: 0.7958 | val_recall: 0.3057 | f1: 0.3944450616836548\n",
      "[295,     4] loss: 0.5751 | train_acc: 0.9941 | train_recall: 0.9844| val_loss: 1.5688 | val_acc: 0.7970 | val_recall: 0.3223 | f1: 0.40518999099731445\n",
      "[296,     4] loss: 0.5724 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.5649 | val_acc: 0.8024 | val_recall: 0.3291 | f1: 0.4136422872543335\n",
      "[297,     4] loss: 0.5700 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5612 | val_acc: 0.8030 | val_recall: 0.3213 | f1: 0.41515684127807617\n",
      "[298,     4] loss: 0.5697 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5589 | val_acc: 0.8029 | val_recall: 0.3281 | f1: 0.41245007514953613\n",
      "[299,     4] loss: 0.5687 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5628 | val_acc: 0.7975 | val_recall: 0.3359 | f1: 0.4072420299053192\n",
      "[300,     4] loss: 0.5660 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5655 | val_acc: 0.8009 | val_recall: 0.3311 | f1: 0.4089512825012207\n",
      "[301,     4] loss: 0.5634 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5663 | val_acc: 0.8009 | val_recall: 0.3271 | f1: 0.40605589747428894\n",
      "Saving checkpoint 300\n",
      "[302,     4] loss: 0.5615 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5735 | val_acc: 0.7997 | val_recall: 0.3271 | f1: 0.40481024980545044\n",
      "[303,     4] loss: 0.5617 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5755 | val_acc: 0.7966 | val_recall: 0.3213 | f1: 0.400599867105484\n",
      "[304,     4] loss: 0.5591 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5763 | val_acc: 0.7991 | val_recall: 0.3213 | f1: 0.3999937176704407\n",
      "[305,     4] loss: 0.5593 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5761 | val_acc: 0.7988 | val_recall: 0.3184 | f1: 0.3972127437591553\n",
      "[306,     4] loss: 0.5573 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5700 | val_acc: 0.8016 | val_recall: 0.3174 | f1: 0.4025092124938965\n",
      "[307,     4] loss: 0.5568 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5693 | val_acc: 0.8038 | val_recall: 0.3184 | f1: 0.4109576940536499\n",
      "[308,     4] loss: 0.5554 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5737 | val_acc: 0.8010 | val_recall: 0.3223 | f1: 0.40697798132896423\n",
      "[309,     4] loss: 0.5545 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5767 | val_acc: 0.8015 | val_recall: 0.3281 | f1: 0.4097527265548706\n",
      "[310,     4] loss: 0.5531 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5742 | val_acc: 0.8040 | val_recall: 0.3232 | f1: 0.41205498576164246\n",
      "[311,     4] loss: 0.5524 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5684 | val_acc: 0.8059 | val_recall: 0.3232 | f1: 0.41554737091064453\n",
      "[312,     4] loss: 0.5513 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5780 | val_acc: 0.8021 | val_recall: 0.3281 | f1: 0.4103735387325287\n",
      "[313,     4] loss: 0.5518 | train_acc: 0.9941 | train_recall: 1.0000| val_loss: 1.5682 | val_acc: 0.8059 | val_recall: 0.3320 | f1: 0.4188375771045685\n",
      "[314,     4] loss: 0.5497 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5729 | val_acc: 0.8052 | val_recall: 0.3281 | f1: 0.4113134443759918\n",
      "[315,     4] loss: 0.5470 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5799 | val_acc: 0.8010 | val_recall: 0.3174 | f1: 0.4032347798347473\n",
      "[316,     4] loss: 0.5453 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5853 | val_acc: 0.8014 | val_recall: 0.3164 | f1: 0.40302586555480957\n",
      "[317,     4] loss: 0.5474 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5880 | val_acc: 0.8002 | val_recall: 0.3047 | f1: 0.39331838488578796\n",
      "[318,     4] loss: 0.5448 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5864 | val_acc: 0.8016 | val_recall: 0.3154 | f1: 0.40013083815574646\n",
      "[319,     4] loss: 0.5436 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5879 | val_acc: 0.8032 | val_recall: 0.3184 | f1: 0.39977747201919556\n",
      "[320,     4] loss: 0.5420 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5837 | val_acc: 0.8027 | val_recall: 0.3184 | f1: 0.40008407831192017\n",
      "[321,     4] loss: 0.5400 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5796 | val_acc: 0.8042 | val_recall: 0.3125 | f1: 0.40392789244651794\n",
      "[322,     4] loss: 0.5390 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5761 | val_acc: 0.8059 | val_recall: 0.3164 | f1: 0.40604066848754883\n",
      "[323,     4] loss: 0.5401 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5792 | val_acc: 0.8068 | val_recall: 0.3174 | f1: 0.404981404542923\n",
      "[324,     4] loss: 0.5377 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5867 | val_acc: 0.8021 | val_recall: 0.3223 | f1: 0.4056677222251892\n",
      "[325,     4] loss: 0.5366 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5837 | val_acc: 0.8068 | val_recall: 0.3145 | f1: 0.4086228311061859\n",
      "[326,     4] loss: 0.5331 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5845 | val_acc: 0.8060 | val_recall: 0.3154 | f1: 0.4061654806137085\n",
      "[327,     4] loss: 0.5336 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5889 | val_acc: 0.8038 | val_recall: 0.3193 | f1: 0.4058859348297119\n",
      "[328,     4] loss: 0.5318 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5830 | val_acc: 0.8065 | val_recall: 0.3096 | f1: 0.40257224440574646\n",
      "[329,     4] loss: 0.5306 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5827 | val_acc: 0.8052 | val_recall: 0.3096 | f1: 0.3998350501060486\n",
      "[330,     4] loss: 0.5285 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5905 | val_acc: 0.8064 | val_recall: 0.3164 | f1: 0.40675827860832214\n",
      "[331,     4] loss: 0.5268 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5906 | val_acc: 0.8030 | val_recall: 0.3145 | f1: 0.39864441752433777\n",
      "[332,     4] loss: 0.5278 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5885 | val_acc: 0.8058 | val_recall: 0.3125 | f1: 0.40376219153404236\n",
      "[333,     4] loss: 0.5277 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5874 | val_acc: 0.8086 | val_recall: 0.3096 | f1: 0.4052117168903351\n",
      "[334,     4] loss: 0.5236 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5874 | val_acc: 0.8058 | val_recall: 0.3076 | f1: 0.4010133147239685\n",
      "[335,     4] loss: 0.5228 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5876 | val_acc: 0.8055 | val_recall: 0.3076 | f1: 0.39976975321769714\n",
      "[336,     4] loss: 0.5216 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5896 | val_acc: 0.8071 | val_recall: 0.3047 | f1: 0.39933833479881287\n",
      "[337,     4] loss: 0.5205 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5905 | val_acc: 0.8060 | val_recall: 0.3076 | f1: 0.39791250228881836\n",
      "[338,     4] loss: 0.5208 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5887 | val_acc: 0.8038 | val_recall: 0.3105 | f1: 0.4046112895011902\n",
      "[339,     4] loss: 0.5188 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5934 | val_acc: 0.8046 | val_recall: 0.3096 | f1: 0.4012289345264435\n",
      "[340,     4] loss: 0.5189 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5882 | val_acc: 0.8090 | val_recall: 0.3154 | f1: 0.4114435613155365\n",
      "[341,     4] loss: 0.5157 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5877 | val_acc: 0.8114 | val_recall: 0.3086 | f1: 0.40848666429519653\n",
      "[342,     4] loss: 0.5169 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5915 | val_acc: 0.8094 | val_recall: 0.3076 | f1: 0.4071454405784607\n",
      "[343,     4] loss: 0.5137 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5920 | val_acc: 0.8077 | val_recall: 0.3125 | f1: 0.4076572060585022\n",
      "[344,     4] loss: 0.5128 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5928 | val_acc: 0.8093 | val_recall: 0.3135 | f1: 0.4098592698574066\n",
      "[345,     4] loss: 0.5128 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5943 | val_acc: 0.8081 | val_recall: 0.3164 | f1: 0.411586195230484\n",
      "[346,     4] loss: 0.5133 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5961 | val_acc: 0.8093 | val_recall: 0.3076 | f1: 0.40402185916900635\n",
      "[347,     4] loss: 0.5104 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5969 | val_acc: 0.8113 | val_recall: 0.3057 | f1: 0.40580442547798157\n",
      "[348,     4] loss: 0.5095 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.5962 | val_acc: 0.8096 | val_recall: 0.3037 | f1: 0.4038689434528351\n",
      "[349,     4] loss: 0.5100 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6049 | val_acc: 0.8057 | val_recall: 0.3047 | f1: 0.40044698119163513\n",
      "[350,     4] loss: 0.5064 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6010 | val_acc: 0.8105 | val_recall: 0.2969 | f1: 0.40067851543426514\n",
      "[351,     4] loss: 0.5064 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6082 | val_acc: 0.8090 | val_recall: 0.3047 | f1: 0.402875691652298\n",
      "[352,     4] loss: 0.5062 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6097 | val_acc: 0.8064 | val_recall: 0.3086 | f1: 0.40383878350257874\n",
      "[353,     4] loss: 0.5031 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6076 | val_acc: 0.8070 | val_recall: 0.3057 | f1: 0.401531845331192\n",
      "[354,     4] loss: 0.5023 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6064 | val_acc: 0.8094 | val_recall: 0.3096 | f1: 0.406963050365448\n",
      "[355,     4] loss: 0.5037 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6104 | val_acc: 0.8079 | val_recall: 0.3047 | f1: 0.4002390503883362\n",
      "[356,     4] loss: 0.5006 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6098 | val_acc: 0.8065 | val_recall: 0.3066 | f1: 0.3974176049232483\n",
      "[357,     4] loss: 0.4996 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6057 | val_acc: 0.8099 | val_recall: 0.3018 | f1: 0.39931154251098633\n",
      "[358,     4] loss: 0.4998 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6095 | val_acc: 0.8094 | val_recall: 0.3018 | f1: 0.39666005969047546\n",
      "[359,     4] loss: 0.4954 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6091 | val_acc: 0.8102 | val_recall: 0.2939 | f1: 0.3939571678638458\n",
      "[360,     4] loss: 0.4970 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6124 | val_acc: 0.8090 | val_recall: 0.2920 | f1: 0.3915725350379944\n",
      "[361,     4] loss: 0.4962 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6149 | val_acc: 0.8086 | val_recall: 0.2939 | f1: 0.3947000801563263\n",
      "[362,     4] loss: 0.4938 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6088 | val_acc: 0.8123 | val_recall: 0.3057 | f1: 0.4091416895389557\n",
      "[363,     4] loss: 0.4930 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6081 | val_acc: 0.8120 | val_recall: 0.3047 | f1: 0.4043556749820709\n",
      "[364,     4] loss: 0.4931 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6092 | val_acc: 0.8127 | val_recall: 0.3008 | f1: 0.39891090989112854\n",
      "[365,     4] loss: 0.4925 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6183 | val_acc: 0.8108 | val_recall: 0.3047 | f1: 0.39960020780563354\n",
      "[366,     4] loss: 0.4906 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6151 | val_acc: 0.8098 | val_recall: 0.3018 | f1: 0.3963327407836914\n",
      "[367,     4] loss: 0.4892 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6150 | val_acc: 0.8130 | val_recall: 0.2969 | f1: 0.395967036485672\n",
      "[368,     4] loss: 0.4881 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6192 | val_acc: 0.8116 | val_recall: 0.2979 | f1: 0.3963657021522522\n",
      "[369,     4] loss: 0.4877 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6208 | val_acc: 0.8080 | val_recall: 0.3008 | f1: 0.39888912439346313\n",
      "[370,     4] loss: 0.4865 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6237 | val_acc: 0.8120 | val_recall: 0.2949 | f1: 0.39808109402656555\n",
      "[371,     4] loss: 0.4873 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6206 | val_acc: 0.8120 | val_recall: 0.2910 | f1: 0.3959255516529083\n",
      "[372,     4] loss: 0.4843 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6209 | val_acc: 0.8079 | val_recall: 0.2988 | f1: 0.3996123671531677\n",
      "[373,     4] loss: 0.4838 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6167 | val_acc: 0.8142 | val_recall: 0.2881 | f1: 0.3955521583557129\n",
      "[374,     4] loss: 0.4829 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6172 | val_acc: 0.8140 | val_recall: 0.2861 | f1: 0.392531156539917\n",
      "[375,     4] loss: 0.4794 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6216 | val_acc: 0.8123 | val_recall: 0.2900 | f1: 0.39494839310646057\n",
      "[376,     4] loss: 0.4793 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6215 | val_acc: 0.8120 | val_recall: 0.2910 | f1: 0.39435991644859314\n",
      "[377,     4] loss: 0.4793 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6189 | val_acc: 0.8116 | val_recall: 0.2910 | f1: 0.3955096900463104\n",
      "[378,     4] loss: 0.4766 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6224 | val_acc: 0.8120 | val_recall: 0.2900 | f1: 0.3969671130180359\n",
      "[379,     4] loss: 0.4764 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6215 | val_acc: 0.8107 | val_recall: 0.2891 | f1: 0.3956913352012634\n",
      "[380,     4] loss: 0.4744 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6209 | val_acc: 0.8105 | val_recall: 0.2920 | f1: 0.392693430185318\n",
      "[381,     4] loss: 0.4741 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6267 | val_acc: 0.8085 | val_recall: 0.2969 | f1: 0.3950573801994324\n",
      "[382,     4] loss: 0.4725 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6294 | val_acc: 0.8088 | val_recall: 0.2930 | f1: 0.39305245876312256\n",
      "[383,     4] loss: 0.4712 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6291 | val_acc: 0.8110 | val_recall: 0.2920 | f1: 0.39275071024894714\n",
      "[384,     4] loss: 0.4714 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6251 | val_acc: 0.8149 | val_recall: 0.2891 | f1: 0.3945175111293793\n",
      "[385,     4] loss: 0.4711 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6256 | val_acc: 0.8136 | val_recall: 0.2900 | f1: 0.3951069414615631\n",
      "[386,     4] loss: 0.4690 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6266 | val_acc: 0.8125 | val_recall: 0.2930 | f1: 0.39423078298568726\n",
      "[387,     4] loss: 0.4685 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6257 | val_acc: 0.8151 | val_recall: 0.2910 | f1: 0.39621269702911377\n",
      "[388,     4] loss: 0.4665 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6322 | val_acc: 0.8116 | val_recall: 0.2959 | f1: 0.39692333340644836\n",
      "[389,     4] loss: 0.4666 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6302 | val_acc: 0.8147 | val_recall: 0.2900 | f1: 0.3935532867908478\n",
      "[390,     4] loss: 0.4670 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6292 | val_acc: 0.8147 | val_recall: 0.2891 | f1: 0.3954230844974518\n",
      "[391,     4] loss: 0.4643 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6390 | val_acc: 0.8136 | val_recall: 0.2861 | f1: 0.38816267251968384\n",
      "[392,     4] loss: 0.4632 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6405 | val_acc: 0.8148 | val_recall: 0.2861 | f1: 0.38640981912612915\n",
      "[393,     4] loss: 0.4612 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6399 | val_acc: 0.8146 | val_recall: 0.2891 | f1: 0.3897826671600342\n",
      "[394,     4] loss: 0.4624 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6421 | val_acc: 0.8103 | val_recall: 0.2891 | f1: 0.3872210383415222\n",
      "[395,     4] loss: 0.4602 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6444 | val_acc: 0.8152 | val_recall: 0.2822 | f1: 0.3894754648208618\n",
      "[396,     4] loss: 0.4581 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6460 | val_acc: 0.8167 | val_recall: 0.2803 | f1: 0.38261985778808594\n",
      "[397,     4] loss: 0.4577 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6453 | val_acc: 0.8181 | val_recall: 0.2705 | f1: 0.3762515187263489\n",
      "[398,     4] loss: 0.4565 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6449 | val_acc: 0.8191 | val_recall: 0.2744 | f1: 0.37693387269973755\n",
      "[399,     4] loss: 0.4557 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6454 | val_acc: 0.8138 | val_recall: 0.2812 | f1: 0.38288232684135437\n",
      "[400,     4] loss: 0.4553 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6433 | val_acc: 0.8162 | val_recall: 0.2754 | f1: 0.3810378611087799\n",
      "[401,     4] loss: 0.4555 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6493 | val_acc: 0.8125 | val_recall: 0.2891 | f1: 0.39609530568122864\n",
      "Saving checkpoint 400\n",
      "[402,     4] loss: 0.4535 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6540 | val_acc: 0.8141 | val_recall: 0.2803 | f1: 0.3904796242713928\n",
      "[403,     4] loss: 0.4524 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6536 | val_acc: 0.8142 | val_recall: 0.2910 | f1: 0.39405763149261475\n",
      "[404,     4] loss: 0.4530 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6451 | val_acc: 0.8162 | val_recall: 0.2900 | f1: 0.3946603834629059\n",
      "[405,     4] loss: 0.4533 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6479 | val_acc: 0.8157 | val_recall: 0.2871 | f1: 0.3921663463115692\n",
      "[406,     4] loss: 0.4514 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6460 | val_acc: 0.8167 | val_recall: 0.2822 | f1: 0.3926548659801483\n",
      "[407,     4] loss: 0.4489 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6493 | val_acc: 0.8152 | val_recall: 0.2861 | f1: 0.3919220566749573\n",
      "[408,     4] loss: 0.4473 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6428 | val_acc: 0.8103 | val_recall: 0.2998 | f1: 0.4017544388771057\n",
      "[409,     4] loss: 0.4470 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6442 | val_acc: 0.8126 | val_recall: 0.2812 | f1: 0.39368462562561035\n",
      "[410,     4] loss: 0.4453 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6521 | val_acc: 0.8121 | val_recall: 0.2832 | f1: 0.3915683627128601\n",
      "[411,     4] loss: 0.4460 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6522 | val_acc: 0.8142 | val_recall: 0.2715 | f1: 0.38425442576408386\n",
      "[412,     4] loss: 0.4447 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6606 | val_acc: 0.8094 | val_recall: 0.2783 | f1: 0.3822040259838104\n",
      "[413,     4] loss: 0.4435 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6604 | val_acc: 0.8138 | val_recall: 0.2803 | f1: 0.3874349296092987\n",
      "[414,     4] loss: 0.4427 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6637 | val_acc: 0.8142 | val_recall: 0.2852 | f1: 0.3870982229709625\n",
      "[415,     4] loss: 0.4404 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6593 | val_acc: 0.8141 | val_recall: 0.2939 | f1: 0.3996286988258362\n",
      "[416,     4] loss: 0.4399 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6563 | val_acc: 0.8151 | val_recall: 0.2900 | f1: 0.39881834387779236\n",
      "[417,     4] loss: 0.4403 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6525 | val_acc: 0.8177 | val_recall: 0.2852 | f1: 0.3980218768119812\n",
      "[418,     4] loss: 0.4373 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6514 | val_acc: 0.8186 | val_recall: 0.2861 | f1: 0.3968164622783661\n",
      "[419,     4] loss: 0.4371 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6560 | val_acc: 0.8134 | val_recall: 0.2861 | f1: 0.3969413638114929\n",
      "[420,     4] loss: 0.4360 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6554 | val_acc: 0.8147 | val_recall: 0.2861 | f1: 0.39598289132118225\n",
      "[421,     4] loss: 0.4359 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6569 | val_acc: 0.8157 | val_recall: 0.2920 | f1: 0.40222659707069397\n",
      "[422,     4] loss: 0.4345 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6586 | val_acc: 0.8162 | val_recall: 0.2920 | f1: 0.4026555120944977\n",
      "[423,     4] loss: 0.4334 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6627 | val_acc: 0.8143 | val_recall: 0.2852 | f1: 0.3950065076351166\n",
      "[424,     4] loss: 0.4352 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6624 | val_acc: 0.8137 | val_recall: 0.2910 | f1: 0.39810043573379517\n",
      "[425,     4] loss: 0.4329 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6585 | val_acc: 0.8103 | val_recall: 0.2881 | f1: 0.39471709728240967\n",
      "[426,     4] loss: 0.4319 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6596 | val_acc: 0.8167 | val_recall: 0.2793 | f1: 0.39020836353302\n",
      "[427,     4] loss: 0.4321 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6617 | val_acc: 0.8175 | val_recall: 0.2861 | f1: 0.3989239037036896\n",
      "[428,     4] loss: 0.4306 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6638 | val_acc: 0.8158 | val_recall: 0.2881 | f1: 0.39667224884033203\n",
      "[429,     4] loss: 0.4317 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6708 | val_acc: 0.8127 | val_recall: 0.2871 | f1: 0.38466036319732666\n",
      "[430,     4] loss: 0.4308 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6700 | val_acc: 0.8126 | val_recall: 0.2822 | f1: 0.3812231719493866\n",
      "[431,     4] loss: 0.4288 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6720 | val_acc: 0.8135 | val_recall: 0.2783 | f1: 0.38246098160743713\n",
      "[432,     4] loss: 0.4278 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6682 | val_acc: 0.8151 | val_recall: 0.2842 | f1: 0.39303383231163025\n",
      "[433,     4] loss: 0.4276 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6712 | val_acc: 0.8164 | val_recall: 0.2900 | f1: 0.3973037600517273\n",
      "[434,     4] loss: 0.4270 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.6691 | val_acc: 0.8171 | val_recall: 0.2852 | f1: 0.39214739203453064\n",
      "[435,     4] loss: 0.4242 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6749 | val_acc: 0.8177 | val_recall: 0.2920 | f1: 0.39895299077033997\n",
      "[436,     4] loss: 0.4231 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6723 | val_acc: 0.8174 | val_recall: 0.2949 | f1: 0.40095242857933044\n",
      "[437,     4] loss: 0.4223 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6752 | val_acc: 0.8152 | val_recall: 0.2900 | f1: 0.39566728472709656\n",
      "[438,     4] loss: 0.4207 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6704 | val_acc: 0.8160 | val_recall: 0.2920 | f1: 0.3914567232131958\n",
      "[439,     4] loss: 0.4213 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6684 | val_acc: 0.8204 | val_recall: 0.2969 | f1: 0.4053919017314911\n",
      "[440,     4] loss: 0.4187 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6737 | val_acc: 0.8168 | val_recall: 0.2959 | f1: 0.4065983295440674\n",
      "[441,     4] loss: 0.4231 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6847 | val_acc: 0.8152 | val_recall: 0.2744 | f1: 0.3889094889163971\n",
      "[442,     4] loss: 0.4188 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6805 | val_acc: 0.8167 | val_recall: 0.2646 | f1: 0.3793666362762451\n",
      "[443,     4] loss: 0.4168 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6808 | val_acc: 0.8159 | val_recall: 0.2783 | f1: 0.38880395889282227\n",
      "[444,     4] loss: 0.4172 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6847 | val_acc: 0.8129 | val_recall: 0.2754 | f1: 0.3890218436717987\n",
      "[445,     4] loss: 0.4144 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6881 | val_acc: 0.8156 | val_recall: 0.2783 | f1: 0.38554373383522034\n",
      "[446,     4] loss: 0.4156 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6847 | val_acc: 0.8127 | val_recall: 0.2891 | f1: 0.39612430334091187\n",
      "[447,     4] loss: 0.4127 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6818 | val_acc: 0.8151 | val_recall: 0.2803 | f1: 0.3934086561203003\n",
      "[448,     4] loss: 0.4123 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6844 | val_acc: 0.8164 | val_recall: 0.2725 | f1: 0.3863983452320099\n",
      "[449,     4] loss: 0.4106 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6944 | val_acc: 0.8131 | val_recall: 0.2773 | f1: 0.3898029327392578\n",
      "[450,     4] loss: 0.4102 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6919 | val_acc: 0.8138 | val_recall: 0.2764 | f1: 0.3906395733356476\n",
      "[451,     4] loss: 0.4086 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6883 | val_acc: 0.8143 | val_recall: 0.2734 | f1: 0.39144766330718994\n",
      "[452,     4] loss: 0.4089 | train_acc: 0.9961 | train_recall: 0.9844| val_loss: 1.6846 | val_acc: 0.8137 | val_recall: 0.2773 | f1: 0.3876100480556488\n",
      "[453,     4] loss: 0.4080 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6828 | val_acc: 0.8164 | val_recall: 0.2793 | f1: 0.3935648798942566\n",
      "[454,     4] loss: 0.4071 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6953 | val_acc: 0.8135 | val_recall: 0.2822 | f1: 0.39769965410232544\n",
      "[455,     4] loss: 0.4088 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6915 | val_acc: 0.8181 | val_recall: 0.2822 | f1: 0.3973191976547241\n",
      "[456,     4] loss: 0.4078 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6886 | val_acc: 0.8179 | val_recall: 0.2764 | f1: 0.38351723551750183\n",
      "[457,     4] loss: 0.4087 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6856 | val_acc: 0.8143 | val_recall: 0.2930 | f1: 0.3950065076351166\n",
      "[458,     4] loss: 0.4046 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.6790 | val_acc: 0.8145 | val_recall: 0.3047 | f1: 0.4063103497028351\n",
      "[459,     4] loss: 0.4061 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6907 | val_acc: 0.8153 | val_recall: 0.2900 | f1: 0.39287468791007996\n",
      "[460,     4] loss: 0.4028 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7087 | val_acc: 0.8077 | val_recall: 0.2832 | f1: 0.3774418234825134\n",
      "[461,     4] loss: 0.4015 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7029 | val_acc: 0.8105 | val_recall: 0.2842 | f1: 0.38800275325775146\n",
      "[462,     4] loss: 0.4017 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7027 | val_acc: 0.8145 | val_recall: 0.2842 | f1: 0.39688584208488464\n",
      "[463,     4] loss: 0.4018 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6990 | val_acc: 0.8153 | val_recall: 0.2852 | f1: 0.3986617624759674\n",
      "[464,     4] loss: 0.3967 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6965 | val_acc: 0.8151 | val_recall: 0.2832 | f1: 0.3907800316810608\n",
      "[465,     4] loss: 0.4001 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6961 | val_acc: 0.8159 | val_recall: 0.2842 | f1: 0.3927578628063202\n",
      "[466,     4] loss: 0.3958 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6989 | val_acc: 0.8181 | val_recall: 0.2783 | f1: 0.3863995373249054\n",
      "[467,     4] loss: 0.3948 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6980 | val_acc: 0.8164 | val_recall: 0.2793 | f1: 0.38696712255477905\n",
      "[468,     4] loss: 0.3958 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6960 | val_acc: 0.8167 | val_recall: 0.2822 | f1: 0.39020836353302\n",
      "[469,     4] loss: 0.3937 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7010 | val_acc: 0.8165 | val_recall: 0.2764 | f1: 0.3918890655040741\n",
      "[470,     4] loss: 0.3949 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6984 | val_acc: 0.8174 | val_recall: 0.2832 | f1: 0.39461562037467957\n",
      "[471,     4] loss: 0.3934 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.6970 | val_acc: 0.8170 | val_recall: 0.2881 | f1: 0.39644303917884827\n",
      "[472,     4] loss: 0.3916 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7029 | val_acc: 0.8145 | val_recall: 0.2861 | f1: 0.3931502103805542\n",
      "[473,     4] loss: 0.3901 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7052 | val_acc: 0.8134 | val_recall: 0.2832 | f1: 0.3894544541835785\n",
      "[474,     4] loss: 0.3906 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7071 | val_acc: 0.8154 | val_recall: 0.2861 | f1: 0.3928888440132141\n",
      "[475,     4] loss: 0.3883 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7059 | val_acc: 0.8158 | val_recall: 0.2812 | f1: 0.39236825704574585\n",
      "[476,     4] loss: 0.3879 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7046 | val_acc: 0.8151 | val_recall: 0.2793 | f1: 0.3890843093395233\n",
      "[477,     4] loss: 0.3872 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7066 | val_acc: 0.8179 | val_recall: 0.2803 | f1: 0.39034757018089294\n",
      "[478,     4] loss: 0.3877 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7061 | val_acc: 0.8171 | val_recall: 0.2812 | f1: 0.39271125197410583\n",
      "[479,     4] loss: 0.3856 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7047 | val_acc: 0.8184 | val_recall: 0.2832 | f1: 0.3966009318828583\n",
      "[480,     4] loss: 0.3850 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7076 | val_acc: 0.8164 | val_recall: 0.2812 | f1: 0.3924387991428375\n",
      "[481,     4] loss: 0.3838 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7032 | val_acc: 0.8191 | val_recall: 0.2754 | f1: 0.3933124840259552\n",
      "[482,     4] loss: 0.3836 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7096 | val_acc: 0.8175 | val_recall: 0.2803 | f1: 0.39312925934791565\n",
      "[483,     4] loss: 0.3825 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7116 | val_acc: 0.8179 | val_recall: 0.2783 | f1: 0.3931715786457062\n",
      "[484,     4] loss: 0.3828 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7172 | val_acc: 0.8179 | val_recall: 0.2754 | f1: 0.3897812068462372\n",
      "[485,     4] loss: 0.3803 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7153 | val_acc: 0.8174 | val_recall: 0.2812 | f1: 0.39330291748046875\n",
      "[486,     4] loss: 0.3796 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7147 | val_acc: 0.8179 | val_recall: 0.2734 | f1: 0.39072486758232117\n",
      "[487,     4] loss: 0.3799 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7163 | val_acc: 0.8175 | val_recall: 0.2852 | f1: 0.39500442147254944\n",
      "[488,     4] loss: 0.3794 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7145 | val_acc: 0.8186 | val_recall: 0.2832 | f1: 0.39438268542289734\n",
      "[489,     4] loss: 0.3776 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7095 | val_acc: 0.8193 | val_recall: 0.2832 | f1: 0.3946552574634552\n",
      "[490,     4] loss: 0.3767 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7168 | val_acc: 0.8182 | val_recall: 0.2832 | f1: 0.3943401575088501\n",
      "[491,     4] loss: 0.3753 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7190 | val_acc: 0.8186 | val_recall: 0.2793 | f1: 0.38986435532569885\n",
      "[492,     4] loss: 0.3770 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7159 | val_acc: 0.8196 | val_recall: 0.2803 | f1: 0.3943081796169281\n",
      "[493,     4] loss: 0.3735 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7180 | val_acc: 0.8192 | val_recall: 0.2822 | f1: 0.392385870218277\n",
      "[494,     4] loss: 0.3738 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7332 | val_acc: 0.8157 | val_recall: 0.2803 | f1: 0.38726353645324707\n",
      "[495,     4] loss: 0.3777 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7256 | val_acc: 0.8179 | val_recall: 0.2803 | f1: 0.39710378646850586\n",
      "[496,     4] loss: 0.3730 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7238 | val_acc: 0.8213 | val_recall: 0.2822 | f1: 0.4014207124710083\n",
      "[497,     4] loss: 0.3770 | train_acc: 0.9990 | train_recall: 0.9922| val_loss: 1.7202 | val_acc: 0.8180 | val_recall: 0.2871 | f1: 0.40287795662879944\n",
      "[498,     4] loss: 0.3732 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7210 | val_acc: 0.8181 | val_recall: 0.2939 | f1: 0.40658071637153625\n",
      "[499,     4] loss: 0.3712 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7282 | val_acc: 0.8124 | val_recall: 0.2764 | f1: 0.37564149498939514\n",
      "[500,     4] loss: 0.3835 | train_acc: 0.9961 | train_recall: 0.9844| val_loss: 1.7434 | val_acc: 0.8198 | val_recall: 0.2695 | f1: 0.3844950795173645\n",
      "[501,     4] loss: 0.3785 | train_acc: 0.9980 | train_recall: 0.9844| val_loss: 1.7424 | val_acc: 0.8157 | val_recall: 0.2773 | f1: 0.3968442678451538\n",
      "Saving checkpoint 500\n",
      "[502,     4] loss: 0.3707 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7366 | val_acc: 0.8226 | val_recall: 0.2764 | f1: 0.39822375774383545\n",
      "[503,     4] loss: 0.3678 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7279 | val_acc: 0.8225 | val_recall: 0.2822 | f1: 0.39970365166664124\n",
      "[504,     4] loss: 0.3680 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7285 | val_acc: 0.8208 | val_recall: 0.2793 | f1: 0.3968869745731354\n",
      "[505,     4] loss: 0.3671 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.7404 | val_acc: 0.8198 | val_recall: 0.2783 | f1: 0.39677271246910095\n",
      "[506,     4] loss: 0.3650 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7358 | val_acc: 0.8197 | val_recall: 0.2852 | f1: 0.3991850018501282\n",
      "[507,     4] loss: 0.3656 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7187 | val_acc: 0.8212 | val_recall: 0.2930 | f1: 0.41099533438682556\n",
      "[508,     4] loss: 0.3639 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7275 | val_acc: 0.8207 | val_recall: 0.2959 | f1: 0.41348984837532043\n",
      "[509,     4] loss: 0.3634 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7346 | val_acc: 0.8214 | val_recall: 0.2900 | f1: 0.40366172790527344\n",
      "[510,     4] loss: 0.3638 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7405 | val_acc: 0.8171 | val_recall: 0.2891 | f1: 0.39533594250679016\n",
      "[511,     4] loss: 0.3623 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7301 | val_acc: 0.8208 | val_recall: 0.2861 | f1: 0.3940734267234802\n",
      "[512,     4] loss: 0.3642 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7393 | val_acc: 0.8208 | val_recall: 0.2812 | f1: 0.3940734267234802\n",
      "[513,     4] loss: 0.3607 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7402 | val_acc: 0.8206 | val_recall: 0.2812 | f1: 0.40003135800361633\n",
      "[514,     4] loss: 0.3611 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7470 | val_acc: 0.8217 | val_recall: 0.2754 | f1: 0.39492374658584595\n",
      "[515,     4] loss: 0.3585 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7491 | val_acc: 0.8215 | val_recall: 0.2764 | f1: 0.3884899616241455\n",
      "[516,     4] loss: 0.3576 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7567 | val_acc: 0.8164 | val_recall: 0.2773 | f1: 0.3918749988079071\n",
      "[517,     4] loss: 0.3584 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7551 | val_acc: 0.8148 | val_recall: 0.2842 | f1: 0.3939419686794281\n",
      "[518,     4] loss: 0.3563 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7389 | val_acc: 0.8174 | val_recall: 0.2930 | f1: 0.4081418514251709\n",
      "[519,     4] loss: 0.3555 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7355 | val_acc: 0.8184 | val_recall: 0.2979 | f1: 0.4128309190273285\n",
      "[520,     4] loss: 0.3550 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7359 | val_acc: 0.8202 | val_recall: 0.2969 | f1: 0.4123341739177704\n",
      "[521,     4] loss: 0.3545 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7499 | val_acc: 0.8204 | val_recall: 0.2822 | f1: 0.39796578884124756\n",
      "[522,     4] loss: 0.3536 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7509 | val_acc: 0.8213 | val_recall: 0.2783 | f1: 0.3903573453426361\n",
      "[523,     4] loss: 0.3546 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7542 | val_acc: 0.8237 | val_recall: 0.2744 | f1: 0.3944101631641388\n",
      "[524,     4] loss: 0.3560 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7510 | val_acc: 0.8218 | val_recall: 0.2852 | f1: 0.3998035490512848\n",
      "[525,     4] loss: 0.3518 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7605 | val_acc: 0.8184 | val_recall: 0.2871 | f1: 0.39435434341430664\n",
      "[526,     4] loss: 0.3516 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7644 | val_acc: 0.8203 | val_recall: 0.2764 | f1: 0.39119163155555725\n",
      "[527,     4] loss: 0.3507 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7669 | val_acc: 0.8181 | val_recall: 0.2715 | f1: 0.3841162323951721\n",
      "[528,     4] loss: 0.3506 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7798 | val_acc: 0.8153 | val_recall: 0.2715 | f1: 0.37979644536972046\n",
      "[529,     4] loss: 0.3479 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7766 | val_acc: 0.8179 | val_recall: 0.2646 | f1: 0.37545323371887207\n",
      "[530,     4] loss: 0.3475 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7735 | val_acc: 0.8185 | val_recall: 0.2686 | f1: 0.37802547216415405\n",
      "[531,     4] loss: 0.3466 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7732 | val_acc: 0.8165 | val_recall: 0.2705 | f1: 0.37800905108451843\n",
      "[532,     4] loss: 0.3463 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7768 | val_acc: 0.8159 | val_recall: 0.2715 | f1: 0.37986263632774353\n",
      "[533,     4] loss: 0.3450 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7752 | val_acc: 0.8196 | val_recall: 0.2695 | f1: 0.38025814294815063\n",
      "[534,     4] loss: 0.3451 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7829 | val_acc: 0.8190 | val_recall: 0.2695 | f1: 0.38172608613967896\n",
      "[535,     4] loss: 0.3444 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7751 | val_acc: 0.8198 | val_recall: 0.2676 | f1: 0.3825846314430237\n",
      "[536,     4] loss: 0.3419 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7778 | val_acc: 0.8209 | val_recall: 0.2676 | f1: 0.3805946409702301\n",
      "[537,     4] loss: 0.3430 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7774 | val_acc: 0.8196 | val_recall: 0.2666 | f1: 0.38025814294815063\n",
      "[538,     4] loss: 0.3410 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7826 | val_acc: 0.8195 | val_recall: 0.2686 | f1: 0.38139593601226807\n",
      "[539,     4] loss: 0.3404 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7770 | val_acc: 0.8190 | val_recall: 0.2695 | f1: 0.3832560181617737\n",
      "[540,     4] loss: 0.3408 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7797 | val_acc: 0.8197 | val_recall: 0.2695 | f1: 0.38448163866996765\n",
      "[541,     4] loss: 0.3399 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7756 | val_acc: 0.8207 | val_recall: 0.2715 | f1: 0.38554248213768005\n",
      "[542,     4] loss: 0.3392 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7765 | val_acc: 0.8215 | val_recall: 0.2695 | f1: 0.38334473967552185\n",
      "[543,     4] loss: 0.3388 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7826 | val_acc: 0.8201 | val_recall: 0.2666 | f1: 0.3814620077610016\n",
      "[544,     4] loss: 0.3373 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7827 | val_acc: 0.8204 | val_recall: 0.2656 | f1: 0.38379842042922974\n",
      "[545,     4] loss: 0.3364 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7804 | val_acc: 0.8204 | val_recall: 0.2705 | f1: 0.3877977430820465\n",
      "[546,     4] loss: 0.3358 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7836 | val_acc: 0.8198 | val_recall: 0.2725 | f1: 0.38487645983695984\n",
      "[547,     4] loss: 0.3347 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7846 | val_acc: 0.8208 | val_recall: 0.2715 | f1: 0.3832649290561676\n",
      "[548,     4] loss: 0.3358 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7766 | val_acc: 0.8219 | val_recall: 0.2705 | f1: 0.390804648399353\n",
      "[549,     4] loss: 0.3334 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7881 | val_acc: 0.8193 | val_recall: 0.2666 | f1: 0.38482263684272766\n",
      "[550,     4] loss: 0.3337 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7853 | val_acc: 0.8215 | val_recall: 0.2686 | f1: 0.3850644826889038\n",
      "[551,     4] loss: 0.3320 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7869 | val_acc: 0.8202 | val_recall: 0.2656 | f1: 0.3854885697364807\n",
      "[552,     4] loss: 0.3327 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7919 | val_acc: 0.8213 | val_recall: 0.2715 | f1: 0.3850376605987549\n",
      "[553,     4] loss: 0.3309 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7847 | val_acc: 0.8215 | val_recall: 0.2754 | f1: 0.3873502016067505\n",
      "[554,     4] loss: 0.3299 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7823 | val_acc: 0.8214 | val_recall: 0.2725 | f1: 0.3854326009750366\n",
      "[555,     4] loss: 0.3304 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7863 | val_acc: 0.8203 | val_recall: 0.2695 | f1: 0.38378506898880005\n",
      "[556,     4] loss: 0.3284 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7930 | val_acc: 0.8209 | val_recall: 0.2734 | f1: 0.3872823119163513\n",
      "[557,     4] loss: 0.3276 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7896 | val_acc: 0.8226 | val_recall: 0.2695 | f1: 0.387662410736084\n",
      "[558,     4] loss: 0.3277 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7923 | val_acc: 0.8221 | val_recall: 0.2725 | f1: 0.3921550214290619\n",
      "[559,     4] loss: 0.3270 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8006 | val_acc: 0.8196 | val_recall: 0.2705 | f1: 0.38883963227272034\n",
      "[560,     4] loss: 0.3258 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7952 | val_acc: 0.8206 | val_recall: 0.2686 | f1: 0.3859099745750427\n",
      "[561,     4] loss: 0.3247 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7886 | val_acc: 0.8218 | val_recall: 0.2764 | f1: 0.3930562436580658\n",
      "[562,     4] loss: 0.3254 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7967 | val_acc: 0.8207 | val_recall: 0.2725 | f1: 0.3895314633846283\n",
      "[563,     4] loss: 0.3248 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7965 | val_acc: 0.8215 | val_recall: 0.2686 | f1: 0.3896276354789734\n",
      "[564,     4] loss: 0.3230 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7967 | val_acc: 0.8229 | val_recall: 0.2686 | f1: 0.3905363380908966\n",
      "[565,     4] loss: 0.3236 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7985 | val_acc: 0.8203 | val_recall: 0.2705 | f1: 0.3883533179759979\n",
      "[566,     4] loss: 0.3222 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8033 | val_acc: 0.8209 | val_recall: 0.2705 | f1: 0.39069417119026184\n",
      "[567,     4] loss: 0.3227 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7983 | val_acc: 0.8212 | val_recall: 0.2666 | f1: 0.38787949085235596\n",
      "[568,     4] loss: 0.3204 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8012 | val_acc: 0.8191 | val_recall: 0.2734 | f1: 0.39387616515159607\n",
      "[569,     4] loss: 0.3202 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8106 | val_acc: 0.8201 | val_recall: 0.2725 | f1: 0.3894626498222351\n",
      "[570,     4] loss: 0.3195 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8001 | val_acc: 0.8220 | val_recall: 0.2715 | f1: 0.3928956687450409\n",
      "[571,     4] loss: 0.3183 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8051 | val_acc: 0.8206 | val_recall: 0.2676 | f1: 0.3889494240283966\n",
      "[572,     4] loss: 0.3200 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8059 | val_acc: 0.8203 | val_recall: 0.2734 | f1: 0.39119163155555725\n",
      "[573,     4] loss: 0.3206 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8078 | val_acc: 0.8220 | val_recall: 0.2686 | f1: 0.39364930987358093\n",
      "[574,     4] loss: 0.3187 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8028 | val_acc: 0.8232 | val_recall: 0.2705 | f1: 0.39209094643592834\n",
      "[575,     4] loss: 0.3178 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8044 | val_acc: 0.8193 | val_recall: 0.2842 | f1: 0.3993277847766876\n",
      "[576,     4] loss: 0.3167 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8038 | val_acc: 0.8230 | val_recall: 0.2803 | f1: 0.4027397930622101\n",
      "[577,     4] loss: 0.3152 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.7982 | val_acc: 0.8230 | val_recall: 0.2793 | f1: 0.406073659658432\n",
      "[578,     4] loss: 0.3145 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8039 | val_acc: 0.8221 | val_recall: 0.2812 | f1: 0.40003329515457153\n",
      "[579,     4] loss: 0.3146 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8031 | val_acc: 0.8219 | val_recall: 0.2764 | f1: 0.3981378674507141\n",
      "[580,     4] loss: 0.3142 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8004 | val_acc: 0.8235 | val_recall: 0.2764 | f1: 0.4000054895877838\n",
      "[581,     4] loss: 0.3135 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8033 | val_acc: 0.8243 | val_recall: 0.2793 | f1: 0.40122538805007935\n",
      "[582,     4] loss: 0.3119 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8084 | val_acc: 0.8224 | val_recall: 0.2783 | f1: 0.3978210687637329\n",
      "[583,     4] loss: 0.3115 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8075 | val_acc: 0.8229 | val_recall: 0.2783 | f1: 0.3993735909461975\n",
      "[584,     4] loss: 0.3114 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8052 | val_acc: 0.8237 | val_recall: 0.2773 | f1: 0.40226981043815613\n",
      "[585,     4] loss: 0.3108 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8134 | val_acc: 0.8223 | val_recall: 0.2744 | f1: 0.3964958190917969\n",
      "[586,     4] loss: 0.3090 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8132 | val_acc: 0.8226 | val_recall: 0.2695 | f1: 0.39371925592422485\n",
      "[587,     4] loss: 0.3086 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8200 | val_acc: 0.8226 | val_recall: 0.2725 | f1: 0.3931539058685303\n",
      "[588,     4] loss: 0.3074 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8210 | val_acc: 0.8217 | val_recall: 0.2715 | f1: 0.3915330469608307\n",
      "[589,     4] loss: 0.3091 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8224 | val_acc: 0.8235 | val_recall: 0.2725 | f1: 0.3921186327934265\n",
      "[590,     4] loss: 0.3080 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8221 | val_acc: 0.8221 | val_recall: 0.2734 | f1: 0.3940398395061493\n",
      "[591,     4] loss: 0.3053 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8227 | val_acc: 0.8232 | val_recall: 0.2725 | f1: 0.3943541646003723\n",
      "[592,     4] loss: 0.3102 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8232 | val_acc: 0.8199 | val_recall: 0.2705 | f1: 0.3890702426433563\n",
      "[593,     4] loss: 0.3060 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8310 | val_acc: 0.8228 | val_recall: 0.2656 | f1: 0.3874857425689697\n",
      "[594,     4] loss: 0.3058 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8276 | val_acc: 0.8241 | val_recall: 0.2656 | f1: 0.3855364918708801\n",
      "[595,     4] loss: 0.3051 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8298 | val_acc: 0.8199 | val_recall: 0.2607 | f1: 0.3820236325263977\n",
      "[596,     4] loss: 0.3040 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8274 | val_acc: 0.8187 | val_recall: 0.2637 | f1: 0.38437411189079285\n",
      "[597,     4] loss: 0.3053 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8202 | val_acc: 0.8225 | val_recall: 0.2666 | f1: 0.386316180229187\n",
      "[598,     4] loss: 0.3029 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8257 | val_acc: 0.8191 | val_recall: 0.2627 | f1: 0.38250482082366943\n",
      "[599,     4] loss: 0.3052 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8316 | val_acc: 0.8207 | val_recall: 0.2627 | f1: 0.3838251233100891\n",
      "[600,     4] loss: 0.3028 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8345 | val_acc: 0.8203 | val_recall: 0.2637 | f1: 0.38263776898384094\n",
      "[601,     4] loss: 0.3011 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8298 | val_acc: 0.8218 | val_recall: 0.2666 | f1: 0.3873773217201233\n",
      "Saving checkpoint 600\n",
      "[602,     4] loss: 0.3000 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8312 | val_acc: 0.8214 | val_recall: 0.2656 | f1: 0.38695627450942993\n",
      "[603,     4] loss: 0.3000 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8356 | val_acc: 0.8223 | val_recall: 0.2607 | f1: 0.38819193840026855\n",
      "[604,     4] loss: 0.2983 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8377 | val_acc: 0.8208 | val_recall: 0.2676 | f1: 0.38859766721725464\n",
      "[605,     4] loss: 0.2967 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8320 | val_acc: 0.8198 | val_recall: 0.2695 | f1: 0.39056962728500366\n",
      "[606,     4] loss: 0.2967 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8318 | val_acc: 0.8179 | val_recall: 0.2695 | f1: 0.38807907700538635\n",
      "[607,     4] loss: 0.2966 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8364 | val_acc: 0.8179 | val_recall: 0.2588 | f1: 0.381032258272171\n",
      "[608,     4] loss: 0.2982 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8377 | val_acc: 0.8191 | val_recall: 0.2607 | f1: 0.38135623931884766\n",
      "[609,     4] loss: 0.2975 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8408 | val_acc: 0.8191 | val_recall: 0.2588 | f1: 0.38250482082366943\n",
      "[610,     4] loss: 0.2948 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8413 | val_acc: 0.8220 | val_recall: 0.2588 | f1: 0.38397178053855896\n",
      "[611,     4] loss: 0.2950 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8444 | val_acc: 0.8229 | val_recall: 0.2617 | f1: 0.38463863730430603\n",
      "[612,     4] loss: 0.2940 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8466 | val_acc: 0.8203 | val_recall: 0.2607 | f1: 0.38435789942741394\n",
      "[613,     4] loss: 0.2935 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8443 | val_acc: 0.8217 | val_recall: 0.2637 | f1: 0.38622185587882996\n",
      "[614,     4] loss: 0.2935 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8531 | val_acc: 0.8251 | val_recall: 0.2637 | f1: 0.38602542877197266\n",
      "[615,     4] loss: 0.2931 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8558 | val_acc: 0.8223 | val_recall: 0.2656 | f1: 0.3874315619468689\n",
      "[616,     4] loss: 0.2923 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8533 | val_acc: 0.8219 | val_recall: 0.2617 | f1: 0.3845318555831909\n",
      "[617,     4] loss: 0.2904 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8550 | val_acc: 0.8223 | val_recall: 0.2666 | f1: 0.38914111256599426\n",
      "[618,     4] loss: 0.2910 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8540 | val_acc: 0.8209 | val_recall: 0.2617 | f1: 0.3849973976612091\n",
      "[619,     4] loss: 0.2901 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8510 | val_acc: 0.8221 | val_recall: 0.2637 | f1: 0.3887479603290558\n",
      "[620,     4] loss: 0.2892 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8542 | val_acc: 0.8235 | val_recall: 0.2656 | f1: 0.38984689116477966\n",
      "[621,     4] loss: 0.2885 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8491 | val_acc: 0.8242 | val_recall: 0.2656 | f1: 0.38878947496414185\n",
      "[622,     4] loss: 0.2862 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8479 | val_acc: 0.8246 | val_recall: 0.2676 | f1: 0.3911074995994568\n",
      "[623,     4] loss: 0.2881 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8446 | val_acc: 0.8251 | val_recall: 0.2676 | f1: 0.39059367775917053\n",
      "[624,     4] loss: 0.2863 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8512 | val_acc: 0.8236 | val_recall: 0.2686 | f1: 0.3900502026081085\n",
      "[625,     4] loss: 0.2868 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8529 | val_acc: 0.8245 | val_recall: 0.2646 | f1: 0.3882460594177246\n",
      "[626,     4] loss: 0.2871 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8566 | val_acc: 0.8228 | val_recall: 0.2646 | f1: 0.3869146704673767\n",
      "[627,     4] loss: 0.2855 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8584 | val_acc: 0.8212 | val_recall: 0.2686 | f1: 0.38787949085235596\n",
      "[628,     4] loss: 0.2844 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8545 | val_acc: 0.8223 | val_recall: 0.2656 | f1: 0.3897099494934082\n",
      "[629,     4] loss: 0.2843 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8510 | val_acc: 0.8236 | val_recall: 0.2676 | f1: 0.3879610300064087\n",
      "[630,     4] loss: 0.2844 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8592 | val_acc: 0.8207 | val_recall: 0.2676 | f1: 0.386684775352478\n",
      "[631,     4] loss: 0.2843 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8660 | val_acc: 0.8218 | val_recall: 0.2666 | f1: 0.3868066072463989\n",
      "[632,     4] loss: 0.2839 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8695 | val_acc: 0.8226 | val_recall: 0.2627 | f1: 0.3859483301639557\n",
      "[633,     4] loss: 0.2825 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8648 | val_acc: 0.8235 | val_recall: 0.2637 | f1: 0.3858514726161957\n",
      "[634,     4] loss: 0.2838 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8635 | val_acc: 0.8213 | val_recall: 0.2725 | f1: 0.39356529712677\n",
      "[635,     4] loss: 0.2815 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8655 | val_acc: 0.8218 | val_recall: 0.2637 | f1: 0.3907908499240875\n",
      "[636,     4] loss: 0.2799 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8663 | val_acc: 0.8230 | val_recall: 0.2686 | f1: 0.3931957185268402\n",
      "[637,     4] loss: 0.2797 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8639 | val_acc: 0.8236 | val_recall: 0.2656 | f1: 0.3904293179512024\n",
      "[638,     4] loss: 0.2787 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8685 | val_acc: 0.8234 | val_recall: 0.2686 | f1: 0.39380308985710144\n",
      "[639,     4] loss: 0.2786 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8675 | val_acc: 0.8237 | val_recall: 0.2695 | f1: 0.3944101631641388\n",
      "[640,     4] loss: 0.2773 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8705 | val_acc: 0.8230 | val_recall: 0.2705 | f1: 0.3928184509277344\n",
      "[641,     4] loss: 0.2773 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8690 | val_acc: 0.8225 | val_recall: 0.2715 | f1: 0.3959614336490631\n",
      "[642,     4] loss: 0.2775 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8703 | val_acc: 0.8214 | val_recall: 0.2744 | f1: 0.3963963985443115\n",
      "[643,     4] loss: 0.2765 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8684 | val_acc: 0.8217 | val_recall: 0.2754 | f1: 0.3960498869419098\n",
      "[644,     4] loss: 0.2747 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8724 | val_acc: 0.8201 | val_recall: 0.2744 | f1: 0.39455243945121765\n",
      "[645,     4] loss: 0.2798 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8831 | val_acc: 0.8234 | val_recall: 0.2676 | f1: 0.38602888584136963\n",
      "[646,     4] loss: 0.2768 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8894 | val_acc: 0.8203 | val_recall: 0.2715 | f1: 0.3883533179759979\n",
      "[647,     4] loss: 0.2755 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8846 | val_acc: 0.8193 | val_recall: 0.2695 | f1: 0.38767486810684204\n",
      "[648,     4] loss: 0.2754 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8810 | val_acc: 0.8206 | val_recall: 0.2754 | f1: 0.39065268635749817\n",
      "[649,     4] loss: 0.2745 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8827 | val_acc: 0.8203 | val_recall: 0.2754 | f1: 0.3917577564716339\n",
      "[650,     4] loss: 0.2741 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8927 | val_acc: 0.8206 | val_recall: 0.2734 | f1: 0.3855290114879608\n",
      "[651,     4] loss: 0.2743 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8847 | val_acc: 0.8243 | val_recall: 0.2715 | f1: 0.38785186409950256\n",
      "[652,     4] loss: 0.2732 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9071 | val_acc: 0.8184 | val_recall: 0.2539 | f1: 0.36967921257019043\n",
      "[653,     4] loss: 0.2822 | train_acc: 0.9941 | train_recall: 0.9688| val_loss: 1.9202 | val_acc: 0.8240 | val_recall: 0.2520 | f1: 0.3667171001434326\n",
      "[654,     4] loss: 0.2831 | train_acc: 0.9990 | train_recall: 0.9922| val_loss: 1.9199 | val_acc: 0.8190 | val_recall: 0.2686 | f1: 0.3849727213382721\n",
      "[655,     4] loss: 0.2745 | train_acc: 0.9980 | train_recall: 0.9844| val_loss: 1.8986 | val_acc: 0.8176 | val_recall: 0.2871 | f1: 0.3903197646141052\n",
      "[656,     4] loss: 0.2747 | train_acc: 0.9980 | train_recall: 0.9844| val_loss: 1.8840 | val_acc: 0.8284 | val_recall: 0.2764 | f1: 0.38924944400787354\n",
      "[657,     4] loss: 0.2868 | train_acc: 0.9961 | train_recall: 0.9844| val_loss: 1.8760 | val_acc: 0.8165 | val_recall: 0.2900 | f1: 0.4063844382762909\n",
      "[658,     4] loss: 0.3349 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.9313 | val_acc: 0.8101 | val_recall: 0.2324 | f1: 0.3365572988986969\n",
      "[659,     4] loss: 0.3148 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.9261 | val_acc: 0.8174 | val_recall: 0.2480 | f1: 0.3577718436717987\n",
      "[660,     4] loss: 0.2957 | train_acc: 0.9961 | train_recall: 0.9844| val_loss: 1.9327 | val_acc: 0.8125 | val_recall: 0.2578 | f1: 0.36693552136421204\n",
      "[661,     4] loss: 0.2878 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9254 | val_acc: 0.8115 | val_recall: 0.2637 | f1: 0.36722567677497864\n",
      "[662,     4] loss: 0.2821 | train_acc: 0.9961 | train_recall: 1.0000| val_loss: 1.9063 | val_acc: 0.8134 | val_recall: 0.2529 | f1: 0.3658502697944641\n",
      "[663,     4] loss: 0.2848 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8872 | val_acc: 0.8136 | val_recall: 0.2725 | f1: 0.38494575023651123\n",
      "[664,     4] loss: 0.2840 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8807 | val_acc: 0.8180 | val_recall: 0.2773 | f1: 0.39205753803253174\n",
      "[665,     4] loss: 0.2744 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8954 | val_acc: 0.8142 | val_recall: 0.2793 | f1: 0.3901172876358032\n",
      "[666,     4] loss: 0.2741 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8995 | val_acc: 0.8169 | val_recall: 0.2695 | f1: 0.39249518513679504\n",
      "[667,     4] loss: 0.2690 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.8981 | val_acc: 0.8174 | val_recall: 0.2686 | f1: 0.3919873833656311\n",
      "[668,     4] loss: 0.2689 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9014 | val_acc: 0.8184 | val_recall: 0.2676 | f1: 0.3909691572189331\n",
      "[669,     4] loss: 0.2666 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9036 | val_acc: 0.8188 | val_recall: 0.2656 | f1: 0.3864811658859253\n",
      "[670,     4] loss: 0.2666 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9012 | val_acc: 0.8175 | val_recall: 0.2686 | f1: 0.38860562443733215\n",
      "[671,     4] loss: 0.2676 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9016 | val_acc: 0.8196 | val_recall: 0.2734 | f1: 0.38940757513046265\n",
      "[672,     4] loss: 0.2659 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9004 | val_acc: 0.8215 | val_recall: 0.2803 | f1: 0.39641061425209045\n",
      "[673,     4] loss: 0.2658 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9071 | val_acc: 0.8207 | val_recall: 0.2744 | f1: 0.3914220631122589\n",
      "[674,     4] loss: 0.2644 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9087 | val_acc: 0.8198 | val_recall: 0.2744 | f1: 0.38829857110977173\n",
      "[675,     4] loss: 0.2613 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9146 | val_acc: 0.8203 | val_recall: 0.2773 | f1: 0.3945806920528412\n",
      "[676,     4] loss: 0.2622 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9135 | val_acc: 0.8220 | val_recall: 0.2715 | f1: 0.38683363795280457\n",
      "[677,     4] loss: 0.2610 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9230 | val_acc: 0.8220 | val_recall: 0.2656 | f1: 0.3835892379283905\n",
      "[678,     4] loss: 0.2620 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9193 | val_acc: 0.8208 | val_recall: 0.2705 | f1: 0.3878386318683624\n",
      "[679,     4] loss: 0.2609 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9232 | val_acc: 0.8224 | val_recall: 0.2666 | f1: 0.38324621319770813\n",
      "[680,     4] loss: 0.2595 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9200 | val_acc: 0.8221 | val_recall: 0.2686 | f1: 0.3847495913505554\n",
      "[681,     4] loss: 0.2600 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9132 | val_acc: 0.8234 | val_recall: 0.2705 | f1: 0.3858380913734436\n",
      "[682,     4] loss: 0.2572 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9177 | val_acc: 0.8245 | val_recall: 0.2695 | f1: 0.38595858216285706\n",
      "[683,     4] loss: 0.2581 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9184 | val_acc: 0.8226 | val_recall: 0.2715 | f1: 0.38518503308296204\n",
      "[684,     4] loss: 0.2592 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9246 | val_acc: 0.8230 | val_recall: 0.2754 | f1: 0.38979214429855347\n",
      "[685,     4] loss: 0.2563 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9258 | val_acc: 0.8247 | val_recall: 0.2754 | f1: 0.38941389322280884\n",
      "[686,     4] loss: 0.2586 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9128 | val_acc: 0.8239 | val_recall: 0.2754 | f1: 0.39498889446258545\n",
      "[687,     4] loss: 0.2640 | train_acc: 0.9922 | train_recall: 0.9688| val_loss: 1.9352 | val_acc: 0.8175 | val_recall: 0.2773 | f1: 0.392753541469574\n",
      "[688,     4] loss: 0.2581 | train_acc: 0.9961 | train_recall: 0.9844| val_loss: 1.9173 | val_acc: 0.8260 | val_recall: 0.2725 | f1: 0.3844096064567566\n",
      "[689,     4] loss: 0.2609 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9154 | val_acc: 0.8268 | val_recall: 0.2744 | f1: 0.3943816125392914\n",
      "[690,     4] loss: 0.2580 | train_acc: 0.9980 | train_recall: 0.9844| val_loss: 1.9166 | val_acc: 0.8256 | val_recall: 0.2656 | f1: 0.3849312961101532\n",
      "[691,     4] loss: 0.2623 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9528 | val_acc: 0.8145 | val_recall: 0.2539 | f1: 0.36811015009880066\n",
      "[692,     4] loss: 0.2583 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9673 | val_acc: 0.8167 | val_recall: 0.2539 | f1: 0.36716148257255554\n",
      "[693,     4] loss: 0.2550 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9261 | val_acc: 0.8185 | val_recall: 0.2646 | f1: 0.3847283124923706\n",
      "[694,     4] loss: 0.2580 | train_acc: 0.9980 | train_recall: 1.0000| val_loss: 1.9220 | val_acc: 0.8192 | val_recall: 0.2637 | f1: 0.3895553648471832\n",
      "[695,     4] loss: 0.2530 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9272 | val_acc: 0.8199 | val_recall: 0.2666 | f1: 0.3824065625667572\n",
      "[696,     4] loss: 0.2549 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9221 | val_acc: 0.8188 | val_recall: 0.2764 | f1: 0.39777880907058716\n",
      "[697,     4] loss: 0.2521 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9344 | val_acc: 0.8170 | val_recall: 0.2705 | f1: 0.39363574981689453\n",
      "[698,     4] loss: 0.2533 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9342 | val_acc: 0.8180 | val_recall: 0.2715 | f1: 0.39149269461631775\n",
      "[699,     4] loss: 0.2517 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9403 | val_acc: 0.8204 | val_recall: 0.2695 | f1: 0.39459481835365295\n",
      "[700,     4] loss: 0.2512 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9369 | val_acc: 0.8203 | val_recall: 0.2695 | f1: 0.39401713013648987\n",
      "[701,     4] loss: 0.2489 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9337 | val_acc: 0.8198 | val_recall: 0.2725 | f1: 0.39714667201042175\n",
      "Saving checkpoint 700\n",
      "[702,     4] loss: 0.2533 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9272 | val_acc: 0.8226 | val_recall: 0.2686 | f1: 0.39447227120399475\n",
      "[703,     4] loss: 0.2488 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9317 | val_acc: 0.8235 | val_recall: 0.2744 | f1: 0.39944541454315186\n",
      "[704,     4] loss: 0.2494 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9309 | val_acc: 0.8247 | val_recall: 0.2734 | f1: 0.39677900075912476\n",
      "[705,     4] loss: 0.2486 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9328 | val_acc: 0.8237 | val_recall: 0.2725 | f1: 0.3955390453338623\n",
      "[706,     4] loss: 0.2470 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9381 | val_acc: 0.8220 | val_recall: 0.2744 | f1: 0.3959048092365265\n",
      "[712,     4] loss: 0.2411 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9494 | val_acc: 0.8239 | val_recall: 0.2686 | f1: 0.39480072259902954\n",
      "[713,     4] loss: 0.2421 | train_acc: 1.0000 | train_recall: 1.0000| val_loss: 1.9449 | val_acc: 0.8247 | val_recall: 0.2676 | f1: 0.3901732861995697\n"
     ]
    }
   ],
   "source": [
    "epochs_to_test = [400]\n",
    "for n in epochs_to_test: \n",
    "    for i in range(len(file_paths)):\n",
    "        #train file path\n",
    "        train_file = file_paths[i]\n",
    "        #train_file = root+r'/'+files[i]\n",
    "        wandb_name = 'color_detect_'+ os.path.basename(os.path.dirname(train_file))\n",
    "        # initilize and make new dir for run\n",
    "        #run_num = '0'+str(i) #I've decided this makes it harder\n",
    "        run_str = os.path.basename(train_file)[34:-4]\n",
    "        run_dir_name = run_str+'/'\n",
    "        if not os.path.exists(run_dir_name):\n",
    "            os.mkdir(run_dir_name)\n",
    "\n",
    "        split_parts = run_str.rsplit('_', 10)\n",
    "        print(split_parts)\n",
    "        # Check if there is at least one underscore in the string\n",
    "        if len(split_parts) > 1:\n",
    "            # Get the substring after the last underscore\n",
    "            num_images = split_parts[-1]\n",
    "            num_ids = split_parts[0]\n",
    "        else:\n",
    "            # Handle the case where there are no underscores in the string\n",
    "            num_images = run_str\n",
    "\n",
    "        print(num_images)\n",
    "        print(num_ids)\n",
    "\n",
    "        #filter particular runs (if needed)\n",
    "        #if num_images == 'max':\n",
    "        \n",
    "        \n",
    "        #\"\"\"#skipping max because I already ran it\n",
    "        if  num_images not in ['64']:\n",
    "            print('Skipping')\n",
    "            continue\n",
    "        #\"\"\"\n",
    "\n",
    "        ##---------- Initilize new config .yml for new training file---------------\n",
    "\n",
    "        #open config yaml to update experiment params\n",
    "        with open('/home/lmeyers/ReID_complete/color_detect_template.yml', 'r') as fo:\n",
    "            config = yaml.safe_load(fo)\n",
    "        \n",
    "        #Update params\n",
    "        #config['model_settings']['num_labels']= run_str[0]\n",
    "        #print('Num labels ',run_str[0])\n",
    "\n",
    "        #Check if batch size needs to be updated\n",
    "        df = pd.read_csv(train_file)\n",
    "        if config['data_settings']['batch_size'] > len(df):\n",
    "            config['data_settings']['batch_size'] = len(df)\n",
    "            print('Updated batch to contain all Data. Size = ',len(df))\n",
    "\n",
    "        #Check if print_k needs to be updated for small dataset\n",
    "        print_k = config['train_settings']['print_k']\n",
    "        if print_k > len(df)/config['data_settings']['batch_size']:\n",
    "            print_k = int(np.floor(len(df)/config['data_settings']['batch_size']))\n",
    "            config['train_settings']['print_k'] = print_k\n",
    "            print('Updating print_k to contain whole epoch. Num_batches =',print_k)\n",
    "        \n",
    "        #Testing a differnt num epochs (EXPIRAMENT HERE)\n",
    "        num_epochs =  1550\n",
    "        config['train_settings']['num_epochs'] = num_epochs\n",
    "        print(\"Number of Epochs\",num_epochs)\n",
    "        \n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_file\n",
    "        config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "\n",
    "        #config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = test_file\n",
    "        config['data_settings']['datafiles']['valid']= valid_file \n",
    "        config['data_settings']['datafiles']['query']= test_file\n",
    "\n",
    "        #update Model path\n",
    "        config['model_settings']['model_path'] = './'+run_dir_name+run_str+'.pth'\n",
    "\n",
    "        #update wandb_project_name\n",
    "        config['train_settings']['wandb_project_name'] = wandb_name\n",
    "        config['train_settings']['wandb_dir_path'] = '/home/lmeyers/ReID_complete/color_detect_experiments/'+ run_dir_name #this should make a seperate wandb folder for runs\n",
    "\n",
    "        #save yml\n",
    "        new_yml_file = './'+run_dir_name+run_str+'.yml'\n",
    "\n",
    "\n",
    "        with open(new_yml_file, 'w') as fo:\n",
    "                yaml.dump(config,fo)   \n",
    "\n",
    "        #---------- actually run training too--------------\n",
    "        !python /home/lmeyers/ReID_complete/pytorch_train_and_eval_color_detect.py --config_file {new_yml_file}\n",
    "\n",
    "        # Save model to wandb file location to prevent overwriting\n",
    "        !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "        with open('/home/lmeyers/ReID_complete/color_detect_experiments/results.pkl','rb') as fi:\n",
    "            results = pickle.load(fi)\n",
    "    \n",
    "      \n",
    "        # Write out run summary to results tracking document\n",
    "        results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "        results_df.loc[len(results_df)] = {'run_str': run_str,\n",
    "                                            'wandb_id':results['wandb_id'],\n",
    "                                            'num_ids':num_ids,\n",
    "                                            'num_images_per_id':num_images,\n",
    "                                            'total_training_images':len(pd.read_csv(train_file)),\n",
    "                                            'batch_size':config['data_settings']['batch_size'],\n",
    "                                            'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                            'train_loss':results['train_loss'],\n",
    "                                            'valid_loss':results['valid_loss'],\n",
    "                                            'percent_correct_samples':results['percent_correct_samples'],\n",
    "                                            'total_acc':results[\"Total Acc:\"],\n",
    "                                            'total_precision':results[\"Total Precision:\"],\n",
    "                                            'total_recall':results[\"Total Recall:\"],\n",
    "                                            'training_file':train_file,\n",
    "                                            'reference_file':reference_file,\n",
    "                                            'query_file':test_file,\n",
    "                                            'start_time':results['start_time'],\n",
    "                                            'train_time':results['train_time'],\n",
    "                                            'stop_epoch':results['stop_epoch']}\n",
    "        results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3c3300",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pytorch_train_and_eval_reid.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     21\u001b[0m     closed_for_this_test\u001b[38;5;241m=\u001b[39m closed_test\u001b[38;5;241m+\u001b[39m  batch_number\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/*\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     if(batch_number == 'batch1'):\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         #open_set_for_test = open_set_for_test+'batch2/'+\"summer_bee_dataset_open_train_bee_balanced_batch2_sample_num_max.csv\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     print(open_set_for_test)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./pytorch_train_and_eval_reid.yml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     32\u001b[0m         config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train_csv \u001b[38;5;129;01min\u001b[39;00m glob(all_csv_directories):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m#print('train_csv')\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m#print(train_csv)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#         print(closed_for_this_test)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#         print(glob(closed_for_this_test))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pytorch_train_and_eval_reid.yml'"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we can specify what we want to train. If we want everything, remove max, it should work\n",
    "directory = '/home/gsantiago/summer_bee_data/closed_sets_max*'\n",
    "closed_set_directory = '/home/gsantiago/summer_bee_data/closed_test_'\n",
    "closed_test = \"/home/gsantiago/summer_bee_data/closed_test_\"\n",
    "\n",
    "model_path = '/home/gsantiago/ReID_model_training/model_path/'\n",
    "for path in glob(directory):\n",
    "    \n",
    "    \n",
    "    batch_number = path.split('_')[-1]\n",
    "    \n",
    "    all_csv_directories = path+'/*'\n",
    "    full_dir_name = path.split('/')[-1]\n",
    "#     print(full_dir_name)\n",
    "#     continue\n",
    "    type_of_split = full_dir_name.replace('closed_sets', '').replace(batch_number,'')\n",
    "    #open_set_for_test = open_sets+\"open\"+type_of_split\n",
    "\n",
    "    #print(batch_number)\n",
    "    \n",
    "    closed_for_this_test= closed_test+  batch_number+'/*'\n",
    "    \n",
    "#     if(batch_number == 'batch1'):\n",
    "        \n",
    "#         #open_set_for_test = open_set_for_test+'batch2/'+\"summer_bee_dataset_open_train_bee_balanced_batch2_sample_num_max.csv\"\n",
    "#     else:\n",
    "#         #open_set_for_test = open_set_for_test+'batch1/'+\"summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv\"\n",
    "        \n",
    "#     print(open_set_for_test)\n",
    "    \n",
    "    with open('./pytorch_train_and_eval_reid.yml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    for train_csv in glob(all_csv_directories):\n",
    "        #print('train_csv')\n",
    "        #print(train_csv)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #changing train folder\n",
    "#         print(closed_for_this_test)\n",
    "#         print(glob(closed_for_this_test))\n",
    "        train_df = pd.read_csv(train_csv)\n",
    "        num_labels = train_df['ID'].nunique()\n",
    "        #print(num_labels)\n",
    "        \n",
    "        #batch setting\n",
    "        config['data_settings']['batch_size']= 64\n",
    "        \n",
    "        #updating labels\n",
    "        \n",
    "        config['model_settings']['num_labels']=num_labels\n",
    "        \n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['reference']= train_csv\n",
    "  \n",
    "\n",
    "        #config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = glob(closed_for_this_test)[0]\n",
    "        config['data_settings']['datafiles']['valid']= glob(closed_for_this_test)[0]\n",
    "        config['data_settings']['datafiles']['query']= glob(closed_for_this_test)[0]\n",
    "        \n",
    "        model_directory =(model_path+\"closed/\"+full_dir_name+'/')\n",
    "        \n",
    "        if not os.path.exists(model_directory):\n",
    "            os.makedirs(model_directory)\n",
    "        config['model_settings']['model_path']= model_directory\n",
    "        \n",
    "        config_name = train_csv.split('/')[-1].replace('.csv', '')\n",
    "        print(config['data_settings']['datafiles'])\n",
    "\n",
    "        new_yml_file = \"./different_yml_configs/closed/\"+config_name+'_config.yml'\n",
    "        output_file = \"./all_outputs/\"+(new_yml_file.split('/')[-1].replace('_config.yml', '_output.txt'))\n",
    "       \n",
    "        with open(new_yml_file, 'w') as f:\n",
    "            yaml.dump(config,f)\n",
    "        \n",
    "        !python pytorch_train_and_eval_reid.py --config_file {new_yml_file} > {output_file} \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f014d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d2bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13c236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c1bc2ca",
   "metadata": {},
   "source": [
    "# open set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv\n",
      "64\n",
      "2023-10-15 22:44:31.603382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 22:44:32.942356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231015_224436-pfstr74p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdaily-plant-25\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/pfstr74p\u001b[0m\n",
      "/home/gsantiago/ReID_model_training/pytorch_train_and_eval_reid.py:77: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  knn_class[k] = np.round(np.sum(knn_pred[mask]==test_labels[mask])/np.sum(mask),4)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.005 MB of 0.012 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss 0.14135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdaily-plant-25\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/pfstr74p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231015_224436-pfstr74p/logs\u001b[0m\n",
      "train_csv\n",
      "64\n",
      "2023-10-15 23:53:43.592262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 23:53:45.135061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231015_235349-b64kuzyj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-voice-26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/b64kuzyj\u001b[0m\n",
      "/home/gsantiago/ReID_model_training/pytorch_train_and_eval_reid.py:77: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  knn_class[k] = np.round(np.sum(knn_pred[mask]==test_labels[mask])/np.sum(mask),4)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.005 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñà‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss 0.16833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mefficient-voice-26\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/b64kuzyj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231015_235349-b64kuzyj/logs\u001b[0m\n",
      "train_csv\n",
      "64\n",
      "2023-10-16 00:45:50.898888: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 00:45:52.091201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231016_004555-9myro3ut\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdandy-voice-27\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/9myro3ut\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we can specify what we want to train. If we want everything, remove max, it should work\n",
    "directory = '/home/gsantiago/summer_bee_data/open_sets/open_max*'\n",
    "open_set_directory = '/home/gsantiago/summer_bee_data/open_sets/'\n",
    "open_test = \"/home/gsantiago/summer_bee_data/open_sets/\"\n",
    "\n",
    "model_path = '/home/gsantiago/ReID_model_training/model_path/'\n",
    "for path in glob(directory):\n",
    "    \n",
    "    #print(path)\n",
    "    batch_number = path.split('_')[-1]\n",
    "    #print(batch_number)\n",
    "    \n",
    "    all_csv_directories = path+'/*'\n",
    "    #print(glob(all_csv_directories))\n",
    "    full_dir_name = path.split('/')[-1]\n",
    "    #print(full_dir_name)\n",
    "    type_of_split = full_dir_name.replace(batch_number,'')\n",
    "    #print(\"type_of_split\")\n",
    "    #print(type_of_split)\n",
    "   \n",
    "\n",
    "    #print(batch_number)\n",
    "    \n",
    "    #open_for_this_test= open_test+  batch_number+'/*'\n",
    "    \n",
    "    if(batch_number == 'batch1'):\n",
    "        \n",
    "        open_set_for_test = open_test+type_of_split+'batch2/'+\"summer_bee_dataset_open_train_bee_balanced_batch2_sample_num_max.csv\"\n",
    "        \n",
    "    else:\n",
    "        open_set_for_test = open_test+type_of_split+'batch1/'+\"summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv\"\n",
    "        \n",
    "    #print(open_set_for_test)\n",
    "    \n",
    "    with open('./pytorch_train_and_eval_reid.yml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    #print(glob(all_csv_directories))\n",
    "    for train_csv in glob(all_csv_directories):\n",
    "        print('train_csv')\n",
    "        #print(train_csv)\n",
    "\n",
    "\n",
    "        # optional, skipping  the finished cvs\n",
    "        \n",
    "        #changing train folder\n",
    "        #print(\"open_set_for_test\")\n",
    "        #print(open_set_for_test)\n",
    "        #print(glob(open_set_for_test))\n",
    "        train_df = pd.read_csv(train_csv)\n",
    "        num_labels = train_df['ID'].nunique()\n",
    "        print(num_labels)\n",
    "        \n",
    "        #batch setting\n",
    "        config['data_settings']['batch_size']= 64\n",
    "        \n",
    "        #updating labels\n",
    "        \n",
    "        config['model_settings']['num_labels']=num_labels\n",
    "        \n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = open_set_for_test\n",
    "        config['data_settings']['datafiles']['valid']= open_set_for_test\n",
    "        config['data_settings']['datafiles']['reference']= train_csv\n",
    "        config['data_settings']['datafiles']['query']= open_set_for_test\n",
    "        \n",
    "        \n",
    "                \n",
    "        model_directory =(model_path+\"open/\"+full_dir_name+'/')\n",
    "        train_model_directory = model_directory+'train/'\n",
    "        eval_model_directory = model_directory +'eval/'\n",
    "        \n",
    "        if not os.path.exists(model_directory):\n",
    "            os.makedirs(model_directory)\n",
    "            os.makedirs(train_model_directory)\n",
    "            os.makedirs(eval_model_directory)\n",
    "            \n",
    "        #sample size for model, saved as name\n",
    "        \n",
    "        sample_num = train_csv.split('_')[-1].replace(\".csv\",'_samples.pth')\n",
    "            \n",
    "        config['model_settings']['model_path']= train_model_directory+sample_num\n",
    "        config['eval_settings']['model_path'] = eval_model_directory +sample_num\n",
    "        \n",
    "\n",
    "        \n",
    "        config_name = train_csv.split('/')[-1].replace('.csv', '')\n",
    "        #print(config['data_settings']['datafiles'])\n",
    "\n",
    "        new_yml_file = \"./different_yml_configs/open/\"+config_name+'_config.yml'\n",
    "        output_file = \"./all_outputs/\"+(new_yml_file.split('/')[-1].replace('_config.yml', '_output.txt'))\n",
    "       \n",
    "        with open(new_yml_file, 'w') as f:\n",
    "            yaml.dump(config,f)\n",
    "        \n",
    "        !python pytorch_train_and_eval_reid.py --config_file {new_yml_file} > {output_file} \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ec5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
