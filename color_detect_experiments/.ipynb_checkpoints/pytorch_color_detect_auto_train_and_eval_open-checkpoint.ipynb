{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a95221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d54ee",
   "metadata": {},
   "source": [
    "# Choose Folder Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bbc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir = '/home/gsantiago/summer_bee_data/closed_sets_4_ids_all_colors_once_batch2'\n",
    "#dir = '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2'\n",
    "dir = '/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1' #'/home/gsantiago/summer_bee_data/open_sets/new_open_max_ids_batch1' # '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2'\n",
    "#dir to start working with \n",
    "\n",
    "#make new wandb project based on dir name\n",
    "\n",
    "\n",
    "test_file = '/home/gsantiago/summer_bee_data/open_sets/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_query_64_ids_batch2.csv' # '/home/gsantiago/summer_bee_data/closed_test_batch1/summer_bee_dataset_closed_test_bee_sample_num_None.csv'\n",
    "#test on all of batch 2 (open set) gonna also test on all of batch 1 \n",
    "\n",
    "valid_file = '/home/gsantiago/summer_bee_data/open_sets/open_reference_query_testing_batch2/summer_bee_dataset_closed_test_bee_valid_64_ids_batch2.csv' #/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch2/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'\n",
    "# run valid on smaller subset of test_set to speed training \n",
    "\n",
    "reference_file = '/home/gsantiago/summer_bee_data/closed_sets_max_ids_batch2/summer_bee_dataset_closed_train_bee_balanced_sample_num_2.csv'\n",
    "#reference knn on smaller subset of batch 1\n",
    "\n",
    "results_file = '/home/lmeyers/ReID_complete/color_detect_experiments/Color_detect_few_shot_expirament_results_tracking.csv'\n",
    "\n",
    "file_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "897a1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_max.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_16.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_04.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_32.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_08.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_64.csv\n",
      "/home/gsantiago/summer_bee_data/open_sets/new_open_04_ids_all_colors_batch1/summer_bee_dataset_open_train_bee_4_ids_batch1_sample_num_02.csv\n"
     ]
    }
   ],
   "source": [
    "# Get file list \n",
    "\n",
    "for root, dirs, files in os.walk(dir):\n",
    "    files = files\n",
    "for f in files:\n",
    "    print(root+r'/'+f)\n",
    "    train_file = root+r'/'+f\n",
    "    file_paths.append(train_file)\n",
    "#     continue\n",
    "\n",
    "#files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b7625",
   "metadata": {},
   "source": [
    "## If setting up two folders to train one after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5a0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_4.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_2.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_8.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_16.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_20.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_double_color_batch2/summer_bee_dataset_closed_train_bee_8_ids_double_colors_batch2_sample_num_max.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_max.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_16.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_2.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_4.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_20.csv', '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2/summer_bee_dataset_closed_train_bee_8_ids_monocolor_batch2_sample_num_8.csv']\n"
     ]
    }
   ],
   "source": [
    "dir2 = '/home/gsantiago/summer_bee_data/closed_sets_8_ids_monocolor_batch2'\n",
    "files2 = []\n",
    "for root2, dirs2, files2 in os.walk(dir2):\n",
    "    files2 = files2\n",
    "for f2 in files2:\n",
    "    #print(root+r'/'+f2)\n",
    "    train_file2 = root2+r'/'+f2\n",
    "    file_paths.append(train_file2)\n",
    "#     continue\n",
    "\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40567111",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c6a97",
   "metadata": {},
   "source": [
    "Change num epochs to test and any run filters (if statement just above with(open(new_yml_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e737d9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', 'ids', 'batch1', 'sample', 'num', 'max']\n",
      "max\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '16']\n",
      "16\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '04']\n",
      "04\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '32']\n",
      "32\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '08']\n",
      "08\n",
      "4\n",
      "Skipping\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '64']\n",
      "64\n",
      "4\n",
      "Updating print_k to contain whole epoch. Num_batches = 4\n",
      "Number of Epochs 1500\n",
      "2023-11-19 17:07:57.123656: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-19 17:07:58.315642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlqmeyers\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "['4', 'ids', 'batch1', 'sample', 'num', '02']\n",
      "02\n",
      "4\n",
      "Skipping\n"
     ]
    }
   ],
   "source": [
    "epochs_to_test = [400]\n",
    "for n in epochs_to_test: \n",
    "    for i in range(len(file_paths)):\n",
    "        #train file path\n",
    "        train_file = file_paths[i]\n",
    "        #train_file = root+r'/'+files[i]\n",
    "        wandb_name = 'color_detect_'+ os.path.basename(os.path.dirname(train_file))\n",
    "        # initilize and make new dir for run\n",
    "        #run_num = '0'+str(i) #I've decided this makes it harder\n",
    "        run_str = os.path.basename(train_file)[34:-4]\n",
    "        run_dir_name = run_str+'/'\n",
    "        if not os.path.exists(run_dir_name):\n",
    "            os.mkdir(run_dir_name)\n",
    "\n",
    "        split_parts = run_str.rsplit('_', 10)\n",
    "        print(split_parts)\n",
    "        # Check if there is at least one underscore in the string\n",
    "        if len(split_parts) > 1:\n",
    "            # Get the substring after the last underscore\n",
    "            num_images = split_parts[-1]\n",
    "            num_ids = split_parts[0]\n",
    "        else:\n",
    "            # Handle the case where there are no underscores in the string\n",
    "            num_images = run_str\n",
    "\n",
    "        print(num_images)\n",
    "        print(num_ids)\n",
    "\n",
    "        #filter particular runs (if needed)\n",
    "        #if num_images == 'max':\n",
    "        \n",
    "        \n",
    "        #\"\"\"#skipping max because I already ran it\n",
    "        if  num_images not in ['64']:\n",
    "            print('Skipping')\n",
    "            continue\n",
    "        #\"\"\"\n",
    "\n",
    "        ##---------- Initilize new config .yml for new training file---------------\n",
    "\n",
    "        #open config yaml to update experiment params\n",
    "        with open('/home/lmeyers/ReID_complete/color_detect_template.yml', 'r') as fo:\n",
    "            config = yaml.safe_load(fo)\n",
    "        \n",
    "        #Update params\n",
    "        #config['model_settings']['num_labels']= run_str[0]\n",
    "        #print('Num labels ',run_str[0])\n",
    "\n",
    "        #Check if batch size needs to be updated\n",
    "        df = pd.read_csv(train_file)\n",
    "        if config['data_settings']['batch_size'] > len(df):\n",
    "            config['data_settings']['batch_size'] = len(df)\n",
    "            print('Updated batch to contain all Data. Size = ',len(df))\n",
    "\n",
    "        #Check if print_k needs to be updated for small dataset\n",
    "        print_k = config['train_settings']['print_k']\n",
    "        if print_k > len(df)/config['data_settings']['batch_size']:\n",
    "            print_k = int(np.floor(len(df)/config['data_settings']['batch_size']))\n",
    "            config['train_settings']['print_k'] = print_k\n",
    "            print('Updating print_k to contain whole epoch. Num_batches =',print_k)\n",
    "        \n",
    "        #Testing a differnt num epochs (EXPIRAMENT HERE)\n",
    "        num_epochs =  1550\n",
    "        config['train_settings']['num_epochs'] = num_epochs\n",
    "        print(\"Number of Epochs\",num_epochs)\n",
    "        \n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_file\n",
    "        config['data_settings']['datafiles']['reference']= reference_file\n",
    "\n",
    "\n",
    "        #config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = test_file\n",
    "        config['data_settings']['datafiles']['valid']= valid_file \n",
    "        config['data_settings']['datafiles']['query']= test_file\n",
    "\n",
    "        #update Model path\n",
    "        config['model_settings']['model_path'] = './'+run_dir_name+run_str+'.pth'\n",
    "\n",
    "        #update wandb_project_name\n",
    "        config['train_settings']['wandb_project_name'] = wandb_name\n",
    "        config['train_settings']['wandb_dir_path'] = '/home/lmeyers/ReID_complete/color_detect_experiments/'+ run_dir_name #this should make a seperate wandb folder for runs\n",
    "\n",
    "        #save yml\n",
    "        new_yml_file = './'+run_dir_name+run_str+'.yml'\n",
    "\n",
    "\n",
    "        with open(new_yml_file, 'w') as fo:\n",
    "                yaml.dump(config,fo)   \n",
    "\n",
    "        #---------- actually run training too--------------\n",
    "        !python /home/lmeyers/ReID_complete/pytorch_train_and_eval_color_detect.py --config_file {new_yml_file}\n",
    "\n",
    "        # Save model to wandb file location to prevent overwriting\n",
    "        !cp {config['model_settings']['model_path']} {config['train_settings']['wandb_dir_path']+'/wandb/latest-run/files/'+os.path.basename(config['model_settings']['model_path'])}\n",
    "\n",
    "        with open('/home/lmeyers/ReID_complete/color_detect_experiments/results.pkl','rb') as fi:\n",
    "            results = pickle.load(fi)\n",
    "    \n",
    "      \n",
    "        # Write out run summary to results tracking document\n",
    "        results_df = pd.read_csv(config['eval_settings']['results_file'])\n",
    "        results_df.loc[len(results_df)] = {'run_str': run_str,\n",
    "                                            'wandb_id':results['wandb_id'],\n",
    "                                            'num_ids':num_ids,\n",
    "                                            'num_images_per_id':num_images,\n",
    "                                            'total_training_images':len(pd.read_csv(train_file)),\n",
    "                                            'batch_size':config['data_settings']['batch_size'],\n",
    "                                            'num_epochs':config['train_settings']['num_epochs'],\n",
    "                                            'train_loss':results['train_loss'],\n",
    "                                            'valid_loss':results['valid_loss'],\n",
    "                                            'percent_correct_samples':results['percent_correct_samples'],\n",
    "                                            'total_acc':results[\"Total Acc:\"],\n",
    "                                            'total_precision':results[\"Total Precision:\"],\n",
    "                                            'total_recall':results[\"Total Recall:\"],\n",
    "                                            'training_file':train_file,\n",
    "                                            'reference_file':reference_file,\n",
    "                                            'query_file':test_file,\n",
    "                                            'start_time':results['start_time'],\n",
    "                                            'train_time':results['train_time'],\n",
    "                                            'stop_epoch':results['stop_epoch']}\n",
    "        results_df.to_csv(config['eval_settings']['results_file'],index=False)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3c3300",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './pytorch_train_and_eval_reid.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     21\u001b[0m     closed_for_this_test\u001b[38;5;241m=\u001b[39m closed_test\u001b[38;5;241m+\u001b[39m  batch_number\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/*\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     if(batch_number == 'batch1'):\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         #open_set_for_test = open_set_for_test+'batch2/'+\"summer_bee_dataset_open_train_bee_balanced_batch2_sample_num_max.csv\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     print(open_set_for_test)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./pytorch_train_and_eval_reid.yml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     32\u001b[0m         config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train_csv \u001b[38;5;129;01min\u001b[39;00m glob(all_csv_directories):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m#print('train_csv')\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m#print(train_csv)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#         print(closed_for_this_test)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#         print(glob(closed_for_this_test))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pytorch_train_and_eval_reid.yml'"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we can specify what we want to train. If we want everything, remove max, it should work\n",
    "directory = '/home/gsantiago/summer_bee_data/closed_sets_max*'\n",
    "closed_set_directory = '/home/gsantiago/summer_bee_data/closed_test_'\n",
    "closed_test = \"/home/gsantiago/summer_bee_data/closed_test_\"\n",
    "\n",
    "model_path = '/home/gsantiago/ReID_model_training/model_path/'\n",
    "for path in glob(directory):\n",
    "    \n",
    "    \n",
    "    batch_number = path.split('_')[-1]\n",
    "    \n",
    "    all_csv_directories = path+'/*'\n",
    "    full_dir_name = path.split('/')[-1]\n",
    "#     print(full_dir_name)\n",
    "#     continue\n",
    "    type_of_split = full_dir_name.replace('closed_sets', '').replace(batch_number,'')\n",
    "    #open_set_for_test = open_sets+\"open\"+type_of_split\n",
    "\n",
    "    #print(batch_number)\n",
    "    \n",
    "    closed_for_this_test= closed_test+  batch_number+'/*'\n",
    "    \n",
    "#     if(batch_number == 'batch1'):\n",
    "        \n",
    "#         #open_set_for_test = open_set_for_test+'batch2/'+\"summer_bee_dataset_open_train_bee_balanced_batch2_sample_num_max.csv\"\n",
    "#     else:\n",
    "#         #open_set_for_test = open_set_for_test+'batch1/'+\"summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv\"\n",
    "        \n",
    "#     print(open_set_for_test)\n",
    "    \n",
    "    with open('./pytorch_train_and_eval_reid.yml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    for train_csv in glob(all_csv_directories):\n",
    "        #print('train_csv')\n",
    "        #print(train_csv)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #changing train folder\n",
    "#         print(closed_for_this_test)\n",
    "#         print(glob(closed_for_this_test))\n",
    "        train_df = pd.read_csv(train_csv)\n",
    "        num_labels = train_df['ID'].nunique()\n",
    "        #print(num_labels)\n",
    "        \n",
    "        #batch setting\n",
    "        config['data_settings']['batch_size']= 64\n",
    "        \n",
    "        #updating labels\n",
    "        \n",
    "        config['model_settings']['num_labels']=num_labels\n",
    "        \n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['reference']= train_csv\n",
    "  \n",
    "\n",
    "        #config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = glob(closed_for_this_test)[0]\n",
    "        config['data_settings']['datafiles']['valid']= glob(closed_for_this_test)[0]\n",
    "        config['data_settings']['datafiles']['query']= glob(closed_for_this_test)[0]\n",
    "        \n",
    "        model_directory =(model_path+\"closed/\"+full_dir_name+'/')\n",
    "        \n",
    "        if not os.path.exists(model_directory):\n",
    "            os.makedirs(model_directory)\n",
    "        config['model_settings']['model_path']= model_directory\n",
    "        \n",
    "        config_name = train_csv.split('/')[-1].replace('.csv', '')\n",
    "        print(config['data_settings']['datafiles'])\n",
    "\n",
    "        new_yml_file = \"./different_yml_configs/closed/\"+config_name+'_config.yml'\n",
    "        output_file = \"./all_outputs/\"+(new_yml_file.split('/')[-1].replace('_config.yml', '_output.txt'))\n",
    "       \n",
    "        with open(new_yml_file, 'w') as f:\n",
    "            yaml.dump(config,f)\n",
    "        \n",
    "        !python pytorch_train_and_eval_reid.py --config_file {new_yml_file} > {output_file} \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f014d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d2bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13c236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c1bc2ca",
   "metadata": {},
   "source": [
    "# open set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_csv\n",
      "64\n",
      "2023-10-15 22:44:31.603382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 22:44:32.942356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231015_224436-pfstr74p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdaily-plant-25\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/pfstr74p\u001b[0m\n",
      "/home/gsantiago/ReID_model_training/pytorch_train_and_eval_reid.py:77: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  knn_class[k] = np.round(np.sum(knn_pred[mask]==test_labels[mask])/np.sum(mask),4)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.005 MB of 0.012 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss â–ƒâ–…â–„â–„â–…â–…â–„â–„â–„â–…â–ƒâ–ƒâ–„â–„â–…â–„â–ƒâ–„â–„â–ƒâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–†â–‚â–‚â–ƒâ–‚â–ˆâ–â–‚â–â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss 0.14135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mdaily-plant-25\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/pfstr74p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231015_224436-pfstr74p/logs\u001b[0m\n",
      "train_csv\n",
      "64\n",
      "2023-10-15 23:53:43.592262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 23:53:45.135061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231015_235349-b64kuzyj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mefficient-voice-26\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/b64kuzyj\u001b[0m\n",
      "/home/gsantiago/ReID_model_training/pytorch_train_and_eval_reid.py:77: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  knn_class[k] = np.round(np.sum(knn_pred[mask]==test_labels[mask])/np.sum(mask),4)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.005 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss â–…â–‡â–†â–…â–ˆâ–…â–…â–…â–„â–ƒâ–…â–ƒâ–„â–…â–ƒâ–‡â–ˆâ–„â–‡â–„â–†â–‚â–„â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–„â–ƒâ–‚â–â–„â–ƒâ–„â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 299\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train loss 0.16833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mefficient-voice-26\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/b64kuzyj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231015_235349-b64kuzyj/logs\u001b[0m\n",
      "train_csv\n",
      "64\n",
      "2023-10-16 00:45:50.898888: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 00:45:52.091201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriel-santiago21\u001b[0m (\u001b[33mmeyers_luke_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/gsantiago/ReID_model_training/wandb/run-20231016_004555-9myro3ut\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdandy-voice-27\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/meyers_luke_lab/ReID_Full_dataset_open/runs/9myro3ut\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we can specify what we want to train. If we want everything, remove max, it should work\n",
    "directory = '/home/gsantiago/summer_bee_data/open_sets/open_max*'\n",
    "open_set_directory = '/home/gsantiago/summer_bee_data/open_sets/'\n",
    "open_test = \"/home/gsantiago/summer_bee_data/open_sets/\"\n",
    "\n",
    "model_path = '/home/gsantiago/ReID_model_training/model_path/'\n",
    "for path in glob(directory):\n",
    "    \n",
    "    #print(path)\n",
    "    batch_number = path.split('_')[-1]\n",
    "    #print(batch_number)\n",
    "    \n",
    "    all_csv_directories = path+'/*'\n",
    "    #print(glob(all_csv_directories))\n",
    "    full_dir_name = path.split('/')[-1]\n",
    "    #print(full_dir_name)\n",
    "    type_of_split = full_dir_name.replace(batch_number,'')\n",
    "    #print(\"type_of_split\")\n",
    "    #print(type_of_split)\n",
    "   \n",
    "\n",
    "    #print(batch_number)\n",
    "    \n",
    "    #open_for_this_test= open_test+  batch_number+'/*'\n",
    "    \n",
    "    if(batch_number == 'batch1'):\n",
    "        \n",
    "        open_set_for_test = open_test+type_of_split+'batch2/'+\"summer_bee_dataset_open_train_bee_balanced_batch2_sample_num_max.csv\"\n",
    "        \n",
    "    else:\n",
    "        open_set_for_test = open_test+type_of_split+'batch1/'+\"summer_bee_dataset_open_train_bee_balanced_batch1_sample_num_max.csv\"\n",
    "        \n",
    "    #print(open_set_for_test)\n",
    "    \n",
    "    with open('./pytorch_train_and_eval_reid.yml', 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    #print(glob(all_csv_directories))\n",
    "    for train_csv in glob(all_csv_directories):\n",
    "        print('train_csv')\n",
    "        #print(train_csv)\n",
    "\n",
    "\n",
    "        # optional, skipping  the finished cvs\n",
    "        \n",
    "        #changing train folder\n",
    "        #print(\"open_set_for_test\")\n",
    "        #print(open_set_for_test)\n",
    "        #print(glob(open_set_for_test))\n",
    "        train_df = pd.read_csv(train_csv)\n",
    "        num_labels = train_df['ID'].nunique()\n",
    "        print(num_labels)\n",
    "        \n",
    "        #batch setting\n",
    "        config['data_settings']['batch_size']= 64\n",
    "        \n",
    "        #updating labels\n",
    "        \n",
    "        config['model_settings']['num_labels']=num_labels\n",
    "        \n",
    "        #updating datafiles\n",
    "        config['data_settings']['datafiles']['train']=train_csv\n",
    "        config['data_settings']['datafiles']['test'] = open_set_for_test\n",
    "        config['data_settings']['datafiles']['valid']= open_set_for_test\n",
    "        config['data_settings']['datafiles']['reference']= train_csv\n",
    "        config['data_settings']['datafiles']['query']= open_set_for_test\n",
    "        \n",
    "        \n",
    "                \n",
    "        model_directory =(model_path+\"open/\"+full_dir_name+'/')\n",
    "        train_model_directory = model_directory+'train/'\n",
    "        eval_model_directory = model_directory +'eval/'\n",
    "        \n",
    "        if not os.path.exists(model_directory):\n",
    "            os.makedirs(model_directory)\n",
    "            os.makedirs(train_model_directory)\n",
    "            os.makedirs(eval_model_directory)\n",
    "            \n",
    "        #sample size for model, saved as name\n",
    "        \n",
    "        sample_num = train_csv.split('_')[-1].replace(\".csv\",'_samples.pth')\n",
    "            \n",
    "        config['model_settings']['model_path']= train_model_directory+sample_num\n",
    "        config['eval_settings']['model_path'] = eval_model_directory +sample_num\n",
    "        \n",
    "\n",
    "        \n",
    "        config_name = train_csv.split('/')[-1].replace('.csv', '')\n",
    "        #print(config['data_settings']['datafiles'])\n",
    "\n",
    "        new_yml_file = \"./different_yml_configs/open/\"+config_name+'_config.yml'\n",
    "        output_file = \"./all_outputs/\"+(new_yml_file.split('/')[-1].replace('_config.yml', '_output.txt'))\n",
    "       \n",
    "        with open(new_yml_file, 'w') as f:\n",
    "            yaml.dump(config,f)\n",
    "        \n",
    "        !python pytorch_train_and_eval_reid.py --config_file {new_yml_file} > {output_file} \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ec5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
